<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Introduction to Matrix Algebra and Linear Models | Notes for: Walsh and Lynch. Genetics and Analysis of Quantitative Traits</title>
  <meta name="description" content="Chapter 8 Introduction to Matrix Algebra and Linear Models | Notes for: Walsh and Lynch. Genetics and Analysis of Quantitative Traits" />
  <meta name="generator" content="bookdown 0.18.1 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Introduction to Matrix Algebra and Linear Models | Notes for: Walsh and Lynch. Genetics and Analysis of Quantitative Traits" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Introduction to Matrix Algebra and Linear Models | Notes for: Walsh and Lynch. Genetics and Analysis of Quantitative Traits" />
  
  
  

<meta name="author" content="Thomas Battram" />


<meta name="date" content="2020-06-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="resemblance-between-relatives.html"/>
<link rel="next" href="questions.html"/>
<script src="libs/header-attrs-2.1.1/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="an-overview-of-quantitative-genetics.html"><a href="an-overview-of-quantitative-genetics.html"><i class="fa fa-check"></i><b>1</b> An overview of quantitative genetics</a></li>
<li class="chapter" data-level="2" data-path="properties-of-distributions.html"><a href="properties-of-distributions.html"><i class="fa fa-check"></i><b>2</b> Properties of distributions</a></li>
<li class="chapter" data-level="3" data-path="covariance-regression-and-correlation.html"><a href="covariance-regression-and-correlation.html"><i class="fa fa-check"></i><b>3</b> Covariance, regression, and correlation</a>
<ul>
<li class="chapter" data-level="3.1" data-path="covariance-regression-and-correlation.html"><a href="covariance-regression-and-correlation.html#covariance"><i class="fa fa-check"></i><b>3.1</b> Covariance</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="covariance-regression-and-correlation.html"><a href="covariance-regression-and-correlation.html#useful-identities-for-covariance"><i class="fa fa-check"></i><b>3.1.1</b> Useful identities for covariance</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="covariance-regression-and-correlation.html"><a href="covariance-regression-and-correlation.html#least-squares-linear-regression"><i class="fa fa-check"></i><b>3.2</b> Least squares linear regression</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="covariance-regression-and-correlation.html"><a href="covariance-regression-and-correlation.html#properties-of-least-squares"><i class="fa fa-check"></i><b>3.2.1</b> Properties of least squares</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="covariance-regression-and-correlation.html"><a href="covariance-regression-and-correlation.html#correlation"><i class="fa fa-check"></i><b>3.3</b> Correlation</a></li>
<li class="chapter" data-level="3.4" data-path="covariance-regression-and-correlation.html"><a href="covariance-regression-and-correlation.html#differential-selection-brief-intro"><i class="fa fa-check"></i><b>3.4</b> Differential selection (brief intro)</a></li>
<li class="chapter" data-level="3.5" data-path="covariance-regression-and-correlation.html"><a href="covariance-regression-and-correlation.html#correlation-between-genotype-and-phenotype-brief-intro"><i class="fa fa-check"></i><b>3.5</b> Correlation between genotype and phenotype (brief intro)</a></li>
<li class="chapter" data-level="3.6" data-path="covariance-regression-and-correlation.html"><a href="covariance-regression-and-correlation.html#end-of-chapter-questions"><i class="fa fa-check"></i><b>3.6</b> End of chapter questions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html"><i class="fa fa-check"></i><b>4</b> Properties of single loci</a>
<ul>
<li class="chapter" data-level="4.1" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#allele-and-genotype-frequencies"><i class="fa fa-check"></i><b>4.2</b> Allele and genotype frequencies</a></li>
<li class="chapter" data-level="4.3" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#the-transmission-of-genetic-information"><i class="fa fa-check"></i><b>4.3</b> The transmission of genetic information</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#the-hardy-weinberg-principle"><i class="fa fa-check"></i><b>4.3.1</b> The Hardy-Weinberg principle</a></li>
<li class="chapter" data-level="4.3.2" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#sex-linked-loci"><i class="fa fa-check"></i><b>4.3.2</b> Sex-linked loci</a></li>
<li class="chapter" data-level="4.3.3" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#polyploidy"><i class="fa fa-check"></i><b>4.3.3</b> Polyploidy</a></li>
<li class="chapter" data-level="4.3.4" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#age-structure"><i class="fa fa-check"></i><b>4.3.4</b> Age structure</a></li>
<li class="chapter" data-level="4.3.5" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#testing-for-hardy-weinberg-proportions"><i class="fa fa-check"></i><b>4.3.5</b> Testing for Hardy-Weinberg proportions</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#characterising-the-influence-of-a-locus-on-the-phenotype"><i class="fa fa-check"></i><b>4.4</b> Characterising the influence of a locus on the phenotype</a></li>
<li class="chapter" data-level="4.5" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#the-basis-of-dominance"><i class="fa fa-check"></i><b>4.5</b> The basis of dominance</a></li>
<li class="chapter" data-level="4.6" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#fishers-decomposition-of-the-genotypic-value"><i class="fa fa-check"></i><b>4.6</b> Fisher’s decomposition of the genotypic value</a></li>
<li class="chapter" data-level="4.7" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#partioning-the-genetic-variance."><i class="fa fa-check"></i><b>4.7</b> Partioning the genetic variance.</a></li>
<li class="chapter" data-level="4.8" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#additive-effects-average-excesses-and-breeding-values"><i class="fa fa-check"></i><b>4.8</b> Additive effects, average excesses and breeding values</a></li>
<li class="chapter" data-level="4.9" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#extensions-for-multiple-alleles-and-non-random-mating"><i class="fa fa-check"></i><b>4.9</b> Extensions for multiple alleles and non random mating</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#average-excess"><i class="fa fa-check"></i><b>4.9.1</b> Average excess</a></li>
<li class="chapter" data-level="4.9.2" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#additive-effects"><i class="fa fa-check"></i><b>4.9.2</b> Additive effects</a></li>
<li class="chapter" data-level="4.9.3" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#additive-genetic-variance"><i class="fa fa-check"></i><b>4.9.3</b> Additive genetic variance</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#end-of-chapter-questions-1"><i class="fa fa-check"></i><b>4.10</b> End of chapter questions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sources-of-genetic-variation-for-multilocus-traits.html"><a href="sources-of-genetic-variation-for-multilocus-traits.html"><i class="fa fa-check"></i><b>5</b> Sources of genetic variation for multilocus traits</a>
<ul>
<li class="chapter" data-level="5.1" data-path="sources-of-genetic-variation-for-multilocus-traits.html"><a href="sources-of-genetic-variation-for-multilocus-traits.html#epistasis"><i class="fa fa-check"></i><b>5.1</b> Epistasis</a></li>
<li class="chapter" data-level="5.2" data-path="sources-of-genetic-variation-for-multilocus-traits.html"><a href="sources-of-genetic-variation-for-multilocus-traits.html#a-general-least-squares-model-for-genetic-effects"><i class="fa fa-check"></i><b>5.2</b> A general least-squares model for genetic effects</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="sources-of-genetic-variation-for-multilocus-traits.html"><a href="sources-of-genetic-variation-for-multilocus-traits.html#extension-to-haploids-and-polyploids"><i class="fa fa-check"></i><b>5.2.1</b> Extension to haploids and polyploids</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="sources-of-genetic-variation-for-multilocus-traits.html"><a href="sources-of-genetic-variation-for-multilocus-traits.html#linkage"><i class="fa fa-check"></i><b>5.3</b> Linkage</a></li>
<li class="chapter" data-level="5.4" data-path="sources-of-genetic-variation-for-multilocus-traits.html"><a href="sources-of-genetic-variation-for-multilocus-traits.html#effect-of-disequilibrium-of-the-genetic-variance"><i class="fa fa-check"></i><b>5.4</b> Effect of disequilibrium of the genetic variance</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="sources-of-genetic-variation-for-multilocus-traits.html"><a href="sources-of-genetic-variation-for-multilocus-traits.html#the-evidence"><i class="fa fa-check"></i><b>5.4.1</b> The evidence</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="sources-of-genetic-variation-for-multilocus-traits.html"><a href="sources-of-genetic-variation-for-multilocus-traits.html#end-of-chapter-questions-2"><i class="fa fa-check"></i><b>5.5</b> End of chapter questions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="sources-of-environmental-variation.html"><a href="sources-of-environmental-variation.html"><i class="fa fa-check"></i><b>6</b> Sources of Environmental Variation</a>
<ul>
<li class="chapter" data-level="6.1" data-path="sources-of-environmental-variation.html"><a href="sources-of-environmental-variation.html#extension-of-the-linear-model-to-phenotypes"><i class="fa fa-check"></i><b>6.1</b> Extension of the linear model to phenotypes</a></li>
<li class="chapter" data-level="6.2" data-path="sources-of-environmental-variation.html"><a href="sources-of-environmental-variation.html#special-environmental-effects"><i class="fa fa-check"></i><b>6.2</b> Special environmental effects</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="sources-of-environmental-variation.html"><a href="sources-of-environmental-variation.html#within-individual-variation"><i class="fa fa-check"></i><b>6.2.1</b> Within-individual variation</a></li>
<li class="chapter" data-level="6.2.2" data-path="sources-of-environmental-variation.html"><a href="sources-of-environmental-variation.html#developmental-homeostasis-and-homozygosity"><i class="fa fa-check"></i><b>6.2.2</b> Developmental homeostasis and homozygosity</a></li>
<li class="chapter" data-level="6.2.3" data-path="sources-of-environmental-variation.html"><a href="sources-of-environmental-variation.html#repeatability"><i class="fa fa-check"></i><b>6.2.3</b> Repeatability</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="sources-of-environmental-variation.html"><a href="sources-of-environmental-variation.html#general-environmental-effects-of-maternal-origin"><i class="fa fa-check"></i><b>6.3</b> General environmental effects of maternal origin</a></li>
<li class="chapter" data-level="6.4" data-path="sources-of-environmental-variation.html"><a href="sources-of-environmental-variation.html#genotype-x-environment-interaction"><i class="fa fa-check"></i><b>6.4</b> Genotype x environment interaction</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="resemblance-between-relatives.html"><a href="resemblance-between-relatives.html"><i class="fa fa-check"></i><b>7</b> Resemblance between relatives</a>
<ul>
<li class="chapter" data-level="7.1" data-path="resemblance-between-relatives.html"><a href="resemblance-between-relatives.html#measures-of-relatedness"><i class="fa fa-check"></i><b>7.1</b> Measures of relatedness</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="resemblance-between-relatives.html"><a href="resemblance-between-relatives.html#coefficients-of-identity"><i class="fa fa-check"></i><b>7.1.1</b> Coefficients of identity</a></li>
<li class="chapter" data-level="7.1.2" data-path="resemblance-between-relatives.html"><a href="resemblance-between-relatives.html#coefficients-of-coancestry-and-inbreeding"><i class="fa fa-check"></i><b>7.1.2</b> Coefficients of coancestry and inbreeding</a></li>
<li class="chapter" data-level="7.1.3" data-path="resemblance-between-relatives.html"><a href="resemblance-between-relatives.html#the-coefficient-of-fraternity"><i class="fa fa-check"></i><b>7.1.3</b> The coefficient of fraternity</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="resemblance-between-relatives.html"><a href="resemblance-between-relatives.html#the-genetic-covariance-between-relatives"><i class="fa fa-check"></i><b>7.2</b> The genetic covariance between relatives</a></li>
<li class="chapter" data-level="7.3" data-path="resemblance-between-relatives.html"><a href="resemblance-between-relatives.html#the-effect-of-linkage-and-gametic-phase-disequilibrium"><i class="fa fa-check"></i><b>7.3</b> The effect of linkage and gametic phase disequilibrium</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="resemblance-between-relatives.html"><a href="resemblance-between-relatives.html#gametic-phase-disequilibrium"><i class="fa fa-check"></i><b>7.3.1</b> Gametic phase disequilibrium</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="resemblance-between-relatives.html"><a href="resemblance-between-relatives.html#assortative-mating"><i class="fa fa-check"></i><b>7.4</b> Assortative mating</a></li>
<li class="chapter" data-level="7.5" data-path="resemblance-between-relatives.html"><a href="resemblance-between-relatives.html#polyploidy-1"><i class="fa fa-check"></i><b>7.5</b> Polyploidy</a></li>
<li class="chapter" data-level="7.6" data-path="resemblance-between-relatives.html"><a href="resemblance-between-relatives.html#environmental-sources-of-covariance-between-relatives"><i class="fa fa-check"></i><b>7.6</b> Environmental sources of covariance between relatives</a></li>
<li class="chapter" data-level="7.7" data-path="resemblance-between-relatives.html"><a href="resemblance-between-relatives.html#the-heritability-concept"><i class="fa fa-check"></i><b>7.7</b> The heritability concept</a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="resemblance-between-relatives.html"><a href="resemblance-between-relatives.html#evolvability"><i class="fa fa-check"></i><b>7.7.1</b> Evolvability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html"><i class="fa fa-check"></i><b>8</b> Introduction to Matrix Algebra and Linear Models</a>
<ul>
<li class="chapter" data-level="8.1" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#multiple-regression"><i class="fa fa-check"></i><b>8.1</b> Multiple regression</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#an-application-to-multivariate-selection"><i class="fa fa-check"></i><b>8.1.1</b> An application to multivariate selection</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#elementary-matrix-algebra"><i class="fa fa-check"></i><b>8.2</b> Elementary matrix algebra</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#basic-notation"><i class="fa fa-check"></i><b>8.2.1</b> Basic notation</a></li>
<li class="chapter" data-level="8.2.2" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#partitioned-matrices"><i class="fa fa-check"></i><b>8.2.2</b> Partitioned matrices</a></li>
<li class="chapter" data-level="8.2.3" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#addition-and-subtraction"><i class="fa fa-check"></i><b>8.2.3</b> Addition and subtraction</a></li>
<li class="chapter" data-level="8.2.4" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#multiplication"><i class="fa fa-check"></i><b>8.2.4</b> Multiplication</a></li>
<li class="chapter" data-level="8.2.5" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#transposition"><i class="fa fa-check"></i><b>8.2.5</b> Transposition</a></li>
<li class="chapter" data-level="8.2.6" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#inverses-and-solutions-to-systems-of-equations"><i class="fa fa-check"></i><b>8.2.6</b> Inverses and solutions to systems of equations</a></li>
<li class="chapter" data-level="8.2.7" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#determinants-and-minors"><i class="fa fa-check"></i><b>8.2.7</b> Determinants and minors</a></li>
<li class="chapter" data-level="8.2.8" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#computing-inverses"><i class="fa fa-check"></i><b>8.2.8</b> Computing inverses</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="questions.html"><a href="questions.html"><i class="fa fa-check"></i>Questions</a>
<ul>
<li class="chapter" data-level="" data-path="questions.html"><a href="questions.html#chapter-4"><i class="fa fa-check"></i>Chapter 4</a></li>
<li class="chapter" data-level="" data-path="questions.html"><a href="questions.html#chapter-5"><i class="fa fa-check"></i>Chapter 5</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes for: Walsh and Lynch. Genetics and Analysis of Quantitative Traits</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction-to-matrix-algebra-and-linear-models" class="section level1" number="8">
<h1><span class="header-section-number">Chapter 8</span> Introduction to Matrix Algebra and Linear Models</h1>
<p><span class="math display">\[\newcommand{\mx}[1]{\mathbf{#1}}\]</span></p>
<div id="multiple-regression" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> Multiple regression</h2>
<p>Simple multiple regression equation:</p>
<p><span class="math display" id="eq:simple-multiple-regression">\[\begin{equation}
    y = \alpha + \beta_1z_1 + \beta_2z_2 + \dots + \beta_nz_n + e
    \tag{8.1}
\end{equation}\]</span></p>
<p><span class="math inline">\(y\)</span> = dependent/response variable, <span class="math inline">\(z_1, z_2, z_n\)</span> = predictors, <span class="math inline">\(e\)</span> = residual error, <span class="math inline">\(\alpha\)</span> is a constant as are <span class="math inline">\(\beta_1, \beta_2, \beta_n\)</span> to be estimated.</p>
<p>Recall from Chapter 3 that the goal of least-squares regression is to find a set of constants (<span class="math inline">\(\alpha\)</span> and the <span class="math inline">\(\beta\)</span>s) that minimise the squared differences between observed and expected values, with expected values is anything that fits on the “line of best fit”. Also, recall equation <a href="covariance-regression-and-correlation.html#eq:intercept-and-slope">(3.6)</a>, to see the relationship between <span class="math inline">\(y\)</span>, <span class="math inline">\(z_n\)</span> (<span class="math inline">\(x\)</span> in the equation) and <span class="math inline">\(b\)</span>. For multiple regression there are many “<span class="math inline">\(b\)</span>” terms and each of them can be estimated by dividing the covariance of the dependent variable and the predictor (<span class="math inline">\(\sigma(y, z_n)\)</span>) by the covariance of the predictor with all other predictors in the model. When <span class="math inline">\(n = 1\)</span>, the model reduces to a simple linear regression and we return to equation <a href="covariance-regression-and-correlation.html#eq:intercept-and-slope">(3.6)</a>. This can be represented in matrix form like so:</p>
<p><span class="math display" id="eq:multiple-regression-matrix-form">\[\begin{equation}
    \begin{pmatrix}
        \sigma^2(z_1) &amp; \sigma(z_1, z_n) &amp; \dots &amp; \sigma(z_1, z_n) \\
        \sigma(z_1, z_1) &amp; \sigma^2(z_2) &amp; \dots &amp; \sigma(z_2, z_n) \\
        \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
        \sigma(z_1, z_n) &amp; \sigma(z_2, z_n) &amp; \dots &amp; \sigma^2(z_n)
    \end{pmatrix}
    \begin{pmatrix}
        \beta_1 \\
        \beta_2 \\
        \vdots \\
        \beta_n
    \end{pmatrix}
    =
    \begin{pmatrix}
        \sigma(y, z_1) \\
        \sigma(y, z_2) \\
        \vdots \\
        \sigma(y, z_n)
    \end{pmatrix}
    \tag{8.2}
\end{equation}\]</span></p>
<p>When estimating each response variable-predictor covariance term, it is the sum of predictor covariance multiplied by beta.</p>
<p>If the covariance matrix and the vectors of <a href="introduction-to-matrix-algebra-and-linear-models.html#eq:multiple-regression-matrix-form">(8.2)</a> are written as <span class="math inline">\(\mx{V}\)</span>, <span class="math inline">\(\mx{\beta}\)</span> and <span class="math inline">\(\mx{c}\)</span> respectively, then the equation can be re-written as:</p>
<p><span class="math display" id="eq:abbreviated-multiple-regression">\[\begin{equation}
    \mx{V\beta} = \mx{c}
    \tag{8.3}
\end{equation}\]</span></p>
<p><strong>NOTE</strong>: It is standard procedure to denote matrices as bold capital letters and vectors as bold lower case letters.</p>
<p>Before going onto matrix methods in more detail, here is an application of <a href="introduction-to-matrix-algebra-and-linear-models.html#eq:simple-multiple-regression">(8.1)</a> in quantitative genetics.</p>
<div id="an-application-to-multivariate-selection" class="section level3" number="8.1.1">
<h3><span class="header-section-number">8.1.1</span> An application to multivariate selection</h3>
<p>Suppose that a large number of individuals in a population have been measured for <span class="math inline">\(n\)</span> characters and for fitness. Individual fitness can then be approximated by the linear model</p>
<p><span class="math display" id="eq:fitness-linear-model">\[\begin{equation}
    w = \alpha + \beta_1z_1 + \beta_2z_2 + ... + \beta_nz_n + e
    \tag{8.4}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(w\)</span> is the relative fitness (observed fitness divided by the mean fitness in the population). In Chapter 3, we learnt that the selection differential for the <span class="math inline">\(i\)</span>th trait is defined as the covariance between phenotype and relative fitness, <span class="math inline">\(S_i = \sigma(z_i, w)\)</span>. Therefore, if we use multiple regression to estimate <span class="math inline">\(S_i\)</span> we’d end up with:</p>
<p><span class="math display" id="eq:ith-selection-differential">\[\begin{equation}
    S_i = \beta_i\sigma^2(z_i) + \sum^n_{j \neq i} {\beta_j\sigma(z_i, z_j)}
    \tag{8.5} 
\end{equation}\]</span></p>
<p>Simple!</p>
</div>
</div>
<div id="elementary-matrix-algebra" class="section level2" number="8.2">
<h2><span class="header-section-number">8.2</span> Elementary matrix algebra</h2>
<div id="basic-notation" class="section level3" number="8.2.1">
<h3><span class="header-section-number">8.2.1</span> Basic notation</h3>
<p>Vectors and matrices in mathematics are just like those in R. A matrix with the same number of rows and columns is called a square matrix. Vectors written vertically are called column vectors, e.g. </p>
<p><span class="math display">\[\begin{equation}
    \mx{a} = 
    \begin{pmatrix}
        12 \\
        13 \\
        47
    \end{pmatrix}
    \notag
\end{equation}\]</span></p>
<p>and those that are written horizontally are called row vectors, e.g. </p>
<p><span class="math display">\[\begin{equation}
    \mx{b} = 
    \begin{pmatrix}
        12 &amp; 13 &amp; 47
    \end{pmatrix}
    \notag
\end{equation}\]</span></p>
<p>Single numbers by themselves are often referred to as scalars.</p>
<p>A matrix can be described by the elements that comprise it, with <span class="math inline">\(M_ij\)</span> denoting the element in the <span class="math inline">\(i\)</span>th row and <span class="math inline">\(j\)</span>th column of matrix <span class="math inline">\(\mx{M}\)</span>.</p>
</div>
<div id="partitioned-matrices" class="section level3" number="8.2.2">
<h3><span class="header-section-number">8.2.2</span> Partitioned matrices</h3>
<p>It is often useful to work with partitioned matrices wherein each element in a matrix is itself a matrix. There are several ways to partition a matrix, for example:</p>
<p><span class="math display">\[\begin{equation}
    \mx{c} = 
    \begin{pmatrix}
        3 &amp; 1 &amp; 2 \\
        2 &amp; 5 &amp; 4 \\
        1 &amp; 1 &amp; 2
    \end{pmatrix}
    =
    \begin{pmatrix}
        3 &amp; \vdots &amp; 1 &amp; 2 \\
        \dots &amp; \dots &amp; \dots &amp; \dots \\
        2 &amp; \vdots &amp; 5 &amp; 4 \\
        1 &amp; \vdots &amp; 1 &amp; 2
    \end{pmatrix}
    =
    \begin{pmatrix}
        \mx{a} &amp; \mx{b} \\
        \mx{d} &amp; \mx{B}
    \end{pmatrix}
    \notag
\end{equation}\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{equation}
    \mx{a} = 
    \begin{pmatrix}
        3
    \end{pmatrix}
    ,\  
    \mx{b} = 
    \begin{pmatrix}
        1 &amp; 2
    \end{pmatrix}
    ,\ 
    \mx{d} = 
    \begin{pmatrix}
        2 \\
        1
    \end{pmatrix}
    ,\ 
    \mx{B}
    \begin{pmatrix}
        5 &amp; 4 \\
        1 &amp; 2
    \end{pmatrix}
    \notag
\end{equation}\]</span></p>
<p>There are also other simple ways to partition matrices.</p>
</div>
<div id="addition-and-subtraction" class="section level3" number="8.2.3">
<h3><span class="header-section-number">8.2.3</span> Addition and subtraction</h3>
<p>To add matrices, they must have the same dimensions. Just add the corresponding elelments, so for adding matrices <span class="math inline">\(\mx{A}\)</span>, and <span class="math inline">\(\mx{B}\)</span> to make <span class="math inline">\(\mx{C}\)</span>, it is simply <span class="math inline">\(C_{ij} = A_{ij} + B_{ij}\)</span>. Subtraction is defined similarly.</p>
<p><strong>Can add examples here if you really want to</strong></p>
</div>
<div id="multiplication" class="section level3" number="8.2.4">
<h3><span class="header-section-number">8.2.4</span> Multiplication</h3>
<p>To multiply matrix <span class="math inline">\(\mx{M}\)</span> by scalar <span class="math inline">\(a\)</span>, then just multiply each element of <span class="math inline">\(\mx{M}\)</span> by <span class="math inline">\(a\)</span>.</p>
<p>Dot product of two vectors, <span class="math inline">\(\mx{a} \cdot \mx{b}\)</span>, is a scalar given by</p>
<p><span class="math display">\[\begin{equation}
    \mx{a} \cdot \mx{b} = \sum^n_{i=1} {a_ib_i} \notag
\end{equation}\]</span></p>
<p>For example, for the two vectors given by</p>
<p><span class="math display">\[\begin{equation}
    \mx{a} = 
    \begin{pmatrix}
        1 &amp; 2 &amp; 3 &amp; 4
    \end{pmatrix} 
\ \ \ \mathrm{and} \ \ \ 
    \mx{b} = 
    \begin{pmatrix}
        4 \\
        5 \\
        7 \\
        9
    \end{pmatrix}
    \notag
\end{equation}\]</span></p>
<p>the dot product <span class="math inline">\(\mx{a} \cdot \mx{b}\)</span> = (1 x 4) + (2 x 5) + (3 x 7) + (4 x 9) = 71. Dot product is not defined if the vectors have different lengths.</p>
<p>Now consider the matrix <span class="math inline">\(\mx{L} = \mx{M}\mx{N}\)</span> produced by multiplying the <span class="math inline">\(r\)</span> x <span class="math inline">\(c\)</span> matrix <span class="math inline">\(\mx{M}\)</span> by the <span class="math inline">\(c\)</span> x <span class="math inline">\(b\)</span> matrix <span class="math inline">\(\mx{N}\)</span>. Partitioning <span class="math inline">\(\mx{M}\)</span> as a column vector of <span class="math inline">\(r\)</span> row vectors,</p>
<p><span class="math display">\[\begin{equation}
    \mx{M} = 
    \begin{pmatrix}
        \mx{m_1} \\
        \mx{m_2} \\
        \vdots \\
        \mx{m_r}
    \end{pmatrix} 
    \ \ \ \mathrm{where}\ \ \ 
    \mx{m_i} = 
    \begin{pmatrix}
        M_{i1} &amp; M_{i2} &amp; \dots &amp; M_{ic}
    \end{pmatrix}
    \notag
\end{equation}\]</span></p>
<p>and <span class="math inline">\(\mx{N}\)</span> as a row vector of <span class="math inline">\(b\)</span> column vectors,</p>
<p><span class="math display">\[\begin{equation}
    \mx{N} = 
    \begin{pmatrix}
        \mx{n_1} &amp; \mx{n_2} &amp; \dots &amp; \mx{n_b}
    \end{pmatrix}
    \ \ \ \mathrm{where}\ \ \ 
    \mx{n_j} = 
    \begin{pmatrix}
        N_{1j} \\
        N_{2j} \\
        \vdots \\
        N_{cj} \\
    \end{pmatrix} 
    \notag
\end{equation}\]</span></p>
<p>the <span class="math inline">\(ij\)</span>th element of <span class="math inline">\(\mx{L}\)</span> is given by the dot product</p>
<p><span class="math display" id="eq:matrix-multiplication-L">\[\begin{equation}
    L_{ij} = \mx{m_i} \cdot \mx{n_j} = \sum^c_{k=1} {M_{ik}N_{kj}}
    \tag{8.6}
\end{equation}\]</span></p>
<p>Use this to write out matrix <span class="math inline">\(\mx{L}\)</span> – cba right now…</p>
<p>To be definied, the number of columns in the first matrix must equal the number of rows in the second matrix. This means that unless the two matrices are square, it is only possible to multiply them together one way round and not the other (e.g. <span class="math inline">\(\mx{M}\mx{N}\)</span> may be defined, but <span class="math inline">\(\mx{N}\mx{M}\)</span> won’t). Writing <span class="math inline">\(\mx{M}_{r \times c}\mx{N}_{c \times b} = \mx{L}_{r \times b}\)</span> shows the inner indices must match, while the outer indices give the number of rows and columns of the resulting matrix. Even if the matrices being multiplied are square, the order in which they are multiplied is important, i.e. multiplying <span class="math inline">\(\mx{A}\)</span> by <span class="math inline">\(\mx{B}\)</span> is not the same as <em>vice versa</em>. So there is terminology to help differentiate what is being multiplied by what. In the example just given, one would say matrix <span class="math inline">\(\mx{B}\)</span> is premultiplied by matrix <span class="math inline">\(\mx{A}\)</span>, or that matrix <span class="math inline">\(\mx{A}\)</span> is postmultiplied by matrix <span class="math inline">\(\mx{B}\)</span>.</p>
</div>
<div id="transposition" class="section level3" number="8.2.5">
<h3><span class="header-section-number">8.2.5</span> Transposition</h3>
<p>The transpose of a matrix <span class="math inline">\(\mx{A}\)</span> is written as <span class="math inline">\(\mx{A}^T\)</span> or <span class="math inline">\(\mx{A}&#39;\)</span>. It is obtained by simply switching the rows and columns of the matrix.</p>
<p><strong>Add example here if you want</strong></p>
<p>A useful identity is</p>
<p><span class="math display" id="eq:transpose-of-ABC">\[\begin{equation}
    (\mx{A}\mx{B}\mx{C})^T = \mx{C}^T\mx{B}^T\mx{A}^T
    \tag{8.7}
\end{equation}\]</span></p>
<p>which holds for any number of matrices.</p>
<p>Vectors tend to be written as column vectors, and therefore will be written as such henceforth, and as lowercase bold letters, e.g. <span class="math inline">\(\mx{a}\)</span>, for a column vector and <span class="math inline">\(\mx{a}^T\)</span> for the corresponding row vector. Further, when multiplying vectors we can assess the inner product (dot product), which yields a scalar and the outer product, which yields a matrix. For the two <span class="math inline">\(n\)</span>-dimensional column vectors <span class="math inline">\(\mx{a}\)</span> and <span class="math inline">\(\mx{b}\)</span>,</p>
<p><span class="math display">\[\begin{equation}
    \mx{a} = 
    \begin{pmatrix}
    a_1 \\
    \vdots \\
    a_n
    \end{pmatrix}
    \ \ \ 
    \mx{b} = 
    \begin{pmatrix}
    b_1 \\
    \vdots \\
    b_n
    \end{pmatrix}
    \notag
\end{equation}\]</span></p>
<p>the inner product is given by</p>
<p><span class="math display" id="eq:inner-product">\[\begin{equation}
    \begin{pmatrix}
    a_1 &amp; \dots &amp; a_n
    \end{pmatrix}
    \begin{pmatrix}
    b_1 \\
    \vdots \\
    b_n
    \end{pmatrix}
     
    = \mx{a}^T\mx{b} = \sum^n_{i=1} {a_ib_i}
    \tag{8.8}
\end{equation}\]</span></p>
<p>while the outer product yields an <span class="math inline">\(n \times n\)</span> matrix</p>
<p><span class="math display" id="eq:outer-product">\[\begin{equation}
    \begin{pmatrix}
        a_1 \\
        \vdots \\
        a_n
    \end{pmatrix}
    \begin{pmatrix}
        b_1 &amp; \dots &amp; b_n
    \end{pmatrix}

    = \mx{a}\mx{b}^T = 

    \begin{pmatrix}
        a_1b_1 &amp; a_1b_2 &amp; \dots &amp; a_1b_n \\
        a_2b_1 &amp; a_2b_2 &amp; \dots &amp; a_2b_n \\
        \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
        a_nb_1 &amp; a_nb_2 &amp; \dots &amp; a_nb_n
    \end{pmatrix}
    \tag{8.9}
\end{equation}\]</span></p>
</div>
<div id="inverses-and-solutions-to-systems-of-equations" class="section level3" number="8.2.6">
<h3><span class="header-section-number">8.2.6</span> Inverses and solutions to systems of equations</h3>
<p>Inverting a matrix is an operation similar to division of scalars. Multiplying an matrix by its inverse gives the identity matrix, <span class="math inline">\(\mx{I}\)</span>, which is a matrix where the diagonals are 1 and all other elements are 0. This plays the same role as the number 1 in scalar multiplication and division. The notation for the inverse of matrix <span class="math inline">\(\mx{A}\)</span> would be <span class="math inline">\(\mx{A}^{-1}\)</span> and <span class="math inline">\(\mx{A}\mx{A}^{-1} = \mx{I}\)</span>. This is useful for solving equations containing matrices, e.g. if you want to divide both sides of the equation by a matrix. A matrix is called nonsingular if its inverse exists. A useful property of inverses is that if the matrix product <span class="math inline">\(\mx{AB}\)</span> is a square matrix (where <span class="math inline">\(\mx{A}\)</span> and <span class="math inline">\(\mx{B}\)</span> are both square), then</p>
<p><span class="math display" id="eq:inverse-of-square-matrix-product">\[\begin{equation}
    (\mx{AB})^{-1} = \mx{B}^{-1}\mx{A}^{-1}
    \tag{8.10}
\end{equation}\]</span></p>
<p>When you have a non-square or singular matrix you can still obtain solutions to equations containing the matrix by using generalised inverses, but such solutions are not unique. See appendix 3 in the book for deets.</p>
<p>Recalling equation <a href="introduction-to-matrix-algebra-and-linear-models.html#eq:abbreviated-multiple-regression">(8.3)</a>, we can see that the solution (<span class="math inline">\(\mx{\beta}\)</span>) can be expressed as</p>
<p><span class="math display" id="eq:beta-multiple-regression">\[\begin{equation}
    \mx{\beta} = \mx{V}^{-1}\mx{c}
    \tag{8.11}
\end{equation}\]</span></p>
<p>Likewise, for the Pearson-Lande-Arnold regression giving the best linear prediction of fitness,</p>
<p><span class="math display" id="eq:beta-pearson-lande-arnold-regression">\[\begin{equation}
    \mx{\beta} = \mx{P}^{-1}\mx{s}
    \tag{8.12}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mx{P}\)</span> is the covariance matrix for phenotypic measures <span class="math inline">\(z_1, ..., z_n\)</span>, and <span class="math inline">\(\mx{s}\)</span> is the vector of selection differentials for the <span class="math inline">\(n\)</span> characters.</p>
<p>Before going on to the formal expression for inverting a matrix, let’s have a look at some sweet sweet examples of extreme, but hella useful cases that lead to simple expressions for the inverse, shall weeeeee? First up, if the matrix is diagonal (all off-diagonal elements are zero), then the matrix inverse is also diagonal, with <span class="math inline">\(\mx{A}^{-1}_{ii} = 1/A_{ii}\)</span>, i.e. just raise the diagonal elements to the power of negative one. Simple! Of course, if any diagonal elements are el zilcho, then it wonee work as you canee divide by zero (inverse is undefined).</p>
<p>Second, for any 2 x 2 matrix <span class="math inline">\(\mx{A}\)</span>,</p>
<p><span class="math display" id="eq:inverse-of-twobytwo-matrix">\[\begin{equation}
    \mx{A} = 
    \begin{pmatrix}
        a &amp; b \\
        c &amp; d
    \end{pmatrix}
    \ \ \ \mathrm{then}\ \ \ 
    \mx{A}^{-1}
    = 
    \frac{1} {ad - bc}
    \begin{pmatrix}
        d &amp; -b \\
        -c &amp; a
    \end{pmatrix}
    \tag{8.13}
\end{equation}\]</span></p>
<p>If <span class="math inline">\(ad = bc\)</span>, the inverse doesn’t exist as division by zero is undefined!</p>
</div>
<div id="determinants-and-minors" class="section level3" number="8.2.7">
<h3><span class="header-section-number">8.2.7</span> Determinants and minors</h3>
<p>F0r a <span class="math inline">\(2 \times 2\)</span> matrix, the quantity</p>
<p><span class="math display" id="eq:determinant-of-twobytwo-matrix">\[\begin{equation}
    |\mx{A}| = A_{11}A_{22} - A_{12}A_{21}
    \tag{8.14}
\end{equation}\]</span></p>
<p>is called the determinant, which more generally is denoted by det(<span class="math inline">\(\mx{A}\)</span>) or <span class="math inline">\(|\mx{A}|\)</span>. <span class="math inline">\(\mx{A}^{-1}\)</span> only exists for a square matrix <span class="math inline">\(\mx{A}\)</span> if <span class="math inline">\(|\mx{A}| \neq 0\)</span>. For square matrices greater than 2, the determinant is obtained recursively from the general expression</p>
<p><span class="math display" id="eq:general-determinant-equation">\[\begin{equation}
    |\mx{A}| = \sum^n_{j=1} {A_ij}(-1)^{i+j}|\mx{A}_{ij}|
    \tag{8.15}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(i\)</span> is any fixed row of the matrix and <span class="math inline">\(\mx{A}\)</span> and <span class="math inline">\(\mx{A}_{ij}\)</span> is a submatrix obtained by deleting the <span class="math inline">\(i\)</span>th row and <span class="math inline">\(j\)</span>th column from <span class="math inline">\(\mx{A}\)</span>. Such a submatrix is known as a minor. Essentially you should be left with some <span class="math inline">\(2 \times 2\)</span> matrices at the end of all this. If the matrix is a diagonal, then the determinant is simply the product of the diagonal elements of that matrix, i.e. if</p>
<p><span class="math display">\[\begin{equation}
    A_{ij} = 
    \begin{cases}
        a_i, &amp; i = j\\
        0, &amp; i \neq j
    \end{cases}
    \ \ \ \mathrm{then} \ \ \ 
    |\mx{A}| = \prod^n_{i=1}a_{i}
    \notag
\end{equation}\]</span></p>
</div>
<div id="computing-inverses" class="section level3" number="8.2.8">
<h3><span class="header-section-number">8.2.8</span> Computing inverses</h3>
<p>The general solution of a matrix inverse is</p>
<p><span class="math display" id="eq:solution-to-matrix-inversion">\[\begin{equation}
    A^{-1}_{ij} = \left[\frac{(-1)^{i+j}|\mx{A}_{ij}|} {|\mx{A}|} \right]^T
    \tag{8.16}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(A^{-1}_{ij}\)</span> denotes the <span class="math inline">\(ij\)</span>th element of <span class="math inline">\(\mx{A}^{-1}\)</span> and <span class="math inline">\(\mx{A}_{ij}\)</span> denotes the <span class="math inline">\(ij\)</span>th minor of <span class="math inline">\(\mx{A}\)</span>. From equation <a href="introduction-to-matrix-algebra-and-linear-models.html#eq:solution-to-matrix-inversion">(8.16)</a>, division by the determinant is required for inversion, therefore the determinant of a matrix must be nonzero if that matrix can be inverted. Thus, a matrix is singular if its determinant is zero. This occurs whenever a matrix contains a row (or column) that can be written as a weighted sum of any other rows (or columns). In the context of our linear model, <a href="introduction-to-matrix-algebra-and-linear-models.html#eq:multiple-regression-matrix-form">(8.2)</a>, this happens if one of the <span class="math inline">\(n\)</span> equations can be written as a combination of the others, a situation that is equivalent to there being <span class="math inline">\(n\)</span> unknowns but less than <span class="math inline">\(n\)</span> independent equations.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="resemblance-between-relatives.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="questions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["quant_genetics_book_notes.pdf", "quant_genetics_book_notes.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

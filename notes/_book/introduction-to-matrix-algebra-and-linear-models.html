<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Introduction to Matrix Algebra and Linear Models | Notes for: Walsh and Lynch. Genetics and Analysis of Quantitative Traits</title>
  <meta name="description" content="Chapter 8 Introduction to Matrix Algebra and Linear Models | Notes for: Walsh and Lynch. Genetics and Analysis of Quantitative Traits" />
  <meta name="generator" content="bookdown 0.18.1 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Introduction to Matrix Algebra and Linear Models | Notes for: Walsh and Lynch. Genetics and Analysis of Quantitative Traits" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Introduction to Matrix Algebra and Linear Models | Notes for: Walsh and Lynch. Genetics and Analysis of Quantitative Traits" />
  
  
  

<meta name="author" content="Thomas Battram" />


<meta name="date" content="2020-07-31" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="resemblance-between-relatives.html"/>
<link rel="next" href="questions.html"/>
<script src="libs/header-attrs-2.1.1/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="an-overview-of-quantitative-genetics.html"><a href="an-overview-of-quantitative-genetics.html"><i class="fa fa-check"></i><b>1</b> An overview of quantitative genetics</a></li>
<li class="chapter" data-level="2" data-path="properties-of-distributions.html"><a href="properties-of-distributions.html"><i class="fa fa-check"></i><b>2</b> Properties of distributions</a></li>
<li class="chapter" data-level="3" data-path="covariance-regression-and-correlation.html"><a href="covariance-regression-and-correlation.html"><i class="fa fa-check"></i><b>3</b> Covariance, regression, and correlation</a>
<ul>
<li class="chapter" data-level="3.1" data-path="covariance-regression-and-correlation.html"><a href="covariance-regression-and-correlation.html#covariance"><i class="fa fa-check"></i><b>3.1</b> Covariance</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="covariance-regression-and-correlation.html"><a href="covariance-regression-and-correlation.html#useful-identities-for-covariance"><i class="fa fa-check"></i><b>3.1.1</b> Useful identities for covariance</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="covariance-regression-and-correlation.html"><a href="covariance-regression-and-correlation.html#least-squares-linear-regression"><i class="fa fa-check"></i><b>3.2</b> Least squares linear regression</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="covariance-regression-and-correlation.html"><a href="covariance-regression-and-correlation.html#properties-of-least-squares"><i class="fa fa-check"></i><b>3.2.1</b> Properties of least squares</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="covariance-regression-and-correlation.html"><a href="covariance-regression-and-correlation.html#correlation"><i class="fa fa-check"></i><b>3.3</b> Correlation</a></li>
<li class="chapter" data-level="3.4" data-path="covariance-regression-and-correlation.html"><a href="covariance-regression-and-correlation.html#differential-selection-brief-intro"><i class="fa fa-check"></i><b>3.4</b> Differential selection (brief intro)</a></li>
<li class="chapter" data-level="3.5" data-path="covariance-regression-and-correlation.html"><a href="covariance-regression-and-correlation.html#correlation-between-genotype-and-phenotype-brief-intro"><i class="fa fa-check"></i><b>3.5</b> Correlation between genotype and phenotype (brief intro)</a></li>
<li class="chapter" data-level="3.6" data-path="covariance-regression-and-correlation.html"><a href="covariance-regression-and-correlation.html#end-of-chapter-questions"><i class="fa fa-check"></i><b>3.6</b> End of chapter questions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html"><i class="fa fa-check"></i><b>4</b> Properties of single loci</a>
<ul>
<li class="chapter" data-level="4.1" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#allele-and-genotype-frequencies"><i class="fa fa-check"></i><b>4.2</b> Allele and genotype frequencies</a></li>
<li class="chapter" data-level="4.3" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#the-transmission-of-genetic-information"><i class="fa fa-check"></i><b>4.3</b> The transmission of genetic information</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#the-hardy-weinberg-principle"><i class="fa fa-check"></i><b>4.3.1</b> The Hardy-Weinberg principle</a></li>
<li class="chapter" data-level="4.3.2" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#sex-linked-loci"><i class="fa fa-check"></i><b>4.3.2</b> Sex-linked loci</a></li>
<li class="chapter" data-level="4.3.3" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#polyploidy"><i class="fa fa-check"></i><b>4.3.3</b> Polyploidy</a></li>
<li class="chapter" data-level="4.3.4" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#age-structure"><i class="fa fa-check"></i><b>4.3.4</b> Age structure</a></li>
<li class="chapter" data-level="4.3.5" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#testing-for-hardy-weinberg-proportions"><i class="fa fa-check"></i><b>4.3.5</b> Testing for Hardy-Weinberg proportions</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#characterising-the-influence-of-a-locus-on-the-phenotype"><i class="fa fa-check"></i><b>4.4</b> Characterising the influence of a locus on the phenotype</a></li>
<li class="chapter" data-level="4.5" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#the-basis-of-dominance"><i class="fa fa-check"></i><b>4.5</b> The basis of dominance</a></li>
<li class="chapter" data-level="4.6" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#fishers-decomposition-of-the-genotypic-value"><i class="fa fa-check"></i><b>4.6</b> Fisher’s decomposition of the genotypic value</a></li>
<li class="chapter" data-level="4.7" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#partioning-the-genetic-variance."><i class="fa fa-check"></i><b>4.7</b> Partioning the genetic variance.</a></li>
<li class="chapter" data-level="4.8" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#additive-effects-average-excesses-and-breeding-values"><i class="fa fa-check"></i><b>4.8</b> Additive effects, average excesses and breeding values</a></li>
<li class="chapter" data-level="4.9" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#extensions-for-multiple-alleles-and-non-random-mating"><i class="fa fa-check"></i><b>4.9</b> Extensions for multiple alleles and non random mating</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#average-excess"><i class="fa fa-check"></i><b>4.9.1</b> Average excess</a></li>
<li class="chapter" data-level="4.9.2" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#additive-effects"><i class="fa fa-check"></i><b>4.9.2</b> Additive effects</a></li>
<li class="chapter" data-level="4.9.3" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#additive-genetic-variance"><i class="fa fa-check"></i><b>4.9.3</b> Additive genetic variance</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="properties-of-single-loci.html"><a href="properties-of-single-loci.html#end-of-chapter-questions-1"><i class="fa fa-check"></i><b>4.10</b> End of chapter questions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sources-of-genetic-variation-for-multilocus-traits.html"><a href="sources-of-genetic-variation-for-multilocus-traits.html"><i class="fa fa-check"></i><b>5</b> Sources of genetic variation for multilocus traits</a>
<ul>
<li class="chapter" data-level="5.1" data-path="sources-of-genetic-variation-for-multilocus-traits.html"><a href="sources-of-genetic-variation-for-multilocus-traits.html#epistasis"><i class="fa fa-check"></i><b>5.1</b> Epistasis</a></li>
<li class="chapter" data-level="5.2" data-path="sources-of-genetic-variation-for-multilocus-traits.html"><a href="sources-of-genetic-variation-for-multilocus-traits.html#a-general-least-squares-model-for-genetic-effects"><i class="fa fa-check"></i><b>5.2</b> A general least-squares model for genetic effects</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="sources-of-genetic-variation-for-multilocus-traits.html"><a href="sources-of-genetic-variation-for-multilocus-traits.html#extension-to-haploids-and-polyploids"><i class="fa fa-check"></i><b>5.2.1</b> Extension to haploids and polyploids</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="sources-of-genetic-variation-for-multilocus-traits.html"><a href="sources-of-genetic-variation-for-multilocus-traits.html#linkage"><i class="fa fa-check"></i><b>5.3</b> Linkage</a></li>
<li class="chapter" data-level="5.4" data-path="sources-of-genetic-variation-for-multilocus-traits.html"><a href="sources-of-genetic-variation-for-multilocus-traits.html#effect-of-disequilibrium-of-the-genetic-variance"><i class="fa fa-check"></i><b>5.4</b> Effect of disequilibrium of the genetic variance</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="sources-of-genetic-variation-for-multilocus-traits.html"><a href="sources-of-genetic-variation-for-multilocus-traits.html#the-evidence"><i class="fa fa-check"></i><b>5.4.1</b> The evidence</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="sources-of-genetic-variation-for-multilocus-traits.html"><a href="sources-of-genetic-variation-for-multilocus-traits.html#end-of-chapter-questions-2"><i class="fa fa-check"></i><b>5.5</b> End of chapter questions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="sources-of-environmental-variation.html"><a href="sources-of-environmental-variation.html"><i class="fa fa-check"></i><b>6</b> Sources of Environmental Variation</a>
<ul>
<li class="chapter" data-level="6.1" data-path="sources-of-environmental-variation.html"><a href="sources-of-environmental-variation.html#extension-of-the-linear-model-to-phenotypes"><i class="fa fa-check"></i><b>6.1</b> Extension of the linear model to phenotypes</a></li>
<li class="chapter" data-level="6.2" data-path="sources-of-environmental-variation.html"><a href="sources-of-environmental-variation.html#special-environmental-effects"><i class="fa fa-check"></i><b>6.2</b> Special environmental effects</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="sources-of-environmental-variation.html"><a href="sources-of-environmental-variation.html#within-individual-variation"><i class="fa fa-check"></i><b>6.2.1</b> Within-individual variation</a></li>
<li class="chapter" data-level="6.2.2" data-path="sources-of-environmental-variation.html"><a href="sources-of-environmental-variation.html#developmental-homeostasis-and-homozygosity"><i class="fa fa-check"></i><b>6.2.2</b> Developmental homeostasis and homozygosity</a></li>
<li class="chapter" data-level="6.2.3" data-path="sources-of-environmental-variation.html"><a href="sources-of-environmental-variation.html#repeatability"><i class="fa fa-check"></i><b>6.2.3</b> Repeatability</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="sources-of-environmental-variation.html"><a href="sources-of-environmental-variation.html#general-environmental-effects-of-maternal-origin"><i class="fa fa-check"></i><b>6.3</b> General environmental effects of maternal origin</a></li>
<li class="chapter" data-level="6.4" data-path="sources-of-environmental-variation.html"><a href="sources-of-environmental-variation.html#genotype-x-environment-interaction"><i class="fa fa-check"></i><b>6.4</b> Genotype x environment interaction</a></li>
<li class="chapter" data-level="6.5" data-path="sources-of-environmental-variation.html"><a href="sources-of-environmental-variation.html#end-of-chapter-questions-3"><i class="fa fa-check"></i><b>6.5</b> End of chapter questions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="resemblance-between-relatives.html"><a href="resemblance-between-relatives.html"><i class="fa fa-check"></i><b>7</b> Resemblance between relatives</a>
<ul>
<li class="chapter" data-level="7.1" data-path="resemblance-between-relatives.html"><a href="resemblance-between-relatives.html#measures-of-relatedness"><i class="fa fa-check"></i><b>7.1</b> Measures of relatedness</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="resemblance-between-relatives.html"><a href="resemblance-between-relatives.html#coefficients-of-identity"><i class="fa fa-check"></i><b>7.1.1</b> Coefficients of identity</a></li>
<li class="chapter" data-level="7.1.2" data-path="resemblance-between-relatives.html"><a href="resemblance-between-relatives.html#coefficients-of-coancestry-and-inbreeding"><i class="fa fa-check"></i><b>7.1.2</b> Coefficients of coancestry and inbreeding</a></li>
<li class="chapter" data-level="7.1.3" data-path="resemblance-between-relatives.html"><a href="resemblance-between-relatives.html#the-coefficient-of-fraternity"><i class="fa fa-check"></i><b>7.1.3</b> The coefficient of fraternity</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="resemblance-between-relatives.html"><a href="resemblance-between-relatives.html#the-genetic-covariance-between-relatives"><i class="fa fa-check"></i><b>7.2</b> The genetic covariance between relatives</a></li>
<li class="chapter" data-level="7.3" data-path="resemblance-between-relatives.html"><a href="resemblance-between-relatives.html#the-effect-of-linkage-and-gametic-phase-disequilibrium"><i class="fa fa-check"></i><b>7.3</b> The effect of linkage and gametic phase disequilibrium</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="resemblance-between-relatives.html"><a href="resemblance-between-relatives.html#gametic-phase-disequilibrium"><i class="fa fa-check"></i><b>7.3.1</b> Gametic phase disequilibrium</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="resemblance-between-relatives.html"><a href="resemblance-between-relatives.html#assortative-mating"><i class="fa fa-check"></i><b>7.4</b> Assortative mating</a></li>
<li class="chapter" data-level="7.5" data-path="resemblance-between-relatives.html"><a href="resemblance-between-relatives.html#polyploidy-1"><i class="fa fa-check"></i><b>7.5</b> Polyploidy</a></li>
<li class="chapter" data-level="7.6" data-path="resemblance-between-relatives.html"><a href="resemblance-between-relatives.html#environmental-sources-of-covariance-between-relatives"><i class="fa fa-check"></i><b>7.6</b> Environmental sources of covariance between relatives</a></li>
<li class="chapter" data-level="7.7" data-path="resemblance-between-relatives.html"><a href="resemblance-between-relatives.html#the-heritability-concept"><i class="fa fa-check"></i><b>7.7</b> The heritability concept</a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="resemblance-between-relatives.html"><a href="resemblance-between-relatives.html#evolvability"><i class="fa fa-check"></i><b>7.7.1</b> Evolvability</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="resemblance-between-relatives.html"><a href="resemblance-between-relatives.html#end-of-chapter-questions-4"><i class="fa fa-check"></i><b>7.8</b> End of chapter questions</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html"><i class="fa fa-check"></i><b>8</b> Introduction to Matrix Algebra and Linear Models</a>
<ul>
<li class="chapter" data-level="8.1" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#multiple-regression"><i class="fa fa-check"></i><b>8.1</b> Multiple regression</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#an-application-to-multivariate-selection"><i class="fa fa-check"></i><b>8.1.1</b> An application to multivariate selection</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#elementary-matrix-algebra"><i class="fa fa-check"></i><b>8.2</b> Elementary matrix algebra</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#basic-notation"><i class="fa fa-check"></i><b>8.2.1</b> Basic notation</a></li>
<li class="chapter" data-level="8.2.2" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#partitioned-matrices"><i class="fa fa-check"></i><b>8.2.2</b> Partitioned matrices</a></li>
<li class="chapter" data-level="8.2.3" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#addition-and-subtraction"><i class="fa fa-check"></i><b>8.2.3</b> Addition and subtraction</a></li>
<li class="chapter" data-level="8.2.4" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#multiplication"><i class="fa fa-check"></i><b>8.2.4</b> Multiplication</a></li>
<li class="chapter" data-level="8.2.5" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#transposition"><i class="fa fa-check"></i><b>8.2.5</b> Transposition</a></li>
<li class="chapter" data-level="8.2.6" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#inverses-and-solutions-to-systems-of-equations"><i class="fa fa-check"></i><b>8.2.6</b> Inverses and solutions to systems of equations</a></li>
<li class="chapter" data-level="8.2.7" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#determinants-and-minors"><i class="fa fa-check"></i><b>8.2.7</b> Determinants and minors</a></li>
<li class="chapter" data-level="8.2.8" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#computing-inverses"><i class="fa fa-check"></i><b>8.2.8</b> Computing inverses</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#expectations-of-random-vectors-and-matrices"><i class="fa fa-check"></i><b>8.3</b> Expectations of random vectors and matrices</a></li>
<li class="chapter" data-level="8.4" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#covariance-matrices-of-transformed-vectors"><i class="fa fa-check"></i><b>8.4</b> Covariance matrices of transformed vectors</a></li>
<li class="chapter" data-level="8.5" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>8.5</b> The multivariate normal distribution</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#properties-of-the-mvn"><i class="fa fa-check"></i><b>8.5.1</b> Properties of the MVN</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#overview-of-linear-models"><i class="fa fa-check"></i><b>8.6</b> Overview of linear models</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#ordinary-least-squares"><i class="fa fa-check"></i><b>8.6.1</b> Ordinary least squares</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="introduction-to-matrix-algebra-and-linear-models.html"><a href="introduction-to-matrix-algebra-and-linear-models.html#generalised-least-squares"><i class="fa fa-check"></i><b>8.7</b> Generalised least squares</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="questions.html"><a href="questions.html"><i class="fa fa-check"></i>Questions</a>
<ul>
<li class="chapter" data-level="" data-path="questions.html"><a href="questions.html#chapter-4"><i class="fa fa-check"></i>Chapter 4</a></li>
<li class="chapter" data-level="" data-path="questions.html"><a href="questions.html#chapter-5"><i class="fa fa-check"></i>Chapter 5</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes for: Walsh and Lynch. Genetics and Analysis of Quantitative Traits</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction-to-matrix-algebra-and-linear-models" class="section level1" number="8">
<h1><span class="header-section-number">Chapter 8</span> Introduction to Matrix Algebra and Linear Models</h1>
<p><span class="math display">\[\newcommand{\mx}[1]{\mathbf{#1}}\]</span></p>
<div id="multiple-regression" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> Multiple regression</h2>
<p>Simple multiple regression equation:</p>
<p><span class="math display" id="eq:simple-multiple-regression">\[\begin{equation}
    y = \alpha + \beta_1z_1 + \beta_2z_2 + \dots + \beta_nz_n + e
    \tag{8.1}
\end{equation}\]</span></p>
<p><span class="math inline">\(y\)</span> = dependent/response variable, <span class="math inline">\(z_1, z_2, z_n\)</span> = predictors, <span class="math inline">\(e\)</span> = residual error, <span class="math inline">\(\alpha\)</span> is a constant as are <span class="math inline">\(\beta_1, \beta_2, \beta_n\)</span> to be estimated.</p>
<p>Recall from Chapter 3 that the goal of least-squares regression is to find a set of constants (<span class="math inline">\(\alpha\)</span> and the <span class="math inline">\(\beta\)</span>s) that minimise the squared differences between observed and expected values, with expected values is anything that fits on the “line of best fit”. Also, recall equation <a href="covariance-regression-and-correlation.html#eq:intercept-and-slope">(3.6)</a>, to see the relationship between <span class="math inline">\(y\)</span>, <span class="math inline">\(z_n\)</span> (<span class="math inline">\(x\)</span> in the equation) and <span class="math inline">\(b\)</span>. For multiple regression there are many “<span class="math inline">\(b\)</span>” terms and each of them can be estimated by dividing the covariance of the dependent variable and the predictor (<span class="math inline">\(\sigma(y, z_n)\)</span>) by the covariance of the predictor with all other predictors in the model. When <span class="math inline">\(n = 1\)</span>, the model reduces to a simple linear regression and we return to equation <a href="covariance-regression-and-correlation.html#eq:intercept-and-slope">(3.6)</a>. This can be represented in matrix form like so:</p>
<p><span class="math display" id="eq:multiple-regression-matrix-form">\[\begin{equation}
    \begin{pmatrix}
        \sigma^2(z_1) &amp; \sigma(z_1, z_n) &amp; \dots &amp; \sigma(z_1, z_n) \\
        \sigma(z_1, z_1) &amp; \sigma^2(z_2) &amp; \dots &amp; \sigma(z_2, z_n) \\
        \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
        \sigma(z_1, z_n) &amp; \sigma(z_2, z_n) &amp; \dots &amp; \sigma^2(z_n)
    \end{pmatrix}
    \begin{pmatrix}
        \beta_1 \\
        \beta_2 \\
        \vdots \\
        \beta_n
    \end{pmatrix}
    =
    \begin{pmatrix}
        \sigma(y, z_1) \\
        \sigma(y, z_2) \\
        \vdots \\
        \sigma(y, z_n)
    \end{pmatrix}
    \tag{8.2}
\end{equation}\]</span></p>
<p>When estimating each response variable-predictor covariance term, it is the sum of predictor covariance multiplied by beta.</p>
<p>If the covariance matrix and the vectors of <a href="introduction-to-matrix-algebra-and-linear-models.html#eq:multiple-regression-matrix-form">(8.2)</a> are written as <span class="math inline">\(\mx{V}\)</span>, <span class="math inline">\(\mx{\beta}\)</span> and <span class="math inline">\(\mx{c}\)</span> respectively, then the equation can be re-written as:</p>
<p><span class="math display" id="eq:abbreviated-multiple-regression">\[\begin{equation}
    \mx{V\beta} = \mx{c}
    \tag{8.3}
\end{equation}\]</span></p>
<p><strong>NOTE</strong>: It is standard procedure to denote matrices as bold capital letters and vectors as bold lower case letters.</p>
<p>Before going onto matrix methods in more detail, here is an application of <a href="introduction-to-matrix-algebra-and-linear-models.html#eq:simple-multiple-regression">(8.1)</a> in quantitative genetics.</p>
<div id="an-application-to-multivariate-selection" class="section level3" number="8.1.1">
<h3><span class="header-section-number">8.1.1</span> An application to multivariate selection</h3>
<p>Suppose that a large number of individuals in a population have been measured for <span class="math inline">\(n\)</span> characters and for fitness. Individual fitness can then be approximated by the linear model</p>
<p><span class="math display" id="eq:fitness-linear-model">\[\begin{equation}
    w = \alpha + \beta_1z_1 + \beta_2z_2 + ... + \beta_nz_n + e
    \tag{8.4}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(w\)</span> is the relative fitness (observed fitness divided by the mean fitness in the population). In Chapter 3, we learnt that the selection differential for the <span class="math inline">\(i\)</span>th trait is defined as the covariance between phenotype and relative fitness, <span class="math inline">\(S_i = \sigma(z_i, w)\)</span>. Therefore, if we use multiple regression to estimate <span class="math inline">\(S_i\)</span> we’d end up with:</p>
<p><span class="math display" id="eq:ith-selection-differential">\[\begin{equation}
    S_i = \beta_i\sigma^2(z_i) + \sum^n_{j \neq i} {\beta_j\sigma(z_i, z_j)}
    \tag{8.5} 
\end{equation}\]</span></p>
<p>Simple!</p>
</div>
</div>
<div id="elementary-matrix-algebra" class="section level2" number="8.2">
<h2><span class="header-section-number">8.2</span> Elementary matrix algebra</h2>
<div id="basic-notation" class="section level3" number="8.2.1">
<h3><span class="header-section-number">8.2.1</span> Basic notation</h3>
<p>Vectors and matrices in mathematics are just like those in R. A matrix with the same number of rows and columns is called a square matrix. Vectors written vertically are called column vectors, e.g. </p>
<p><span class="math display">\[\begin{equation}
    \mx{a} = 
    \begin{pmatrix}
        12 \\
        13 \\
        47
    \end{pmatrix}
    \notag
\end{equation}\]</span></p>
<p>and those that are written horizontally are called row vectors, e.g. </p>
<p><span class="math display">\[\begin{equation}
    \mx{b} = 
    \begin{pmatrix}
        12 &amp; 13 &amp; 47
    \end{pmatrix}
    \notag
\end{equation}\]</span></p>
<p>Single numbers by themselves are often referred to as scalars.</p>
<p>A matrix can be described by the elements that comprise it, with <span class="math inline">\(M_ij\)</span> denoting the element in the <span class="math inline">\(i\)</span>th row and <span class="math inline">\(j\)</span>th column of matrix <span class="math inline">\(\mx{M}\)</span>.</p>
</div>
<div id="partitioned-matrices" class="section level3" number="8.2.2">
<h3><span class="header-section-number">8.2.2</span> Partitioned matrices</h3>
<p>It is often useful to work with partitioned matrices wherein each element in a matrix is itself a matrix. There are several ways to partition a matrix, for example:</p>
<p><span class="math display">\[\begin{equation}
    \mx{c} = 
    \begin{pmatrix}
        3 &amp; 1 &amp; 2 \\
        2 &amp; 5 &amp; 4 \\
        1 &amp; 1 &amp; 2
    \end{pmatrix}
    =
    \begin{pmatrix}
        3 &amp; \vdots &amp; 1 &amp; 2 \\
        \dots &amp; \dots &amp; \dots &amp; \dots \\
        2 &amp; \vdots &amp; 5 &amp; 4 \\
        1 &amp; \vdots &amp; 1 &amp; 2
    \end{pmatrix}
    =
    \begin{pmatrix}
        \mx{a} &amp; \mx{b} \\
        \mx{d} &amp; \mx{B}
    \end{pmatrix}
    \notag
\end{equation}\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{equation}
    \mx{a} = 
    \begin{pmatrix}
        3
    \end{pmatrix}
    ,\  
    \mx{b} = 
    \begin{pmatrix}
        1 &amp; 2
    \end{pmatrix}
    ,\ 
    \mx{d} = 
    \begin{pmatrix}
        2 \\
        1
    \end{pmatrix}
    ,\ 
    \mx{B}
    \begin{pmatrix}
        5 &amp; 4 \\
        1 &amp; 2
    \end{pmatrix}
    \notag
\end{equation}\]</span></p>
<p>There are also other simple ways to partition matrices.</p>
</div>
<div id="addition-and-subtraction" class="section level3" number="8.2.3">
<h3><span class="header-section-number">8.2.3</span> Addition and subtraction</h3>
<p>To add matrices, they must have the same dimensions. Just add the corresponding elelments, so for adding matrices <span class="math inline">\(\mx{A}\)</span>, and <span class="math inline">\(\mx{B}\)</span> to make <span class="math inline">\(\mx{C}\)</span>, it is simply <span class="math inline">\(C_{ij} = A_{ij} + B_{ij}\)</span>. Subtraction is defined similarly.</p>
<p><strong>Can add examples here if you really want to</strong></p>
</div>
<div id="multiplication" class="section level3" number="8.2.4">
<h3><span class="header-section-number">8.2.4</span> Multiplication</h3>
<p>To multiply matrix <span class="math inline">\(\mx{M}\)</span> by scalar <span class="math inline">\(a\)</span>, then just multiply each element of <span class="math inline">\(\mx{M}\)</span> by <span class="math inline">\(a\)</span>.</p>
<p>Dot product of two vectors, <span class="math inline">\(\mx{a} \cdot \mx{b}\)</span>, is a scalar given by</p>
<p><span class="math display">\[\begin{equation}
    \mx{a} \cdot \mx{b} = \sum^n_{i=1} {a_ib_i} \notag
\end{equation}\]</span></p>
<p>For example, for the two vectors given by</p>
<p><span class="math display">\[\begin{equation}
    \mx{a} = 
    \begin{pmatrix}
        1 &amp; 2 &amp; 3 &amp; 4
    \end{pmatrix} 
\ \ \ \mathrm{and} \ \ \ 
    \mx{b} = 
    \begin{pmatrix}
        4 \\
        5 \\
        7 \\
        9
    \end{pmatrix}
    \notag
\end{equation}\]</span></p>
<p>the dot product <span class="math inline">\(\mx{a} \cdot \mx{b}\)</span> = (1 x 4) + (2 x 5) + (3 x 7) + (4 x 9) = 71. Dot product is not defined if the vectors have different lengths.</p>
<p>Now consider the matrix <span class="math inline">\(\mx{L} = \mx{M}\mx{N}\)</span> produced by multiplying the <span class="math inline">\(r\)</span> x <span class="math inline">\(c\)</span> matrix <span class="math inline">\(\mx{M}\)</span> by the <span class="math inline">\(c\)</span> x <span class="math inline">\(b\)</span> matrix <span class="math inline">\(\mx{N}\)</span>. Partitioning <span class="math inline">\(\mx{M}\)</span> as a column vector of <span class="math inline">\(r\)</span> row vectors,</p>
<p><span class="math display">\[\begin{equation}
    \mx{M} = 
    \begin{pmatrix}
        \mx{m_1} \\
        \mx{m_2} \\
        \vdots \\
        \mx{m_r}
    \end{pmatrix} 
    \ \ \ \mathrm{where}\ \ \ 
    \mx{m_i} = 
    \begin{pmatrix}
        M_{i1} &amp; M_{i2} &amp; \dots &amp; M_{ic}
    \end{pmatrix}
    \notag
\end{equation}\]</span></p>
<p>and <span class="math inline">\(\mx{N}\)</span> as a row vector of <span class="math inline">\(b\)</span> column vectors,</p>
<p><span class="math display">\[\begin{equation}
    \mx{N} = 
    \begin{pmatrix}
        \mx{n_1} &amp; \mx{n_2} &amp; \dots &amp; \mx{n_b}
    \end{pmatrix}
    \ \ \ \mathrm{where}\ \ \ 
    \mx{n_j} = 
    \begin{pmatrix}
        N_{1j} \\
        N_{2j} \\
        \vdots \\
        N_{cj} \\
    \end{pmatrix} 
    \notag
\end{equation}\]</span></p>
<p>the <span class="math inline">\(ij\)</span>th element of <span class="math inline">\(\mx{L}\)</span> is given by the dot product</p>
<p><span class="math display" id="eq:matrix-multiplication-L">\[\begin{equation}
    L_{ij} = \mx{m_i} \cdot \mx{n_j} = \sum^c_{k=1} {M_{ik}N_{kj}}
    \tag{8.6}
\end{equation}\]</span></p>
<p>Use this to write out matrix <span class="math inline">\(\mx{L}\)</span> – cba right now…</p>
<p>To be definied, the number of columns in the first matrix must equal the number of rows in the second matrix. This means that unless the two matrices are square, it is only possible to multiply them together one way round and not the other (e.g. <span class="math inline">\(\mx{M}\mx{N}\)</span> may be defined, but <span class="math inline">\(\mx{N}\mx{M}\)</span> won’t). Writing <span class="math inline">\(\mx{M}_{r \times c}\mx{N}_{c \times b} = \mx{L}_{r \times b}\)</span> shows the inner indices must match, while the outer indices give the number of rows and columns of the resulting matrix. Even if the matrices being multiplied are square, the order in which they are multiplied is important, i.e. multiplying <span class="math inline">\(\mx{A}\)</span> by <span class="math inline">\(\mx{B}\)</span> is not the same as <em>vice versa</em>. So there is terminology to help differentiate what is being multiplied by what. In the example just given, one would say matrix <span class="math inline">\(\mx{B}\)</span> is premultiplied by matrix <span class="math inline">\(\mx{A}\)</span>, or that matrix <span class="math inline">\(\mx{A}\)</span> is postmultiplied by matrix <span class="math inline">\(\mx{B}\)</span>.</p>
</div>
<div id="transposition" class="section level3" number="8.2.5">
<h3><span class="header-section-number">8.2.5</span> Transposition</h3>
<p>The transpose of a matrix <span class="math inline">\(\mx{A}\)</span> is written as <span class="math inline">\(\mx{A}^T\)</span> or <span class="math inline">\(\mx{A}&#39;\)</span>. It is obtained by simply switching the rows and columns of the matrix.</p>
<p><strong>Add example here if you want</strong></p>
<p>A useful identity is</p>
<p><span class="math display" id="eq:transpose-of-ABC">\[\begin{equation}
    (\mx{A}\mx{B}\mx{C})^T = \mx{C}^T\mx{B}^T\mx{A}^T
    \tag{8.7}
\end{equation}\]</span></p>
<p>which holds for any number of matrices.</p>
<p>Vectors tend to be written as column vectors, and therefore will be written as such henceforth, and as lowercase bold letters, e.g. <span class="math inline">\(\mx{a}\)</span>, for a column vector and <span class="math inline">\(\mx{a}^T\)</span> for the corresponding row vector. Further, when multiplying vectors we can assess the inner product (dot product), which yields a scalar and the outer product, which yields a matrix. For the two <span class="math inline">\(n\)</span>-dimensional column vectors <span class="math inline">\(\mx{a}\)</span> and <span class="math inline">\(\mx{b}\)</span>,</p>
<p><span class="math display">\[\begin{equation}
    \mx{a} = 
    \begin{pmatrix}
    a_1 \\
    \vdots \\
    a_n
    \end{pmatrix}
    \ \ \ 
    \mx{b} = 
    \begin{pmatrix}
    b_1 \\
    \vdots \\
    b_n
    \end{pmatrix}
    \notag
\end{equation}\]</span></p>
<p>the inner product is given by</p>
<p><span class="math display" id="eq:inner-product">\[\begin{equation}
    \begin{pmatrix}
    a_1 &amp; \dots &amp; a_n
    \end{pmatrix}
    \begin{pmatrix}
    b_1 \\
    \vdots \\
    b_n
    \end{pmatrix}
     
    = \mx{a}^T\mx{b} = \sum^n_{i=1} {a_ib_i}
    \tag{8.8}
\end{equation}\]</span></p>
<p>while the outer product yields an <span class="math inline">\(n \times n\)</span> matrix</p>
<p><span class="math display" id="eq:outer-product">\[\begin{equation}
    \begin{pmatrix}
        a_1 \\
        \vdots \\
        a_n
    \end{pmatrix}
    \begin{pmatrix}
        b_1 &amp; \dots &amp; b_n
    \end{pmatrix}

    = \mx{a}\mx{b}^T = 

    \begin{pmatrix}
        a_1b_1 &amp; a_1b_2 &amp; \dots &amp; a_1b_n \\
        a_2b_1 &amp; a_2b_2 &amp; \dots &amp; a_2b_n \\
        \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
        a_nb_1 &amp; a_nb_2 &amp; \dots &amp; a_nb_n
    \end{pmatrix}
    \tag{8.9}
\end{equation}\]</span></p>
</div>
<div id="inverses-and-solutions-to-systems-of-equations" class="section level3" number="8.2.6">
<h3><span class="header-section-number">8.2.6</span> Inverses and solutions to systems of equations</h3>
<p>Inverting a matrix is an operation similar to division of scalars. Multiplying an matrix by its inverse gives the identity matrix, <span class="math inline">\(\mx{I}\)</span>, which is a matrix where the diagonals are 1 and all other elements are 0. This plays the same role as the number 1 in scalar multiplication and division. The notation for the inverse of matrix <span class="math inline">\(\mx{A}\)</span> would be <span class="math inline">\(\mx{A}^{-1}\)</span> and <span class="math inline">\(\mx{A}\mx{A}^{-1} = \mx{I}\)</span>. This is useful for solving equations containing matrices, e.g. if you want to divide both sides of the equation by a matrix. A matrix is called nonsingular if its inverse exists. A useful property of inverses is that if the matrix product <span class="math inline">\(\mx{AB}\)</span> is a square matrix (where <span class="math inline">\(\mx{A}\)</span> and <span class="math inline">\(\mx{B}\)</span> are both square), then</p>
<p><span class="math display" id="eq:inverse-of-square-matrix-product">\[\begin{equation}
    (\mx{AB})^{-1} = \mx{B}^{-1}\mx{A}^{-1}
    \tag{8.10}
\end{equation}\]</span></p>
<p>When you have a non-square or singular matrix you can still obtain solutions to equations containing the matrix by using generalised inverses, but such solutions are not unique. See appendix 3 in the book for deets.</p>
<p>Recalling equation <a href="introduction-to-matrix-algebra-and-linear-models.html#eq:abbreviated-multiple-regression">(8.3)</a>, we can see that the solution (<span class="math inline">\(\mx{\beta}\)</span>) can be expressed as</p>
<p><span class="math display" id="eq:beta-multiple-regression">\[\begin{equation}
    \mx{\beta} = \mx{V}^{-1}\mx{c}
    \tag{8.11}
\end{equation}\]</span></p>
<p>Likewise, for the Pearson-Lande-Arnold regression giving the best linear prediction of fitness,</p>
<p><span class="math display" id="eq:beta-pearson-lande-arnold-regression">\[\begin{equation}
    \mx{\beta} = \mx{P}^{-1}\mx{s}
    \tag{8.12}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mx{P}\)</span> is the covariance matrix for phenotypic measures <span class="math inline">\(z_1, ..., z_n\)</span>, and <span class="math inline">\(\mx{s}\)</span> is the vector of selection differentials for the <span class="math inline">\(n\)</span> characters.</p>
<p>Before going on to the formal expression for inverting a matrix, let’s have a look at some sweet sweet examples of extreme, but hella useful cases that lead to simple expressions for the inverse, shall weeeeee? First up, if the matrix is diagonal (all off-diagonal elements are zero), then the matrix inverse is also diagonal, with <span class="math inline">\(\mx{A}^{-1}_{ii} = 1/A_{ii}\)</span>, i.e. just raise the diagonal elements to the power of negative one. Simple! Of course, if any diagonal elements are el zilcho, then it wonee work as you canee divide by zero (inverse is undefined).</p>
<p>Second, for any 2 x 2 matrix <span class="math inline">\(\mx{A}\)</span>,</p>
<p><span class="math display" id="eq:inverse-of-twobytwo-matrix">\[\begin{equation}
    \mx{A} = 
    \begin{pmatrix}
        a &amp; b \\
        c &amp; d
    \end{pmatrix}
    \ \ \ \mathrm{then}\ \ \ 
    \mx{A}^{-1}
    = 
    \frac{1} {ad - bc}
    \begin{pmatrix}
        d &amp; -b \\
        -c &amp; a
    \end{pmatrix}
    \tag{8.13}
\end{equation}\]</span></p>
<p>If <span class="math inline">\(ad = bc\)</span>, the inverse doesn’t exist as division by zero is undefined!</p>
</div>
<div id="determinants-and-minors" class="section level3" number="8.2.7">
<h3><span class="header-section-number">8.2.7</span> Determinants and minors</h3>
<p>F0r a <span class="math inline">\(2 \times 2\)</span> matrix, the quantity</p>
<p><span class="math display" id="eq:determinant-of-twobytwo-matrix">\[\begin{equation}
    |\mx{A}| = A_{11}A_{22} - A_{12}A_{21}
    \tag{8.14}
\end{equation}\]</span></p>
<p>is called the determinant, which more generally is denoted by det(<span class="math inline">\(\mx{A}\)</span>) or <span class="math inline">\(|\mx{A}|\)</span>. <span class="math inline">\(\mx{A}^{-1}\)</span> only exists for a square matrix <span class="math inline">\(\mx{A}\)</span> if <span class="math inline">\(|\mx{A}| \neq 0\)</span>. For square matrices greater than 2, the determinant is obtained recursively from the general expression</p>
<p><span class="math display" id="eq:general-determinant-equation">\[\begin{equation}
    |\mx{A}| = \sum^n_{j=1} {A_ij}(-1)^{i+j}|\mx{A}_{ij}|
    \tag{8.15}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(i\)</span> is any fixed row of the matrix and <span class="math inline">\(\mx{A}\)</span> and <span class="math inline">\(\mx{A}_{ij}\)</span> is a submatrix obtained by deleting the <span class="math inline">\(i\)</span>th row and <span class="math inline">\(j\)</span>th column from <span class="math inline">\(\mx{A}\)</span>. Such a submatrix is known as a minor. Essentially you should be left with some <span class="math inline">\(2 \times 2\)</span> matrices at the end of all this. If the matrix is a diagonal, then the determinant is simply the product of the diagonal elements of that matrix, i.e. if</p>
<p><span class="math display">\[\begin{equation}
    A_{ij} = 
    \begin{cases}
        a_i, &amp; i = j\\
        0, &amp; i \neq j
    \end{cases}
    \ \ \ \mathrm{then} \ \ \ 
    |\mx{A}| = \prod^n_{i=1}a_{i}
    \notag
\end{equation}\]</span></p>
</div>
<div id="computing-inverses" class="section level3" number="8.2.8">
<h3><span class="header-section-number">8.2.8</span> Computing inverses</h3>
<p>The general solution of a matrix inverse is</p>
<p><span class="math display" id="eq:solution-to-matrix-inversion">\[\begin{equation}
    A^{-1}_{ij} = \left[\frac{(-1)^{i+j}|\mx{A}_{ij}|} {|\mx{A}|} \right]^T
    \tag{8.16}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(A^{-1}_{ij}\)</span> denotes the <span class="math inline">\(ij\)</span>th element of <span class="math inline">\(\mx{A}^{-1}\)</span> and <span class="math inline">\(\mx{A}_{ij}\)</span> denotes the <span class="math inline">\(ij\)</span>th minor of <span class="math inline">\(\mx{A}\)</span>. From equation <a href="introduction-to-matrix-algebra-and-linear-models.html#eq:solution-to-matrix-inversion">(8.16)</a>, division by the determinant is required for inversion, therefore the determinant of a matrix must be nonzero if that matrix can be inverted. Thus, a matrix is singular if its determinant is zero. This occurs whenever a matrix contains a row (or column) that can be written as a weighted sum of any other rows (or columns). In the context of our linear model, <a href="introduction-to-matrix-algebra-and-linear-models.html#eq:multiple-regression-matrix-form">(8.2)</a>, this happens if one of the <span class="math inline">\(n\)</span> equations can be written as a combination of the others, a situation that is equivalent to there being <span class="math inline">\(n\)</span> unknowns but less than <span class="math inline">\(n\)</span> independent equations.</p>
</div>
</div>
<div id="expectations-of-random-vectors-and-matrices" class="section level2" number="8.3">
<h2><span class="header-section-number">8.3</span> Expectations of random vectors and matrices</h2>
<p>Matrix algebra provides a powerful approach for analysing linear combinations of random variables. Let <span class="math inline">\(\mx{x}\)</span> be a column vector containing <span class="math inline">\(n\)</span> random variables, <span class="math inline">\(\mx{x} = (x_1, x_2, \dots, x_n)^T\)</span>. We may want to construct a new univariate (scalar) random variable <span class="math inline">\(y\)</span> by taking some linear combination of the elements of <span class="math inline">\(x\)</span>,</p>
<p><span class="math display">\[\begin{equation}
    y = \sum^n_{i=1} {a_ix_i} = \mx{a}^T \mx{x}
    \notag
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mx{a} = (a_1, a_2, \dots, a_n)^T\)</span> is a column vector of constants. Good example of this is when we want to make a weighted genetic score. Likewise, we can construct a new <span class="math inline">\(k\)</span>-dimensional vector <span class="math inline">\(\mx{y}\)</span> by premultiplying <span class="math inline">\(\mx{x}\)</span> by a <span class="math inline">\(k \times n\)</span> matrix <span class="math inline">\(\mx{A}\)</span> of constants, <span class="math inline">\(\mx{y} = \mx{A}\mx{x}\)</span>. More generally, an (<span class="math inline">\(n \times k\)</span>) matrix <span class="math inline">\(\mx{X}\)</span> of random variables can be transformed into a new <span class="math inline">\(m \times l\)</span> dimensional matrix <span class="math inline">\(\mx{Y}\)</span> of elements consisting of linear combinations of the elements of <span class="math inline">\(\mx{X}\)</span> by</p>
<p><span class="math display" id="eq:transforming-matrix-of-random-variables">\[\begin{equation}
    \mx{Y}_{m \times l} = \mx{A}_{m \times n} \mx{X}_{n \times k} \mx{B}_{k \times l}
    \tag{8.17}
\end{equation}\]</span></p>
<p>where the matrices <span class="math inline">\(\mx{A}\)</span> and <span class="math inline">\(\mx{B}\)</span> are constants with dimensions as subscripted.</p>
<p>If <span class="math inline">\(\mx{X}\)</span> is full of random variables, the expected value of <span class="math inline">\(\mx{X}\)</span> is <span class="math inline">\(E(\mx{X})\)</span> containing the expected value of each element of <span class="math inline">\(\mx{X}\)</span>. If <span class="math inline">\(\mx{X}\)</span> and <span class="math inline">\(\mx{Z}\)</span> are matrices of the same dimension, then</p>
<p><span class="math display" id="eq:matrices-addition-expectations">\[\begin{equation}
    E(\mx{X} + \mx{Z}) = E(\mx{X}) + E(\mx{Z})
    \tag{8.18}
\end{equation}\]</span></p>
<p>Similarly, just remember other rules of expectations to obtain an expression for the expectation of <span class="math inline">\(\mx{Y}\)</span> from <a href="introduction-to-matrix-algebra-and-linear-models.html#eq:transforming-matrix-of-random-variables">(8.17)</a>. The important point is of course that the dimensions of matrices need to match up!</p>
</div>
<div id="covariance-matrices-of-transformed-vectors" class="section level2" number="8.4">
<h2><span class="header-section-number">8.4</span> Covariance matrices of transformed vectors</h2>
<p>If we have an <span class="math inline">\(n \times n\)</span> square matrix <span class="math inline">\(\mx{A}\)</span> and an <span class="math inline">\(n \times 1\)</span> column vector <span class="math inline">\(\mx{x}\)</span>, then:</p>
<p><span class="math display" id="eq:matrix-columnvec-multiplication">\[\begin{equation}
    \mx{x}^T\mx{Ax} = \sum^n_{i=1} \sum^n_{j=1} {a_{ij}}x_ix_j
    \tag{8.19}
\end{equation}\]</span></p>
<p>This expression is called a quadratic form (or quadratic product) and yield a scalar. A generalisation of a quadratic form is the bilinear form, <span class="math inline">\(\mx{b^TAa}\)</span>, where <span class="math inline">\(\mx{b}\)</span> and <span class="math inline">\(\mx{a}\)</span> are, respectively <span class="math inline">\(n \times 1\)</span> and <span class="math inline">\(m \times 1\)</span> column vectors and <span class="math inline">\(\mx{A}\)</span> is an <span class="math inline">\(n \times m\)</span> matrix. Index the matrices and you can tell what the resulting product is! Hint: it’s a scaler, figure out why! As scalars, bilinear forms equal their transposes, giving the useful identity:</p>
<p><span class="math display" id="eq:bilinear-form">\[\begin{equation}
    \mx{b^TAa} = (\mx{b^TAa})^T = \mx{a^TA^Tb}
    \tag{8.20}
\end{equation}\]</span></p>
<p>If <span class="math inline">\(\mx{x}\)</span> is a vector of <span class="math inline">\(n\)</span> random variables, then you can express the <span class="math inline">\(n\)</span> variances and <span class="math inline">\(n(n-1)/2\)</span> covariances associated with the elements of <span class="math inline">\(x\)</span> as the matrix <span class="math inline">\(\mx{V}\)</span>, where <span class="math inline">\(V_{ij} = \sigma(x_i, x_j)\)</span> is the covariance between the random variables <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span>. This is a covariance matrix (or variance-covariance matrix)! Remember: diagonals = variances and off-diagonals = covariances! The <span class="math inline">\(\mx{V}\)</span> matrix is symmetric such that:</p>
<p><span class="math display">\[\begin{equation}
    V_{ij} = \sigma(x_i, x_j) = \sigma(x_j, x_i) = V_{ji}
    \notag
\end{equation}\]</span></p>
<p>If we have a univariate random variable <span class="math inline">\(y = \sum{c_{k}x_{k}}\)</span> generated from a linear combination of the elements of <span class="math inline">\(\mx{x}\)</span>, in matrix notation we have <span class="math inline">\(y = \mx{c}^T\mx{x}\)</span>, where <span class="math inline">\(\mx{c}\)</span> is a column vector of constants. The variance of <span class="math inline">\(y\)</span> can be expressed as a quadratic form involving the covariance matrix <span class="math inline">\(\mx{V}\)</span> for the elements of <span class="math inline">\(\mx{x}\)</span>,</p>
<p><span class="math display" id="eq:variance-of-univariate-random-variable-y">\[\begin{equation}
    \mx{c}^T\mx{V}\mx{c}
    \tag{8.21}
\end{equation}\]</span></p>
<p>(See book for derivation, <a href="introduction-to-matrix-algebra-and-linear-models.html#eq:variance-of-univariate-random-variable-y">(8.21)</a> is the final result)</p>
<p>Likewise, the covariance between two univariate random variables created from different linear combinations of <span class="math inline">\(\mx{x}\)</span> is given by the bilinear form</p>
<p><span class="math display" id="eq:covariance-of-univariate-random-variables-created-from-x">\[\begin{equation}
    \sigma(\mx{a^Tx}, \mx{b^Tx}) = \mx{a^TVb}
    \tag{8.22}
\end{equation}\]</span></p>
<p>If we transform <span class="math inline">\(\mx{x}\)</span> to two new vectors <span class="math inline">\(\mx{y}_{x \times 1} = \mx{A}_{l \times n}\mx{x}_{n \times 1}\)</span> and <span class="math inline">\(\mx{z}_{m \times 1} = \mx{B}_{m \times n}\mx{x}_{n \times 1}\)</span>, then instead of a single covariance we have an <span class="math inline">\(l \times m\)</span> dimensional covariance matrix, denoted <span class="math inline">\(\mx{\sigma(y, z)}\)</span>. Letting <span class="math inline">\(\mx{\mu_{y}} = \mx{A_\mu}\)</span> and <span class="math inline">\(\mx{\mu_{z}} = \mx{B_\mu}\)</span>, with <span class="math inline">\(E(\mx{x}) = \mu\)</span>, then <span class="math inline">\(\mx{\sigma(y, z)}\)</span> can be expressed in terms of <span class="math inline">\(\mx{V}\)</span>, the covariance matrix of <span class="math inline">\(\mx{x}\)</span>,</p>
<p><span class="math display" id="eq:covariance-of-two-vectors">\[\begin{equation}
\begin{split}
    \mx{\sigma(y, z)} &amp;= \mx{\sigma(Ax, Bx)} \notag \\
    &amp;= E\left[(\mx{y} - \mx{\mu_{y}})(\mx{z} - \mx{\mu_{z}})^T \right] \notag \\
    &amp;= E\left[\mx{A}(\mx{x} - \mx{\mu})(\mx{x} - \mx{\mu})^T \mx{B}^T \right] \notag \\
    &amp;= \mx{AVB}^T 
\end{split}
    \tag{8.23}
\end{equation}\]</span></p>
<p>In particular, the covariance matrix for <span class="math inline">\(\mx{y} = \mx{Ax}\)</span> is</p>
<p><span class="math display" id="eq:covariance-matrix-of-y-equals-Ax">\[\begin{equation}
    \mx{\sigma(y, y)} = \mx{AVA}^T
    \tag{8.24}
\end{equation}\]</span></p>
<p>so that the covariance between <span class="math inline">\(y_i\)</span> and <span class="math inline">\(y_\)</span> is given by the <span class="math inline">\(ij\)</span>th element of the matrix product <span class="math inline">\(\mx{AVA}^T\)</span>. Finally, note that if <span class="math inline">\(\mx{x}\)</span> is a vector of random variables with expected value <span class="math inline">\(\mu\)</span>, then the expected value of the scalar quadratic product <span class="math inline">\(\mx{x}^T\mx{Ax}\)</span> is</p>
<p><span class="math display" id="eq:no-idea-what-to-call-this-eq">\[\begin{equation}
    \mx{x}^T\mx{Ax} = tr(\mx{AV}) + \mx{\mu}^T\mx{A\mu}
    \tag{8.25}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mx{V}\)</span> is the covariance matrix for the elements of <span class="math inline">\(\mx{x}\)</span>, and the trace of a square matrix, <span class="math inline">\(tr(\mx{M}) = \sum{M_{ii}}\)</span> is the sum of its diagonal values.</p>
</div>
<div id="the-multivariate-normal-distribution" class="section level2" number="8.5">
<h2><span class="header-section-number">8.5</span> The multivariate normal distribution</h2>
<p>Multivariate normal distribution = MVN.</p>
<p>Consider the probability density function for <span class="math inline">\(n\)</span> independent normal random variables, where <span class="math inline">\(x_i\)</span> is normally distributed with mean <span class="math inline">\(\mu_i\)</span> and variance <span class="math inline">\(\sigma^2_i\)</span>. In this case, because the variables are independent, the joint probability density function is simply the product of each univariate density,</p>
<p><span class="math display" id="eq:joint-prob-density-function-independent-vars">\[\begin{equation}
\begin{split}
    p(\mx{x}) &amp;= \prod^{n}_{i=1} {(2\pi)^{-1/2} \sigma^{-1}_{i} \mathrm{exp} \left(-\frac{(x_i - \mu_i)^2} {2\sigma^2_i}\right)}
    &amp;= (2\pi)^{-n/2} \left(\prod^{n}_{i=1} {\sigma_i}\right)^{-1} \mathrm{exp} \left(- \sum^n_{i=1} \frac{(x_i - \mu_i)^2} {2\sigma^2_i}\right)
\end{split}
    \tag{8.26}
\end{equation}\]</span></p>
<p>Let’s express this in matrix form, because we’re cool! (also it compacts things a lot). To do this, we need to defin these matrices</p>
<p><span class="math display">\[\begin{equation}
    \mx{V} = 
    \begin{pmatrix}
        \sigma^2_1 &amp; 0 &amp; \dots &amp; 0 \\
        0 &amp; \sigma^2_2 &amp; \dots &amp; 0 \\
        \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
        0 &amp; 0 &amp; 0 &amp; \sigma^2_n
    \end{pmatrix}
    \ \ \ \mathrm{and}\ \ \
    \mx{\mu} = 
    \begin{pmatrix}
        \mu_1 \\
        \mu_2 \\
        \vdots \\
        \mu_n
    \end{pmatrix}
    \notag
\end{equation}\]</span></p>
<p>Since <span class="math inline">\(\mx{V}\)</span> is diagonal, its determinant is simply the product of the diagonal elements</p>
<p><span class="math display">\[\begin{equation}
    |\mx{V}| = \prod^n_{i=1}\sigma^2_i
    \notag
\end{equation}\]</span></p>
<p>Likewise, using quadratic products, note that</p>
<p><span class="math display">\[\begin{equation}
    \sum^n_{i=1} \frac{(x_i - \mu_i)^2} {\sigma^2_i} = (\mx{x} - \mx{\mu}^T)\mx{V}^{-1} (\mx{x} - \mx{\mu})
    \notag
\end{equation}\]</span></p>
<p>putting these together, equation <a href="introduction-to-matrix-algebra-and-linear-models.html#eq:joint-prob-density-function-independent-vars">(8.26)</a> can be written as</p>
<p><span class="math display" id="eq:joint-prob-density-function-independent-vars-as-matrices">\[\begin{equation}
    p(\mx{x}) = (2\pi)^{-n/2} |\mx{V}|^{-1/2} \mathrm{exp} \left[-\frac{1}{2} (\mx{x} - \mx{\mu})^{T}\mx{V}^{-1}(\mx{x} - \mx{\mu})\right]
    \tag{8.27}
\end{equation}\]</span></p>
<p>This can also be written as <span class="math inline">\(p(\mx{x}, \mx{\mu}, \mx{V}\)</span> to stress that it is a function of the mean vector <span class="math inline">\(\mx{\mu}\)</span> and the covariance matrix <span class="math inline">\(\mx{V}\)</span>.</p>
<p>More generally, when the elements of <span class="math inline">\(\mx{x}\)</span> are correlated, equation <a href="introduction-to-matrix-algebra-and-linear-models.html#eq:joint-prob-density-function-independent-vars-as-matrices">(8.27)</a> gives the probability density function for a vector of multivariate normally distributed random variables, with mean vector <span class="math inline">\(\mx{\mu}\)</span> and covariance matrix <span class="math inline">\(\mx{V}\)</span>. We denote this by</p>
<p><span class="math display">\[\begin{equation}
    \mx{x} \sim \mathrm{MVN_n}(\mx{\mu, V})
    \notag
\end{equation}\]</span></p>
<p>where the subscript indicating the dimensionality of <span class="math inline">\(\mx{x}\)</span> is usually omitted. MVN also called Gaussian distribution.</p>
<div id="properties-of-the-mvn" class="section level3" number="8.5.1">
<h3><span class="header-section-number">8.5.1</span> Properties of the MVN</h3>
<p>Like the univariate normal distribution, the MVN is expected to arise naturally when the trait of interest result from a large number of underlying variables, so fits many human traits! Prepare your butts for some useful properties of MVN, because here they come!!!</p>
<ol style="list-style-type: decimal">
<li><p>If <span class="math inline">\(\mx{x} \sim \mathrm{MVN}\)</span>, then the distribution of any subset of the variables in <span class="math inline">\(\mx{x}\)</span> is also MVN. For example, each <span class="math inline">\(x_i\)</span> is normally distributed and each pair <span class="math inline">\((x_i, x_j)\)</span> is bivariate normally distributed.</p></li>
<li><p>If <span class="math inline">\(\mx{x} \sim \mathrm{MVN}\)</span>, then any linear combination of the elements of <span class="math inline">\(\mx{x}\)</span> is also MVN. Specifically, if <span class="math inline">\(\mx{x} \sim \mathrm{MVN_n}(\mx{\mu}, \mx{V})\)</span>, <span class="math inline">\(\mx{a}\)</span> is a vector of constants, and <span class="math inline">\(\mx{A}\)</span> is a matrix of constants, then</p></li>
</ol>
<p><span class="math display">\[\begin{equation}
    \mathrm{for}\ \ \ \mx{y} = \mx{x} + \mx{a}, \ \ \ \ \ \mx{y} \sim \mathrm{MVN_n}(\mx{\mu} + \mx{a}, \mx{V}) \\
    \mathrm{for}\ \ \ \mx{y} = \mx{a}^T \mx{x} = \sum^n_{k=1} {a_ix_i}, \ \ \ \ \ y \sim \mathrm{N}(\mx{a}^{T}\mx{\mu}, \mx{a}^{T}\mx{V}\mx{a}) \\
    \mathrm{for}\ \ \ \mx{y} = \mx{A}\mx{x}, \ \ \ \ \ \mx{y} \sim \mathrm{MVN_m}(\mx{A\mu}, \mx{A^{T}VA})
\end{equation}\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li><p>Conditional distributions associated with MVN are also multivariate normal. <strong>ADD IN BOOK PART HERE!!</strong></p></li>
<li><p>If <span class="math inline">\(\mx{x} \sim \mathrm{MVN}\)</span>, the regression of any subset of <span class="math inline">\(\mx{x}\)</span> on another subset is linear and homoscedastic. <strong>ADD IN BOOK PART HERE!!</strong></p></li>
</ol>
</div>
</div>
<div id="overview-of-linear-models" class="section level2" number="8.6">
<h2><span class="header-section-number">8.6</span> Overview of linear models</h2>
<p>Consider a multivariate linear model where all the covariates are binary. For example, consider the half-sib design wherein each of <span class="math inline">\(p\)</span> unrelated sires is mated at random to a number of unrelated dams and a single offspring is measured from each cross. The simplest model for this design is</p>
<p><span class="math display">\[\begin{equation}
    y_{ij} = \mu + s_i + e_{ij}
    \notag
\end{equation}\]</span></p>
<p>where <span class="math inline">\(y_{ij}\)</span> is the phenotype of the <span class="math inline">\(j\)</span>th offspring from sire <span class="math inline">\(i\)</span>, <span class="math inline">\(\mu\)</span> is the population mean, <span class="math inline">\(s_i\)</span> is the sire effect, and <span class="math inline">\(e_{ij}\)</span> is the residual error. <strong>DON’T UNDERSTAND THIS SENTENCE</strong> Although this is clearly a linear model, it differs significantly from the regression model described above in that while there are parameters to estimate (the sire effects, <span class="math inline">\(s_i\)</span>), the only measured values are the <span class="math inline">\(y_{ij}\)</span>. Nevertheless, we can express this model in a form that is essentially identical to the standard regression model by using <span class="math inline">\(p\)</span> binary variables to classify the sires of the offspring. The resulting linear model becomes</p>
<p><span class="math display">\[\begin{equation}
    y_{ij} = \mu + \sum^p_{k=1}s_{k}x_{ik} + e_{ij} 
    \notag
\end{equation}\]</span>
where
<span class="math display">\[\begin{equation}
    x_{ik} = 
    \begin{cases}
        0, &amp; \text{ if k = i } \\
        1, &amp; \text{ otherwise }
    \end{cases}
\end{equation}\]</span></p>
<p>By using binary variables, an wide class of problems can be handled by linear models. Models containing only binary variables are usually called ANOVA (analysis of variance) models! Whether predictor variables are a continuous (as in regression) or not (as in ANOVA), the procedures are special cases of the general linear model (GLM), wherein each observation (<span class="math inline">\(\mx{y}\)</span>) is assumed to be a linear function of <span class="math inline">\(p\)</span> observed and / or binary variables plus a residual error (<span class="math inline">\(e\)</span>),</p>
<p><span class="math display" id="eq:general-linear-model">\[\begin{equation}
    y_i = \sum^p_{k=1} \beta_{k}x_{ik} + e_{i}
    \tag{8.28}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(x_{i1}, \dots, x_{ip}\)</span> are the values of the <span class="math inline">\(p\)</span> predictor variables for the <span class="math inline">\(i\)</span>th individual. For a vector of <span class="math inline">\(n\)</span> observations, the GLM can be written in matrix form as</p>
<p><span class="math display" id="eq:general-linear-model-matrix-form">\[\begin{equation}
    \mx{y} = \mx{X\beta} + \mx{e}
    \tag{8.29} 
\end{equation}\]</span></p>
<p>where the design or incidence matrix <span class="math inline">\(\mx{X}\)</span> is <span class="math inline">\(n \times p\)</span>, and <span class="math inline">\(\mx{e}\)</span> is the vector of residual errors. <span class="math inline">\(\mx{\beta}\)</span> is to be estimated obvs.</p>
<div id="ordinary-least-squares" class="section level3" number="8.6.1">
<h3><span class="header-section-number">8.6.1</span> Ordinary least squares</h3>
<p>Estimates of <span class="math inline">\(\mx{\beta}\)</span> are usually obtained using least-squares that make assumptions about the residuals. Ordinary least squares assumes that the residual errors are homoscedastic and uncorrelated, i.e., <span class="math inline">\(\sigma^2(e_i) = \sigma^2_e\)</span> for all <span class="math inline">\(i\)</span> and <span class="math inline">\(\sigma(e_i, e_j) = 0\)</span> for <span class="math inline">\(i \neq j\)</span>.</p>
<p>SOME MATHS HERE</p>
<p><span class="math display" id="eq:estimate-for-beta">\[\begin{equation}
    \mx{b} = (\mx{X}^T\mx{X})^{-1}\mx{X}^T\mx{y}
    \tag{8.30}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mx{b}\)</span> is an estimate of <span class="math inline">\(\mx{\beta}\)</span>. Under the assumption the residual errors are uncorrelated and homoscedastic, the covariance matrix of the elements of <span class="math inline">\(\mx{b}\)</span> is</p>
<p><span class="math display" id="eq:estimate-for-beta-covariance-matrix">\[\begin{equation}
    \mx{V_b} = (\mx{X}^T\mx{X})^{-1}\sigma^{2}_{e}
    \tag{8.31}
\end{equation}\]</span></p>
<p>If the residuals follow a multivariate distribution with <span class="math inline">\(\mx{e} \sim \mathrm{MVN}(0, \sigma^{2}_{e} \cdot \mx{I})\)</span>, the OLS estimate is also the maximum-likelihood estimate.</p>
</div>
</div>
<div id="generalised-least-squares" class="section level2" number="8.7">
<h2><span class="header-section-number">8.7</span> Generalised least squares</h2>
<p>Under OLS, the unweighted sum of squared residuals is minimized. However, if some residuals are inherently more variable than others (have a higher variance), less weight should be assigned to the more variable data. Correlations between residuals can also influence the weight that should be assigned to each individual, as the data are not independent. Thus, if the residual errors are heteroscedastic and/or correlated, OLS estimates of regression parameters and standard errors of these estimates are potentially biased.</p>
<p>A more general approach to regression analysis expressesthe covariance matrix of the vector of residuals as <span class="math inline">\(\sigma^2_{e}\mx{R}\)</span>, with <span class="math inline">\(\sigma(e_{i}e_{j}) = R_{ij}\sigma^2_{e}\)</span>. Lack of independence between residuals is indicated by the presence of nonzero diagonal elements in <span class="math inline">\(\mx{R}\)</span>, while heteroscedasticity is indicated by differences in the diagonal elements of <span class="math inline">\(\mx{R}\)</span>. Generalised (or weighted) least squares (GLS) takes the complications into account. If the linear model is</p>
<p><span class="math display">\[\begin{equation}
    \mx{y} = \mx{X\beta} + e\ \ \ \mathrm{with} \ \mx{e} \sim (0, \mx{R}\sigma^2_{e})
    \notag
\end{equation}\]</span></p>
<p>the GLS estimate of <span class="math inline">\(\mx{\beta}\)</span> is</p>
<p><span class="math display" id="eq:gls-estimate-of-beta">\[\begin{equation}
    \mx{b} = \left(\mx{X}^T\mx{R}^{-1}\mx{X}\right)^{-1}\mx{X}^T\mx{R}^{-1}\mx{y}
    \tag{8.32}
\end{equation}\]</span></p>
<p>The covariance matrix for the GLS estimates is</p>
<p><span class="math display" id="eq:covariance-matrix-for-gls-estimates">\[\begin{equation}
    \mx{V_{b}} = \left(\mx{X}^T\mx{R}^{-1}\mx{X}\right)^{-1}\sigma^2_{e}
    \tag{8.33}
\end{equation}\]</span></p>
<p>If residuals are independent and homoscedastic, <span class="math inline">\(\mx{R} = \mx{I}\)</span>, and GLS estimates are the same as OLS estimates. If <span class="math inline">\(\mx{e} \sim \mathrm{MVN}(0, \mx{R}\sigma^2_e\)</span>, the GLS estimate of <span class="math inline">\(\mx{\beta}\)</span> is also the maximum likelihood estimate.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="resemblance-between-relatives.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="questions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["quant_genetics_book_notes.pdf", "quant_genetics_book_notes.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

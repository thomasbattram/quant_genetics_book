[
["index.html", "Notes for: Walsh and Lynch. Genetics and Analysis of Quantitative Traits Preface", " Notes for: Walsh and Lynch. Genetics and Analysis of Quantitative Traits Thomas Battram 2020-06-18 Preface This is a good book, but if I make it through the whole thing I deserve several medals and some cake. "],
["an-overview-of-quantitative-genetics.html", "Chapter 1 An overview of quantitative genetics", " Chapter 1 An overview of quantitative genetics BORWANG!!! This chapter just introduces the book and some simple concepts. "],
["properties-of-distributions.html", "Chapter 2 Properties of distributions", " Chapter 2 Properties of distributions ALSO BORWANG! You can guess what this chapter was on and also how much of a hoot it was… "],
["covariance-regression-and-correlation.html", "Chapter 3 Covariance, regression, and correlation 3.1 Covariance 3.2 Least squares linear regression 3.3 Correlation 3.4 Differential selection (brief intro) 3.5 Correlation between genotype and phenotype (brief intro) 3.6 End of chapter questions", " Chapter 3 Covariance, regression, and correlation 3.1 Covariance Covariance is a measure of association and the covariance between x and y would be denoted by \\(\\sigma(x, y)\\). If \\(x\\) and \\(y\\) are independent then \\(\\sigma(x, y) = 0\\), BUT if \\(\\sigma(x, y) = 0\\), \\(x\\) and \\(y\\) aren’t necessarily independent. 3.1.1 Useful identities for covariance Covariance of \\(x\\) with itself = variance of \\(x\\): \\[\\begin{equation} \\sigma(x, x) = \\sigma^2(x) \\tag{3.1} \\end{equation}\\] For constants (here represented by \\(a\\)) see (3.2) below \\[\\begin{align} \\sigma(a, x) &amp;= 0 \\notag \\\\ \\sigma(ax, y) &amp;= a\\sigma(x, y) \\notag \\\\ \\sigma^2(a, x) &amp;= a^2\\sigma^2(x) \\notag \\\\ \\sigma[(a + x), y] &amp;= \\sigma(x, y) \\tag{3.2} \\end{align}\\] The covariance of 2 sums can be written as the sum of covariances, i.e. just multiply out the brackets: \\[\\begin{equation} \\sigma[(x + y),(w + z)] = \\sigma(x, w) + \\sigma(x, z) + \\sigma(y, w) + \\sigma(y, z) \\tag{3.3} \\end{equation}\\] Variance of a sum is sum of variances and covariances: \\[\\begin{equation} \\sigma^2(x + y) = \\sigma^2(x) + 2\\sigma(x, y) + \\sigma^2(y) \\tag{3.4} \\end{equation}\\] 3.2 Least squares linear regression Linear model: \\[\\begin{equation} y = \\alpha + \\beta{x} + e \\tag{3.5} \\end{equation}\\] Continuing on, \\(\\alpha\\) and \\(\\beta\\) will be the true population values and \\(a\\) and \\(b\\) will be the intercept and slope for the line of best fit derived from observed data. The derivation of \\(a\\) and \\(b\\) using the least-squares model can be found on pages 39-41. Buuut, who cares about that, here are the results: \\[\\begin{align} a &amp;= \\bar{y} - b\\bar{x} \\notag \\\\ b &amp;= \\frac{Cov(x, y)} {Var(x)} \\tag{3.6} \\end{align}\\] 3.2.1 Properties of least squares 6 in the book, just writing down important/not obvious ones. The mean residual (\\(\\bar{e}\\)) is 0 Residual errors are uncorrelated with predictor variable \\(x\\) (see book for why) BUT \\(e\\) and \\(x\\) may not be independent if the relationship between \\(x\\) and \\(y\\) is non-linear. If it is truly non-linear \\(E(e|x) != 0\\) Variance of \\(e\\) can vary with \\(x\\), in this situation the the regression is said to display heteroscedasticity (see Figure 3.4 for great illustration) The regression of \\(y\\) on \\(x\\) is different to the regression of \\(x\\) on \\(y\\)! 3.3 Correlation Correlation coefficient between \\(x\\) and \\(y\\): \\[\\begin{equation} r(x, y) = \\frac{Cov(x, y)} {\\sqrt{Var(x) Var(y)}} \\tag{3.7} \\end{equation}\\] The correlation coefficient is a dimensionless measure of association and it is symmetrical (i.e. \\(r(x, y) = r(y, x)\\)). Scaling \\(x\\) or \\(y\\) by constants does not change the correlation coefficient, but it does affect variances and covariances. The correlation coefficient is a standardised regression coefficient -&gt; the regression coefficient resulting from rescaling \\(x\\) and \\(y\\) such that each has unit variance). \\(r^2\\) assumes \\(E(y|x)\\) is linear! 3.4 Differential selection (brief intro) The directional selection differential, \\(S\\), is the difference between the mean phenotype within that generation before selection (\\(\\mu_s\\)) and the mean phenotype within that generation after (\\(\\mu\\)) selection. \\[\\begin{equation} S = \\mu_s - \\mu \\tag{3.8} \\end{equation}\\] If all individuals have equal fertility and viability then selecting individuals won’t change anything so \\(\\mu_s = \\mu\\) and \\(S = 0\\). If \\(W(z)\\) is the probability that individuals with phenotype \\(z\\) survive to reproduce and \\(p(z)\\) is the density of \\(z\\) (pretty much means distribution) before selection, then the density after selection is: \\[\\begin{equation} p_{s}(z) = \\frac{W(z)p(z)} {\\int W(z)p(z)dz} \\tag{3.9} \\end{equation}\\] The denominator here is the mean individual fitness (\\(\\bar{W}\\)). The relative fitness of \\(z\\) is \\(w(z) = \\frac{W(z)} {\\bar{W}}\\). After some sweet derivation (see page 46), you finish with: \\[\\begin{equation} S = \\sigma[z, w(z)] \\tag{3.10} \\end{equation}\\] Therefore the directional selection is equivalent to the covariance of the phenotype and the relative fitness. If you regress offspring phenotype on the midparent phenotype and that relationship is linear with slope \\(\\beta\\), a change in mean midparent phenotype induces an expected change in mean phenotype across generations equal to: \\[\\begin{equation} \\begin{split} \\Delta\\mu &amp;= \\mu_0 - \\mu \\\\ &amp;= \\beta(\\mu_s - \\mu) \\\\ &amp;= \\beta{S} \\end{split} \\tag{3.11} \\end{equation}\\] This is the breeders’ equation! 3.5 Correlation between genotype and phenotype (brief intro) Only when there is no gene-environment interaction is the variance explained by genetics (broad-sense heritability) the equation below: \\[\\begin{equation} H^2 = \\frac{\\sigma^2_G} {\\sigma^2_z}, \\tag{3.12} \\end{equation}\\] where \\(z\\) is the phenotype and \\(G\\) is the sum of the total effects (not just additive) at all loci on the trait. The slope of a midparent-offspring regression provides an estimate of the proportion of the phenotypic variance that is attributable to additive genetic factors (the narrow-sense heritability). \\[\\begin{equation} h^2 = \\frac{\\sigma^2_A} {\\sigma^2_z} \\tag{3.13} \\end{equation}\\] So as \\(h^2\\) is just the regression of offspring phenotype on midparent phenotype it can actually be used in the breeders’ equation! \\[\\begin{equation} \\Delta\\mu = h^2S \\tag{3.14} \\end{equation}\\] So the narrow-sense heritability can be thought of as the efficiency of the response to selection. If \\(h^2 = 0\\) there can be no evolutionary change regardless of strength of selection. Although this should be obvious because if \\(h^2\\) is 0 then there is clearly no passing of genetic material onto the next generation that is influencing that trait. 3.6 End of chapter questions True or false, if \\(\\sigma(x,y) = 0\\), \\(x\\) and \\(y\\) are independent Finish these equations: \\[\\begin{align} \\sigma[(x + y),(w + z)] &amp;= ... \\notag \\\\ \\sigma^2(a, x) &amp;= ... \\notag \\\\ \\sigma[(a + x), y] &amp;= ... \\notag \\\\ \\sigma^2(x + y) &amp;= ... \\notag \\\\ \\end{align}\\] Give 2 properties of residuals from least-squares regression What is heteroscedasticity? True or false, \\(y\\) ~ \\(x\\) will always give the same effect estimate as \\(x\\) ~ \\(y\\) Give an assumption of the correlation coefficient How can you work out \\(h^2\\) from trio data? Give two definitions of the breeders’ equation. "],
["properties-of-single-loci.html", "Chapter 4 Properties of single loci 4.1 Introduction 4.2 Allele and genotype frequencies 4.3 The transmission of genetic information 4.4 Characterising the influence of a locus on the phenotype 4.5 The basis of dominance 4.6 Fisher’s decomposition of the genotypic value 4.7 Partioning the genetic variance. 4.8 Additive effects, average excesses and breeding values 4.9 Extensions for multiple alleles and non random mating 4.10 End of chapter questions", " Chapter 4 Properties of single loci 4.1 Introduction too easy 4.2 Allele and genotype frequencies too easy 4.3 The transmission of genetic information 4.3.1 The Hardy-Weinberg principle \\[\\begin{equation} p^2 + 2pq + q^2 = 1 \\tag{4.1} \\end{equation}\\] where \\(p\\) = allele frequency of first allele at a locus and \\(q\\) = allele frequency of the second allele at that same locus. Assumptions of H-W: No selection No mutation Random mating No differential migration No random drift Even though these assumptions will never be met completely in the real world, for the majority of the time the H-W prinicple holds regardless. Assuming assumptions are met, 2 important points from H-W: It takes no more than a single generation to equilibriate and stabilize gene frequencies in the two sexes. Only one additional generation is required for the stabilisation of the genotype frequencies into the predictible Hardy-Weinberg proportions. 4.3.2 Sex-linked loci Alleles on sex chromosomes in diploid organisms are obviously different. Sons can only receive an X chromosome from their mother so the frequency of X linked loci in the sons is equal to that of their mothers. Daughters receive both an X chromosome from Mum + from Dad. Overall this means allele frequencies oscilate around an equilibrium state, but continually get closer to that state over the generations (see Figure 4.2 and page 56 for equation). 4.3.3 Polyploidy Skipped over this section because it’s not relevant to human quant gen. Buuut, essentially it just details how to derive allele frequencies under a certain case of polyploidy. Also, it should be noted that of course H-W does not hold under polyploidy! 4.3.4 Age structure Age structure also complicates our idealised model of H-W. In populations composed of several age classes, the generations overlap, and this causes the approach of genotype frequencies towards the H-W expectations to be gradual (rather than just by 1 or 2 generations), even in the case of an autosomal locus. Doesn’t explain this very much, but it’s covered elsewhere. Importantly, when newly founded populations have significant age structure, fluctuations in both gene and genotoype frequencies may occur for a substantial period of time even in the abscence of selection! 4.3.5 Testing for Hardy-Weinberg proportions Says in the book that LRT can be used to test for departures from HWE and it can, but another common method is the chi-squared test and in PLINK they used Haldane’s exact test which is apparently analogus to Fisher’s exact test (papier on it) (can also use Fisher’s exact test if the sample size is tiny and the allele is rare.). Essentially, in a population, at a specific locus, you can calculate the allele frequencies (and from that expected genotype frequencies) from the observed genotype frequencies then test if there is a difference between the observed and expected values. LRT equation for it given on page 60. See code for some comparisons. Should remember (as pointed out above), that just because some assumptions are violated, doesn’t mean you’d get a departure from HWE! 4.4 Characterising the influence of a locus on the phenotype If a trait is entirely influenced by a single locus then the genetic effect on that trait can be characterised pretty easily and the dominance and additive effects of the alleles can be calculated. So if a locus has genotypes \\(B_1B_1\\), \\(B_1B_2\\), \\(B_2B_2\\), then the values given to these genotypes can be said to be: \\(-a\\), \\((1 + k)a\\) and \\(+a\\). Now if you have genotype data at that locus and data on the trait you can work out the effect of the \\(B_2\\) allele by taking the mean phenotypic value of individuals with \\(B_2B_2\\) and subtracting the mean phenotypic value of individuals with \\(B_1B_1\\) and dividing by 2 i.e. \\[\\begin{equation} B_{2eff} = \\frac{p_{B2} - p_{B1}} {2} \\tag{4.2} \\end{equation}\\] where \\(B_{2eff}\\) is the effect of allele \\(B_2\\), \\(p_{B2}\\) is the mean phenotypic value of individuals with \\(B_2B_2\\) and \\(p_{B1}\\) is the mean phneotypic value of individuals with \\(B_1B_1\\). As \\(B_{2eff} = a\\) you can then substitute this in to \\((1 + k)a\\) to get the dominance coefficient \\(k\\). Of course if \\(k = 0\\) then there is no dominance (in reality you would calculate probability of dominance). 4.5 The basis of dominance Confusing part… Don’t really get the enzyme activity bit… Main point (I think) is that new deleterious mutations are very likely to be recessive and new mutations with a slight deleterious effect interact in an almost entirely additive fashion (no dominance!). 4.6 Fisher’s decomposition of the genotypic value Recalling that the phenotypic value can be partitioned like so: \\[\\begin{equation} z = G + E \\tag{4.3} \\end{equation}\\] where \\(z\\) is the phenotype, \\(G\\) is the genotypic value and \\(E\\) is the environmental value. The genotypic value of a specific locus can be partitioned into it’s “expected” values based on there being only additive effects (\\(\\hat{G}\\)) and the deviations from the expected values or dominance effects (\\(\\delta\\)). So for genotype \\(B_iB_j\\): \\[\\begin{equation} G_{ij} = \\hat{G_{ij}} + \\delta_{ij} \\tag{4.4} \\end{equation}\\] This can be formalised (whatever the fuck that means) by regressing the genotypic values on the number of \\(B_1\\) and \\(B_2\\) alleles in the genotype (\\(N_1\\) and \\(N_2\\)): \\[\\begin{equation} G_{ij} = \\hat{G_{ij}} + \\delta_{ij} = \\mu_G + \\alpha_1N_1 + \\alpha_2N_2 \\tag{4.5} \\end{equation}\\] \\(\\mu_G\\) = the mean genotypic value in the population, \\(\\alpha_1\\) and \\(\\alpha_2\\) are the slopes of the regression, \\(N_1\\) and \\(N_2\\) are the number of \\(B_1\\) and \\(B_2\\) alleles. So the regression is: \\[\\begin{equation} G_{ij} ~ N_2 + N_1 \\end{equation}\\] By noting that for any individual, \\(N_1 = 2 - N_2\\) you can reduce the multiple regression model into an easier to work with univariate model. Give it a go (use equation (4.5)): \\[\\begin{equation} \\begin{split} G_{ij} &amp;= \\mu_G + \\alpha_1(2 - N_2) + \\alpha2N_2 + \\delta_{ij} \\\\ &amp;= l + (\\alpha_2 - \\alpha_1)N_2 + \\delta_{ij} \\end{split} \\tag{4.6} \\end{equation}\\] where \\(l = \\mu_G + 2\\alpha_1\\) is the intercept and the slope is \\(\\alpha = \\alpha_2 - \\alpha_1\\) If you plotted the genotypic value (\\(G\\)) against gene content (\\(N_2\\) or number of \\(B_2\\) alleles) and calculated residuals these residuals would be \\(\\delta\\), the dominance deviation (see Figure 4.6). The rest of the chapter uses this regression and what we know about genotype frequencies to derive a formula for the average effect of allelic substitution: \\[\\begin{equation} \\alpha = a[1 + k(p_1 - p_2)] \\tag{4.7} \\end{equation}\\] where \\(a\\) = genotypic value of \\(B_2\\) (see above), \\(k\\) is the dominance coefficient and \\(p_1\\) and \\(p_2\\) are the frequencies of \\(B_1\\) and \\(B_2\\). This value \\(\\alpha\\) represents the average change in genotypic value that results when a \\(B_2\\) allele is randomly substituted for a \\(B_1\\) allele. If no dominance (\\(k = 0\\)) then \\(\\alpha = a\\). Except in the case of additivity, the average effect of allelic substitution is not simply a function of the inherent physiological properties of the allele. It can only be defined in the context of the population! 4.7 Partioning the genetic variance. Deriving variance of \\(G\\): \\[\\begin{align} \\begin{split} G &amp;= \\hat{G} + \\delta \\notag \\\\ \\sigma^2_G &amp;= \\sigma^2(\\hat{G} + \\delta) \\notag \\\\ &amp;= \\sigma^2(\\hat{G}) + 2\\sigma(\\hat{G} + \\delta) + \\sigma^2(\\delta) \\end{split} \\tag{4.8} \\end{align}\\] The top equation is just like a regression, with \\(\\delta\\) being the residual error and we know that for least-squares there is no correlation between the residual error and the predictor. So there is no correlation between \\(\\hat{G}\\) and \\(\\delta\\). Therefore: \\[ \\sigma^2_G = \\sigma^2(\\hat{G}) + \\sigma^2(\\delta) \\] OR more commonly \\[\\begin{equation} \\sigma^2_G = \\sigma^2_A + \\sigma^2_D \\tag{4.9} \\end{equation}\\] \\(\\sigma^2_A\\) is the variance of \\(G\\) explained by regression on \\(N_2\\) (or \\(N_1\\)), and \\(\\sigma^2_D\\) is the residual variance of that regression. The variance of the additive and dominance effects! For a diallelic locus we can do some rearranging of equations in Table 4.1 of the book and get these equations: \\[\\begin{align} \\sigma^2_A &amp;= 2p_1p_2\\alpha^2 \\tag{4.10} \\\\ \\sigma^2_D &amp;= (2p_1p_2ak)^2 \\tag{4.11} \\end{align}\\] From these we can clearly see that both components depend on allele frequencies, the dominance coefficient and the homozygous effect (remember \\(\\alpha\\) is just the slope of the \\(G ~ N_2\\) regression!). By plotting how genetic variance changes with gene frequency under different scenarios (see 4.1). You see some interesting patterns. Firstly, at a single diallelic locus, you see that \\(\\sigma^2_A\\) reaches it’s peak when \\(p_1 = p_2 = 0.5\\). Secondly, it’s clear that, even in the case of overdominance (which is rare!), additive genetic variance will almost always be much higher than genetic variance from dominance effects, even when the frequency of the dominant allele is high. Figure 4.1: The dependence of components of genetic variance at a locus on the frequency of the \\(B_2\\) allele. \\(a\\) is set to be one, which scales the vertical axes so that for any particular case, the actual variances are obtainable by multiplying by \\(a^2\\). 4.8 Additive effects, average excesses and breeding values The dominance deviation of a parent, which is a function of the interaction between the two parental alleles, is eliminated when gametes are produced. Thus, one can think of \\(\\hat{G}\\) and \\(\\delta\\) as the heritable and nonheritable components of an individual’s genotypic value. Fisher proposed two different measures of the effect of an allele: one being the additive effect (\\(\\alpha_i\\)) and then the average excess \\(\\alpha^x_i\\). The average excess \\(\\alpha^x_2\\) of allele \\(B_2\\) is the difference between the mean genotypic value of individuals carrying at least one copy of \\(B_2\\) and the mean genotypic value of a random individual from the entire population: \\[\\begin{equation} \\alpha^x_2 = (G_{12}P_{12|2} + G_{22}P_{22|2}) - \\mu_G \\tag{4.12} \\end{equation}\\] where \\(P_{ij}\\) is the conditional probability of a \\(B_iB_j\\) genotype given that one allele is \\(B_i\\). Under random mating \\(P_{ij|i} = p_j\\) (\\(p_j\\) = frequency of allele \\(B_j\\)). THINK ABOUT HARDY-WEINBERG AND IT MAKES SENSE! So under random mating, \\[\\begin{equation} \\alpha^x_2 = G_{12}p_1 + G_{22}p_2 - \\mu_G \\tag{4.13} \\end{equation}\\] \\(G_{12} = a(1+k)\\) and \\(G_{22} = 2a\\). By substituting these into the equation above for \\(\\alpha^x_1\\) and \\(\\alpha^x_2\\) and then calculating \\(\\alpha_1\\) and \\(\\alpha_2\\) (additive effects) by the method previously mentioned (regressing genotypic value \\(G\\) on the number of \\(B_2\\) alleles, \\(N_2\\)), we will see they’re equivalent (shown on page 72): \\[\\begin{align} \\alpha_2 &amp;= p_1\\alpha \\notag \\\\ \\alpha_1 &amp;= -p_2\\alpha \\end{align}\\] The breeding value of an individual (\\(A\\)) is the sum of the additive effects of its genes. So the breeding value of a \\(B_1B_1\\) homozygote is just \\(2\\alpha_1\\). In randomly mating populations the breeding value of a genotype is equivalent to twice the expected deviation of its offspring mean phenotype from the population mean. Soooo, no genotype information is needed to calculate the breeding value. All we have to do is mate an individual to many randomly chosen individuals from the population and taking twice the deviation of its offspring mean from the population mean. EASY IN HUMANS!!! In Chapter 13 this will be discussed wrt candidate gene studies. 4.9 Extensions for multiple alleles and non random mating So this section seems mostly unrelevant as we’re unlikely to deal with situations with more than 2 alleles. Non-random mating could be encountered if we’re interested in some phenotypes (e.g. alcohol intake). Buuuut, it’s still good to note some of the generalised equations for what we’ve been discussing so far in the chapter. 4.9.1 Average excess When \\(n\\) alleles are present, the average excess, \\(\\alpha^x_i\\), for any allele \\(B_i\\) is given by \\[\\begin{equation} \\alpha^x_i = \\sum_{j=1}^{n} P_{ij|i}G_{ij} - \\mu_G \\tag{4.14} \\end{equation}\\] Remember, under random mating \\(P_{ij|i} == p_j\\) 4.9.2 Additive effects The genotypic value can also be obtained using regression as before, but in it’s generalised form is a multivariate regression. For \\(n\\) alleles \\[\\begin{equation} G = \\mu_G + \\sum_{i=1}^{n} \\alpha_{i}N_{i} + \\delta \\tag{4.15} \\end{equation}\\] After some re-arranging can derive the regression coefficients and finally end with \\[\\begin{equation} \\alpha_i = \\sum_{j=1}^{n} p_jG_{ij} - \\mu_G \\tag{4.16} \\end{equation}\\] i.e. under random mating, the average effects (\\(\\alpha_i\\)) are equal to the conditional mean deviations from the mean genotypic value of the population (\\(\\mu_G\\)). For non-random mating we need the inbreeding coefficient, \\(f\\) to define our genotype frequencies: \\[\\begin{align} P_{ii} &amp;= (1 - f)p^2_i + fp_i \\notag \\\\ P_{ij} &amp;= 2(1 -f)p_ip_j \\tag{4.17} \\end{align}\\] Unsure of why, but this means \\[\\begin{equation} \\alpha_i = \\frac{\\alpha^x_i} {1 + f} \\tag{4.18} \\end{equation}\\] so \\(f\\) is the fractional reduction of heterozygote frequencies relative to those expected under random mating. This means you can kind of do a test for random mating by checking heterozygote and homozygote frequencies in a population! 4.9.3 Additive genetic variance The additive genetic variance across \\(n\\) alleles is \\[\\begin{equation} \\sigma^2_A = 2 \\sum_{i=1}^{n} p_i\\alpha_i\\alpha^x_i \\tag{4.19} \\end{equation}\\] In general inbreeding inflates the additive genetic variance by causing correlations among the effects of alleles within the same individuals. The broad sense heritability, even under scenarios of non-random mating can be given by \\[\\begin{equation} \\sigma^2_G = \\sigma^{2}(\\alpha_i + \\alpha_j) + \\sigma^{2}(\\delta_{ij}) \\end{equation}\\] although it should be noted that the definitions of \\(\\alpha_i\\) and \\(\\delta_ij\\) change with the degree of inbreeding! Random mating means \\(\\alpha_i\\) and \\(\\alpha_j\\) are uncorrelated so we get back to the good old equation \\[ \\sigma^2_G = \\sigma^2_A + \\sigma^2_D \\] Importantly, under random mating, \\(\\sigma^2_A\\) is equivalent to the variance of breeding values of individuals in the population. Summarising some key terms The homozygous effect, \\(a\\), and the dominance coefficient, \\(k\\), are intrinsic properties of allelic products. They are not functions of allele frequencies, but may vary with genetic background The additive effect, \\(\\alpha_i\\), and the average excess, \\(\\alpha^x_i\\), are properties of alleles in a particular population. They are functions of \\(a\\), \\(k\\) and genotype frequencies (\\(p_i\\)). The breeding value, \\(A\\), is a property of a particular individual in reference to a particular population. It’s equivalent to the sum of the additive effects of an individual’s alleles. The additive genetic variance, \\(\\sigma^2_A\\) is a property of a particular population. It is equivalent to the variance of the breeding values of individuals within the population. 4.10 End of chapter questions What is the Hardy-Weinberg principle and what are it’s assumptions? What does the H-W principle mean for gene and genotype frequencies across generations? What is age structure and how does it affect HWE? How can you test for HWE? Are deliterious mutations likely to be dominant or recessive? Assuming a trait was entirely influenced by a single locus, how could you calculate dominance and additive effects knowing the genotypes and phenotypes of the individuals in the sample? What is the formula for the average effect of allelic substitution? For a diallelic locus, what does the additive genetic variance and dominance genetic variance depend on? How does the contribution of additive genetic variance to total genetic variance change when \\(k\\) varies? What is the breeding value of an individual? Define the additive genetic variance in the presence of \\(n\\) alleles Learn the definitions of the key terms! "],
["sources-of-genetic-variation-for-multilocus-traits.html", "Chapter 5 Sources of genetic variation for multilocus traits 5.1 Epistasis 5.2 A general least-squares model for genetic effects 5.3 Linkage 5.4 Effect of disequilibrium of the genetic variance 5.5 End of chapter questions", " Chapter 5 Sources of genetic variation for multilocus traits 5.1 Epistasis Epistasis describes the nonadditivity of effects between loci, i.e. the alleles of one loci influence the effects of another loci. The genotypic value, \\(G_{ijkl}\\), needs to take into account all the interaction terms that can arrive between loci, for two loci it’s additive x additive effects (\\(\\alpha\\alpha\\)), additive x dominance effects (\\(\\alpha\\delta\\)), and dominance x dominance effects (\\(\\delta\\delta\\)). As the number of loci increases the number of interaction terms increase steadily e.g. \\(\\alpha\\alpha\\alpha\\) will be there for three loci. 5.2 A general least-squares model for genetic effects This is just an extension of the one-locus linear model introduced in Chapter 4. For this section, imagine we are interested in measuring the genetic effects of two loci, \\(G_{ijkl}\\), which can easily be extended to more. The additive effect of an allele on a phenotype is just the phenotypic value in people with that allele minus the mean phenotypic value of the population. When considering epistatic effects we can define it in the same way. \\[\\begin{equation} \\alpha_{i} = G_{i...} - \\mu_{G} \\tag{5.1} \\end{equation}\\] \\(G_{i...}\\) is just the conditional mean phenotype of individuals with allele \\(i\\) at the first locus without regard to the other allele at that locus or to the genotype at the second locus. The other additive terms (for \\(\\alpha_{j}\\), \\(\\alpha_{k}\\), \\(\\alpha_{l}\\)) are defined in the same way. Within each locus, the mean value of average effects (weighted by allele frequency) = 0. Dominance effects can be defined in a similar way, complete these equations by recalling (4.15): \\[\\begin{align} \\delta_{ij} &amp;= G_{ij..} - ... \\tag{5.2} \\\\ \\delta_{lk} &amp;= G_{..lk} - ... \\tag{5.3} \\end{align}\\] Like with the additive effects, the mean dominance deviation at each locus is equal to zero. Epistatic effect terms proceed in a similar fashion. Letting \\(G_{i.k.}\\) be the mean phenotype of individuals with gene \\(i\\) at locus 1 and \\(k\\) at locus 2, without regard to the other two genes, the \\(ik\\)th additive x additive effect is: \\[\\begin{equation} \\left(\\alpha\\alpha\\right)_{ik} = G_{i.k.} - \\mu_{G} - \\alpha_{i} - \\alpha_{k} \\tag{5.4} \\end{equation}\\] So \\(\\left(\\alpha\\alpha\\right)_{ik}\\) is the deviation of the conditional mean \\(G_{i.k.}\\) from the expectation based on the population mean \\(\\mu_{G}\\) and the additive effects \\(\\alpha_{i}\\) and \\(\\alpha_{k}\\). An additive x dominance effect measures the interaction between an allele at one locus with a genotype of another locus (see equation 5.5 in book) and the dominance x dominance effect involves an interaction between the genotypes at each locus (see equation 5.6 in book). \\[\\begin{equation} TO DO \\tag{5.5} \\end{equation}\\] The complete genotypic value, \\(G_{ijkl...}\\) can be found in equation 5.7 in the book. These parameters depend on genotype frequencies in the population, but the mean value of each type of effect is always equal to zero. The genotypic value of an individual is often impossible to quantify because of variation in the phenotype due to the environment, but the genotypic value for an individual equation can be extended to populations. Providing mating is random and segregation of loci is independent, there is no statistical relationship between the genes found within or among loci. So the total genetic variance is just the sum of the variance of the individual effects, simplified this is: \\[\\begin{equation} \\sigma^{2}_{G} = \\sigma^{2}_{A} + \\sigma^{2}_{D} + \\sigma^{2}_{AA} + \\sigma^{2}_{AD} + \\sigma^{2}_{DD} + ... \\tag{5.6} \\end{equation}\\] … here and in other cases just symbolises more terms can be added if more than two loci are used. Epistatic effects are expected to be common throughout the genome and Wright thought they were the rule, rather than exception. See example two in the book for calculations of epistatic effects and how much variance they contribute to the overall genetic variance component. Overall, it is clear that even with large epistatic effects, additive genetic variance, \\(\\sigma^{2}_{A}\\) will pretty much always (if not always) contribute to the vast majority of overall genetic variance \\(\\sigma^{2}_{G}\\). This is important for two reasons: Variance components provide limited insight into the physiological mode of gene action, i.e. just because genetic variance is explained by additive effects (which means you essentially count each gene separately), it does not mean the interaction between genes is not important in terms of their function! When interested in the variance of a trait that is explained by genetics, you can expect the vast majority of that variance to be explained by additive genetic effects, which makes things like estimating heritability far easier. 5.2.1 Extension to haploids and polyploids Skipped this section as not relevant to humans. 5.3 Linkage Genes of the same chromosome tend to be inherited as a group, a tendency that declines with increasing distance between the loci. Crossing-over during meiosis is responsible for this decline. Difference between linkage and linkage disequilibrium Loci are linked if they tend to be inherited together. If loci are correlated for any reason (don’t need to be inherited together), they are in linkage disequilibrium. The census units for measuring linkage are gamete frequencies, so you can use an individual to estimate this. LD is measured across a population. Can get linkage without LD, can get linkage and LD, and can get two correlated loci (LD) that aren’t linked. Even though you can get correlated loci for reasons other than linkage, the LD between linked loci are more likely to persist over time as seen in (5.9). Under linkage equilibrium, the frequency of gametes is the product of allele frequencies, so for loci \\(A\\) and \\(B\\), \\[\\begin{equation} Freq(AB) = Freq(A) * Freq(B) \\tag{5.7} \\end{equation}\\] So A and B are independent of each other. Measure of disequilibrium is just the departure from this: \\[\\begin{equation} D_{AB} = Freq(AB) - Freq(A) * Freq(B) \\tag{5.8} \\end{equation}\\] \\(D_{AB}\\) can be positive or negative depending on whether \\(A\\) and \\(B\\) are in coupling (\\(AB\\) gametes are overrepresented) or repulsion (\\(AB\\) gametes are underrepresented) disequilibrium. \\(D\\) is often referred to as the coefficient of linkage disequilibrium (although can be non-zero without linkage!). Selection, migration, mutation and drift can help maintain LD. Even without these forces, once LD is established it can be maintained for many generations (especially if loci are more tightly linked!). Expected LD changes over time depend on the recombination fraction between loci, \\(c\\). This value ranges from 0 to 0.5, where 0 essentially means the loci are inherited together and 0.5 is free recombination between loci. If recombination frequency between the \\(A\\) and \\(B\\) loci is \\(c\\), the disequilibrium in generation \\(t\\) is given by: \\[\\begin{equation} D(t) = (1 - c)^{t}D(0) \\tag{5.9} \\end{equation}\\] This equation is graphically displayed in 5.1. Figure 5.1: The decline, under random mating, of linkage disequilibrium when the initial value [D(0)] is set to 1 as a function of the recombination frequency, c. To estimate \\(D\\) you can directly count gamete frequencies of an individual. This is not possible for most organisms though and usually you have to make-do with measured multilocus genotypes across a population. You can then workout gametes used to produce the genotypes (e.g. someone with an \\(AABb\\) genotype would be formed from a \\(AB\\) gamete and \\(aB\\) gamete). However, with double heterozygotes you can’t be sure if an \\(ABab\\) individual was formed from \\(AB\\) and \\(ab\\) gametes or from \\(Ab\\) and \\(aB\\) gametes. Under random mating, it’s not necessary to distinguish between coupling and repulsion heterozygotes so this doesn’t really matter. In this case an estimate of \\(D\\) is given by: \\[\\begin{equation} \\hat{D_{AB}} = \\frac{N} {N - 1} [\\frac{4N_{AABB} + 2(N_{AABb} + N_{AaBB}) + N_{AbBb}} {2N} -2\\hat{p_{A}}\\hat{p_{B}}] \\tag{5.10} \\end{equation}\\] The equation for the sampling variance of \\(D\\) is shown on page 99. Ideally you’d have 1000s of samples to achieve reasonable statistical power when estimating \\(D\\) using multilocus genotype frequencies. 5.4 Effect of disequilibrium of the genetic variance The aggregate effects of gametic phase disequilibrium might be extensive for quantitative traits whose expression is based on large numbers of loci, even if the average level of disequilibrium between pairs of loci is relatively small. If genes with a positive influence on a character tend to be associated on some chromosomes, and those with a negative influence on others (coupling disequilbrium), the observed genetic variation will be inflated relative to the expectation under random assortment. The opposite will occur if “plus” alleles at one locus tend to be associated with “minus” alleles at another (repulsion disequilibrium). This is illustrated nicely by figure 5.6 in the book. Think of it this way, you’re studying gene \\(A\\), gene \\(B\\) and phenotype \\(X\\). If upregulation of \\(A\\) leads to an increase in \\(X\\) and an upregulation of \\(B\\) leads to an increase in \\(X\\) by the same proportions, then if genetic variation always occurs so that whenever \\(A\\) is upregulated, \\(B\\) is downregulated by the same amount, then what will be observed at both of the loci is that variation at them is not associated with variation in \\(X\\). This situation described is complete repulsion linkage (again, see figure 5.6 in book). This is assuming additive effects across loci. When there is no disequilibrium between loci, the variance at each locus is just \\(2pqa^{2}\\), see (4.10). As always, dominance effects muddy the waters, but here are the formalized multilocus analogs of (4.10) and (4.11): \\[\\begin{align} \\sigma^2_A &amp;= 2\\sum_{i=1}^{n} \\alpha(i)^{2}p_{i}q_{i} + 2\\sum_{i=1}^{n} \\sum_{j\\ne1}^{n} \\alpha(i)\\alpha(j)D_{ij} \\tag{5.11} \\\\ \\sigma^2_D &amp;= 4\\sum_{i=1}^{n} (a_{i}k_{i}p_{i}q_{i})^{2} + 4\\sum_{i=1}^{n} \\sum_{j\\ne1}^{n} a_{i}a_{j}k_{i}k_{j}D^2_{ij} \\tag{5.12} \\end{align}\\] where \\(\\alpha(i)\\) is the average effect of allelic substitution at the \\(i\\)th locus (defined in equation (4.7)). Epistatic interactions make things crazy complicated! In summary, the componenets of expressed genetic variance for quantitative traits can be partitioned into expected values under gametic phase equilibrium and deviations from these caused by disequilibrium. From the book: “When the disequilibrium covariance is negative, we refer to it as hidden genetic variance because it is subject to conversion to expressed genetic variance via the breakdown of gametic phase disequilibria”. What I think this means: Negative disequilibrium covariance is just when correlated loci covary in a way so that when one increases a traits value, the other decreases it, this is what happens in repulsion linkage! The reason it’s not just called repulsion linkage is because you can have covarying loci without them being linked. Sooo, what the book is saying here, is that if linkage disequilibrium (correlation) between the two loci that negatively covary is reduced, the amount of genetic variance they explain in the trait will increase! 5.4.1 The evidence Hidden genetic variation is expected to be a natural consequence of stabilising selection, which favours linkage groups for their composite properties without regard to the alleles at individual loci. Theoretical work has suggested that stabilising selection encourages the development of substantial hidden genetic variance, potentially depressing the level of expressed genetic variance to 50% or less than its equilibrium expectation. Of course selection doesn’t always favour an increase in hidden genetic variance. Sometimes coupling selection is favoured, so that expressed genetic variance exceeds equilibrium expectations. In this case the disequilibrium covariance is positive, and recombination would be expected to result in a reduction in the expressed genetic variance. Here we’ve just considered one trait, but of course the same thinking applies to selection upon multiple traits simultaneously. For example, in populations of insects (LIKE BEES) that exploit multiple host plants, one might expect a genetic correlation to evolve such that individuals prefer to feed on the plant species upon which they perform best. Such correlations could result from LD between a set of genes influencing preference and another influencing performance. 5.5 End of chapter questions Define epistasis Describe the terms that will be needed to define the genotypic value \\(G_{ijkl}\\) Complete equations (5.2) and (5.3) Give two important inferences from the fact the total genetic variance will mostly be attributable to the additive genetic variance, even if there are large dominance and epistatic effects What is linkage and linkage disequilibrium? What is the coefficient of linkage disequilibrium? What can influence maintenance of LD? (5 things) What is the relationship between linkage and LD over time? Give two methods of estimating \\(D\\) What are coupling disequlibrium and repulsion disequilibrium and how do they effect genetic variance? Explain equations (5.11) and (5.12) What is hidden genetic variance? How is stabilising selection thought to influence hidden genetic variance? Why? "],
["sources-of-environmental-variation.html", "Chapter 6 Sources of Environmental Variation 6.1 Extension of the linear model to phenotypes 6.2 Special environmental effects 6.3 General environmental effects of maternal origin 6.4 Genotype x environment interaction", " Chapter 6 Sources of Environmental Variation This book divides environmental effects up into 2 different classes: General environmental effects: influential factors that are shared by groups of individuals (they include maternal effects in this) Special environmental effects: residual deviations from the phenotype expected based on genotype and general environmental effects 6.1 Extension of the linear model to phenotypes Here we let \\(E\\) and \\(e\\) denote the contributions of general and specifc environmental effects and \\(I\\) denote GxE. Phenotype for \\(k\\)th individual of the \\(i\\)th genotype exposed to the \\(j\\)th general environmental effect can then be described as a linear function of 4 components: \\[\\begin{equation} z_{ijk} = G_{i} + I_{ij} + E_{j} + e_{ijk} \\tag{6.1} \\end{equation}\\] NOTE to fit an interaction term in R just put the terms into the model and multiply them, e.g. lm(y ~ x*i, data=df). Explaining some terms: \\(I_{ij}\\), \\(E_{j}\\), \\(e_{ijk}\\) are defined in a least-squares sense as deviations from lower-order expectations and so have mean values equal to zero \\(\\mu_{G} = \\overline{z}_{ijk}\\) is the mean phenotype of all genotypes in the population \\(G_i\\) is the expected phenotype of the particular genotype \\(i\\) averaged over all possible environmental conditions \\(\\mu_G + E_{j}\\) is the mean phenotypic value expected if all genotypes were assayed in the \\(j\\)th macroenvironment \\(G_i + I_{ij} + E_{j}\\) is the expected phenotype of individuals with genotype \\(i\\) in the \\(j\\)th macroenvironment \\(e_{ijk}\\) is the deviation from that expected phenotype so, as per least-squares rules, it isn’t correlated with \\(G_i\\), \\(I_{ij}\\) or \\(E_{j}\\) \\(I\\) and \\(e\\) are uncorrelated with other variables (by construction). Remembering that the variance of a sum of uncorrelated variables is just the sum of the variances of each variable (and using equation (3.4)), we can define the phenotypic variance: \\[\\begin{equation} \\sigma^2_{P} = \\sigma^2_{G} + \\sigma^2_{I} + 2\\sigma_{G.E} + \\sigma^2_{E} + \\sigma^2{e} \\tag{6.2} \\end{equation}\\] \\(\\sigma^2_{I}\\) is the GxE variance and \\(\\sigma_{G.E}\\) is the genotype-environment covariance. These terms are quite different. GxE is concerned variation in phenotypic response of specific genotypes within specific environments. Genotype-environment covariance is simply a measure of association between particular environments and genotypes. So, if individuals were randomly distributed across all environments, \\(\\sigma_{G.E} = 0\\), but \\(\\sigma^2_{I}\\) will be non-zero if genotypic and environmental effects are non-additive. Maternal or paternal effects can cause genotype-environment covariance if there is correlation between parental genotype and ability to provision the young. Genotype-environment covariance is often hard to estimate so and often contributes and unknown amount to estimates of genetic variance. 6.2 Special environmental effects Two sources: internal developmental noise and external microenvironmental heterogeneity. 6.2.1 Within-individual variation Can gain some information on within-individual variation by measuring the right and left components of a bilaterally symetrical individual. Pretty difficult to rule out external environmental contributions here though. Total variance of special environmental effects can be written as the sum of within-individual and among-individual environmental components \\[\\begin{equation} \\sigma^2_{e} = \\sigma^2_{ew} + \\sigma^2_{ea} \\tag{6.3} \\end{equation}\\] 3 types of asymmetry: Directional - consistent bias in one direction (e.g. heart being more to the left) Antisymmetry - asymmetry is the rule rather than the exception, but it is nondirectional Fluctuating asymmetry - the difference between left and right measures is symmetrically distributed around a mean and mode of 0 Unbiased estimate of the within-individual variance for a trait: \\[\\begin{equation} \\sigma^2_{ew} = \\sum_{i=1}^{N} \\frac{(r_i - l_i)^2}{2N} - \\sigma^2_{em} \\tag{6.4} \\end{equation}\\] \\(N\\) is the number of individuals sampled, \\(r_i\\) and \\(l_i\\) are the right and left measures for the \\(i\\)th individual, and \\(\\sigma^2{em}\\) is variance due to measurement error. The effects of environmental stress on fluctuating asymmetry are fairly predictable - \\(\\sigma^2_{ew}\\) tends to increase in extreme or novel environments. A study suggested humans suffering from malnutrition show increases in fluctuating asymmetry. 6.2.2 Developmental homeostasis and homozygosity Lerner endorsed the idea that the degree of developmental stability is positively correlated with the overall level of individual heterozygosity. The usual mechanistic explanation is that heterozygosity acts as a buffer against environmental variation. Rest of this section discusses evidence for this hypothesis. It might be a useful exercise to think through how you’d do experiments to test the hypothesis based on the different components of variance that need to be considered. For now, here is the conclusion: “The acceptance of a general causal relationship between heterozygosity and developmental stability should be postponed until additional adequately designed experiments have been performed.” 6.2.3 Repeatability Variance among repeated measures on the same individual can only be due to environmental causes (or measurement errors), so information on the within-individual component of variance can provide some insight into the possible magnitude of the environmental variance for a trait. Time complicates things (phenotypes can vary within individuals at one time and across time), but that aside, the upper-bound estimate of the genetic variance of a trait is provided by: \\[\\begin{equation} \\sigma^2_{G(max)} = \\sigma^2_{z} - \\sigma^2_{ew} \\tag{6.5} \\end{equation}\\] \\(\\sigma^2_{z}\\) is an estimate for the total phenotypic variance for the trait. Measurement error always inflates estimates of within-individual variance. As it contributes to total phenotypic variance, this cancels out in the equation above, but it’s a pain because often we want to know the contribution of genetic variance to the total phenotypic variance. Repeated measures can help correct for measurement error where the measure won’t change over time - e.g. adult limb length. This is less tractable for measures that vary over time as you can’t distinguish variation due natural organismal changes over time and those due to measurement error. Expected value of \\(\\sigma^2_{G(max)}\\) is greater than the total genetic variance for the trait because it includes the among-individual component of variance due to the special environmental effects (\\(\\sigma^2_{ea}\\)) and variance due to general environmental effects (\\(\\sigma^2_{E}\\)). Letting var(e) denote the variance associated with measurement error, the repeatablitity is: \\[\\begin{equation} r = \\frac{\\sigma^2_{z} - \\sigma^2_{ew}} {\\sigma^2_{z} - \\sigma^2_{em}} \\tag{6.6} \\end{equation}\\] and it provides an upper-bound estimate of the broad-sense heritability of a trait (\\(H^2\\)). The degree to which \\(r\\) exceeds \\(H^2\\) depends on the magnitude of \\(\\sigma^2_{ea} + \\sigma^2_{E}\\) relative to \\(\\sigma^2_{ew}\\). If all environmental variance is just within-individual variance and no measurement error is present, then \\(r\\) gives an unbiased estimate of \\(H^2\\). Nice thing is that it gives an upper-bound regardless, so if \\(r\\) is low you can say that the environmental components must dominate. Unfortunately, repeatability is often computed as the correlation between two repeated measures (\\(z_1\\) and \\(z_2\\)) on the same individuals: \\[\\begin{equation} r_F = \\frac{\\sigma(z_1, z_2)} {\\sigma(z_1)\\sigma(z_2)} \\tag{6.7} \\end{equation}\\] and as measurement error is contained in the denominator, it downwardly biases \\(r_F\\). So we are no longer necessarily measuring the upper-bound of \\(H^2\\) :( 6.3 General environmental effects of maternal origin Before thinking of maternal effects on offspring, remember there is little evidence for intrauterine effects on complex traits in humans, quote from GDS’s twitter: “virtually all disease mother-offspring and father-offspring risk concordance the same, except maternal small excess for epilepsy (intrauterine valproate?) and type 2 diabetes” (Link to tweet). Example MR paper. Unless one runs an experiment where the environment of the past generation is the same as the current generation, one runs the risk that observed phenotypes are largely due to past generation. Similar thinking applies to estimates of heritability when there is assortative mating! There are some striking examples of maternal effects in the wild and there are plenty of associations that have been drawn between maternal age and various human traits too, for example the chances of Down’s syndrome increases with maternal age. Lack of data for multigenerational transmission of environmental effects – still a lack of data in 2020 in humans! 6.4 Genotype x environment interaction This part gives examples of experiments done to detect GxE. In the examples, it was possible to make some inference as to the existence of GxE because members of the same genetic groups were evaluated under well-defined treatments. Of course, for natural populations, assigning individuals to discrete environmental groups is often impossible, so GxE becomes unmeasurable because any GxE will be confounded with the environmental source of variance. Interestingly, I think for Wes’s GxE MR paper, they’re able to apply the method in cases without clear discrete environments. "],
["resemblance-between-relatives.html", "Chapter 7 Resemblance between relatives 7.1 Measures of relatedness 7.2 The genetic covariance between relatives 7.3 The effect of linkage and gametic phase disequilibrium 7.4 Assortative mating 7.5 Polyploidy 7.6 Environmental sources of covariance between relatives 7.7 The heritability concept", " Chapter 7 Resemblance between relatives If you ignore GxE you can express the phenotypic values of individuals \\(x\\) and \\(y\\) (recall equation (6.1)) simply as \\(Z_x = G_x + E_x + e_x\\) and \\(Z_y = G_y + E_y + e_y\\). This chapter is interested in the resemblence between relatives, so using these equations we can specify what the covariance between phenotypic values will be: \\[\\begin{equation} \\begin{split} \\sigma_z(x, y) &amp;= \\sigma[(G_x + E_x + e_x), (G_y + E_y + e_y)] \\\\ &amp;= \\sigma_G(x, y) + \\sigma_{G.E}(x, y) + \\sigma_{G.E}(y, x) + \\sigma_{E}(x, y) \\end{split} \\tag{7.1} \\end{equation}\\] Remember, \\(e\\) (special environmental effects) are derived from random residual deviations so are uncorrelated between individuals (think within-individual variation). You can design experiments so all terms with \\(E\\) in them have expected values of 0 and here we’re going to assume that one individual’s genotypic effects are not covarying with the others general environmental effects, i.e. \\(\\sigma_{G.E}(x, y) = \\sigma_{G.E}(y, x) = 0\\). This boils everything nicely down to this simple equation: \\[\\begin{equation} \\sigma_z(x, y) = \\sigma_G(x, y) + \\sigma_{E}(x, y) \\tag{7.2} \\end{equation}\\] \\(\\sigma_G(x, y)\\) will be the focus of things to come! Like genetic variance, the covariance can be split into components attributable to additive, dominance, and epistatic effects. Each term is simply one of the terms used to describe genetic variance (e.g. equation (5.6)), weighted by a coefficient that describes the joint distribution of effects in pairs of relatives. Complications of estimating these coefficients include, non-random mating, LD, assortative mating, sex-linkage, maternal genetic effects and inbreeding. 7.1 Measures of relatedness Relatedness can only be defined with respect to a specified frame of reference as all individuals are related (DUH). From here on the reference population is the base of the observed pedigree. So if the observed data is just trios, then the base population is the parents in those trios. If, data on grandparents is observed then they’re the base population and so-on. Members of the base population are assumed to be unrelated. Also when discussing relatedness we refer to identity by descent (IBD), not identity by state (IBS). Identity by descent and identity by state Genes that are identical by descent are those that have been passed down by a common ancestor. The same gene from two individuals may share the same genetic sequence, making them identical by state, but if they do not derive from the same common ancestor they are not identical by descent. So, genes that are identical by descent must, except for mutations, be identical by state, BUT genes that are identical by state might not be identical by descent. 7.1.1 Coefficients of identity At a single locus in a diploid individual there are two alleles so with two individuals you have four alleles. Each allele is inherited singularly (a gamete only passes on one copy), so has it’s own identity with each of the other three alleles. This means identity within individuals and between individuals can exist. This scenario gives rise to 15 different configurations of identity by descent. Individuals that contain pairs of alleles that are identical by descent are said to be inbred. Ignoring difference between maternally and paternally derived alleles, the number of IBD configurations reduces to nine. These range from a state where all four alleles are identical by descent (two inbred individuals that share a common ancestor) to a state where none of the alleles are identical by descent. In a large population with randomly mating individuals most states don’t exist. The probabilities associated with each of the nine states are called the condensed coefficients of identity. Consider the case of a single gene for two non-inbred full sibs. There is a probability of 0.5 that both sibs inherit the same allele from their father and, independently, the probability they inherit the same allele from their mother is 0.5. So there is a probability of 0.25 that both pairs of alleles are identical by descent (i.e. the alleles inherited from the mother were the same and the alleles inherited from the father were the same.), there is a probability of 0.5 that just one pair is identical by descent and a probability of 0.25 that neither pair are identical by descent. All other states have a probability of 0. 7.1.2 Coefficients of coancestry and inbreeding Suppose single genes (or alleles – side note alleles and genes will probably be used interchangably in this chapter) are drawn randomly from individuals \\(x\\) and \\(y\\). The probability that these two genes are identical by descent, \\(\\Theta_{xy}\\), is the coefficient of coancestry (can be called coefficient of consanguinity or coefficient of kinship). See figure 7.2 in the book for a graphical depiction of the nine IBD state classes and equation 7.2 in the book relates these states to \\(\\Theta_{xy}\\) –&gt; each state is weighted by the conditional probability that a randomly drawn gene from \\(x\\) is identical by descent with a randomly drawn gene from \\(y\\). For an individual, \\(z\\), their inbreeding coefficient (\\(f_z\\)) is equal to their parents coefficient of coancestry (\\(f_z = \\Theta_{xy}\\)). To derive \\(\\Theta_{xy}\\), we first need to derive \\(\\Theta_{xx}\\) (may seem weird, but it ain’t). If you took a gene with two alleles, \\(A_1\\) and \\(A_2\\) and you could know which parent each came from to distinguish them, if you drew one allele at random then replaced it and drew another you could draw \\(A_1\\) twice, \\(A_1\\) then \\(A_2\\), \\(A_2\\) then \\(A_1\\) or \\(A_2\\) twice. If they’re not copies of the same allele (i.e. \\(A_1\\) doesn’t equal \\(A_2\\)) then if \\(A_1\\) is drawn twice it must be identical by descent and the same goes for \\(A_2\\). In this scenario, \\(\\Theta_{xx} = (1/4)(1) + (1/4)(1)\\). Of course, the individual could be inbred and so the probability that \\(A_1\\) and \\(A_2\\) are identical by descent is \\(f_x\\). A general expression for the coefficient of coancestry of an individual with itself is given below in equation (7.3) \\[\\begin{equation} \\Theta_{xx} = \\frac{1}{4}(1 + f_x + f_x + 1) = \\frac{1}{2}(1 + f_x) \\tag{7.3} \\end{equation}\\] Something to note, the fact this coefficient increases with inbreeding is used (kind of) in current SNP-heritability estimations. After creating a kinship matrix, the diagonal of the matrix represents \\(2\\Theta_{xx}\\). Due to the software, it will always be centred on one (mean of one) and without inbreeding expect a normal distribution with small variation around the mean, and with inbreeding this distribution would be right-skewed and you can identify inbred individuals by assessing which individuals diagonal elements are significantly larger than one. Parent (\\(p\\)) and offspring (\\(o\\)) scenario now!! If neither are inbred (\\(p\\)’s parents unrelated and they are unrelated to their mate), then that makes things simple. When drawing one of the two alleles from the mother and then one of the two alleles from the offspring, there is only one scenario in which they are the same. As each scenario has an equal probability of occurring, \\(\\Theta_{po} = \\frac{1}{4}\\). If \\(p\\) is inbred, probability of their alleles being identical by descent is \\(f_p\\). This is the same as the probability of the offspring allele being identical by descent to the maternal allele the offspring did not inherit. Probability of drawing inherited allele from offspring and allele not passed on from parent is (like the others) 1/4. Therfore, inbreeding inflates the coefficient of coancestry to \\(\\Theta_{po} = \\frac{f_p}{4} + \\frac{1}{4} = \\frac{1 + f_p}{4}\\). Complete inbreeding means \\(f_p = 1\\) so \\(\\Theta_{po} = \\frac{1}{2}\\). By thinking about probability of picking paternally derived allele and stuff you can add in \\(f_o\\) (see book page 136) and the general expression is given in equation (7.4) below. \\[\\begin{equation} \\Theta_{po} = \\frac{1}{4}(1 + f_p + 2f_o) \\tag{7.4} \\end{equation}\\] Often in the literature \\(\\Theta_{po}\\) is considered to simply be 1/4. So no inbreeding is assumed. Full sibs time! \\(m\\) = mother, \\(f\\) = father, \\(x\\) = kid1 and \\(y\\) = kid2. So if \\(m\\) and \\(f\\) are not inbred or related themselves then there are two situations from which a child could inherit the same allele by descent. Either that allele has come from \\(m\\) or that allele has come from \\(f\\). Both have the same probability so let’s just look at \\(m\\) -&gt; \\(x\\) and \\(m\\) -&gt; \\(y\\). The probability \\(x\\) and \\(y\\) receive the same maternal allele is 1/2 (i.e. the coefficient of coancestry of the mum with herself, \\(\\Theta_{mm}\\)). The probability of randomly drawing the maternally inherited allele from \\(x\\) is 1/2 and the same is true for \\(y\\). Therefore the probability of drawing one allele from \\(x\\) and one from \\(y\\), that are identical by descent, passed down from \\(m\\) is \\(\\Theta_{mm}/4 = 1/8\\). Adding contribution from el dado, we get \\(\\Theta_{xy} = 1/4\\) EASY! Appendix 2 contains path analysis, developed by Sewall Wright, that can derive these results. Now we allow for inbreeding of parents so introduce \\(f_m\\) and \\(f_f\\). There are still only two paths that lead to alleles identical by descent in \\(x\\) and \\(y\\) (they’re just more likely with inbreeding because alleles within the father and within the mother are more likely to be identical by descent, i.e. \\(\\Theta_{mm}\\) &gt; 1/2 and so is \\(\\Theta_{ff}\\) if there is inbreeding). Including these terms gives: \\[\\begin{equation} \\Theta_{xy} = \\frac{1}{4}(\\Theta_{mm} + \\Theta_{ff}) = \\frac{1}{4}(\\frac{1 + f_m}{2} + \\frac{1 + f_f}{2}) = \\frac{1}{8}(2 + f_m + f_f) \\notag \\end{equation}\\] By taking into account inbreeding coefficients of kids, we end up with: \\[\\begin{equation} \\Theta_{xy} = \\frac{1}{8}(2 + f_m + f_f + 4\\Theta_{mf}) \\notag \\tag{7.5} \\end{equation}\\] Under random mating \\(\\Theta_{xy} = 1/4\\). These techniques can be extended to more distant relatives and more complicated schemes of relatedness. The coefficient of coancestry is always the sum of a series of two types of paths between \\(x\\) and \\(y\\). The first type of path leads from a single common ancestor to the two individuals of interest, while the second type passes through two remote ancestors that are related to each other. Neither type of path is allowed to pass through the same ancestor more than once. This procedure is summarised by equation (7.6) below. \\[\\begin{equation} \\Theta_{xy} = \\sum_{i}\\Theta_{ii}(\\frac{1}{2})^{n_i - 1} + \\sum_{j}\\sum_{j \\ne k}\\Theta_{jk}(\\frac{1}{2})^{n_{jk} - 2} \\tag{7.6} \\end{equation}\\] where \\(n_i\\) is the number of individuals (including \\(x\\) and \\(y\\)) in the path leading from common ancestor \\(i\\), and \\(n_{jk}\\) is the number of individuals (including \\(x\\) and \\(y\\)) on the path leading from two different but related ancestors, \\(j\\) and \\(k\\). Been assuming autosomal genes until this point! Sex-linked genes means we have to change things a bit. See book for deets (only 1 paragraph). 7.1.3 The coefficient of fraternity Before we were focusing on single alleles, but it is also of importance to consider the probability that two individuals (again \\(x\\) and \\(y\\)) contain genotypes that are identical by descent, this is called the coefficient of fraternity, \\(\\Delta_{xy}\\). Mother of \\(x\\) = \\(m_x\\) and father of \\(y\\) = \\(f_y\\) etc. There are four combinations of parents (excluding the current combination). There are two ways in which the genotype of \\(x\\) can match the genotype of \\(y\\) (1) If allele from \\(m_x\\) and \\(m_y\\) are identical by descent and the allele from \\(f_x\\) and \\(f_y\\) are identical by descent (2) allele from \\(m_x\\) is identical by descent with allele from \\(f_y\\) and allele from \\(f_x\\) is identical by descent with that from \\(m_y\\). So the coefficient of fraternity can be defined as \\[\\begin{equation} \\Delta_{xy} = \\Theta_{m_{x}m_{y}}\\Theta_{f_{x}f_{y}} + \\Theta_{m_{x}f_{y}}\\Theta_{f_{x}m_{y}} \\tag{7.7} \\end{equation}\\] For full sibs (\\(m_x\\) = \\(m_y\\) = \\(m\\) and \\(f_x\\) = \\(f_y\\) = \\(f\\)) equation (7.7) reduces to: \\[\\begin{equation} \\Delta_{xy} = \\Theta_{mm}\\Theta_{ff} + \\Theta^2_{mf} \\tag{7.8} \\end{equation}\\] If the parents are unrelated \\(\\Theta_{mf} = 0\\); and if the parents are not inbred \\(\\Theta_{mm} = \\Theta_{ff} = 1/2\\) so \\(\\Delta_{xy} = 1/4\\). Now for paternal half-sibs, where fathers are the same (i.e. \\(f_x\\) = \\(f_y\\) = \\(f\\)). \\[\\begin{equation} \\Delta_{xy} = \\Theta_{m_{x}m_{y}}\\Theta_{ff} + \\Theta_{m_{x}f}\\Theta_{fm_{y}} \\tag{7.9} \\end{equation}\\] If parents are unrelated then \\(\\Theta_{ff} = 1/2\\) and \\(\\Theta_{m_{x}m_{y}} = \\Theta_{m_{x}f} = \\Theta_{fm_{y}} = 0\\) so \\(\\Delta_{xy} = 0\\). So the genotypes of two individuals can’t be identical by descent if their maternally (or paternally) derived alleles come from unrelated individuals. 7.2 The genetic covariance between relatives To assess genetic covariance between individuals we must use what we learnt from partitioning genetic variance in Chapter 5. To start with we’ll make a lot of assumptions: diploid autosomal loci only random mating loci are unlinked and are not in LD no genetic variation for maternal effects no GxE or genotype-environment covariance no sexual dimorphism no selection Recalling equation (5.5), we can see the complete genotypic value for two loci, which will be the same for individuals \\(x\\) and \\(y\\). The multilocus genetic variance for each individual is then given in equation (5.6). As different types of effects (e.g. additive and dominance) are uncorrelated within individuals, they’re also uncorrelated between individuals. So the genetic covariance is simply: \\[\\begin{equation} \\sigma_G(x, y) = \\sigma_A(x, y) + \\sigma_D(x, y) + \\sigma_{AA}(x, y) + \\sigma_{AD}(x, y) + \\sigma_{DD}(x, y) + ... \\tag{7.10} \\end{equation}\\] Remember \\(\\sigma^2(x) == \\sigma(x,x)\\) Then the book expresses terms in equation (7.10) in terms of variance components and coefficients of relationships. Confusing, come back to this! The genetic covariance between relatives becomes (equation 7.12 in book): \\[\\begin{equation} \\begin{split} \\sigma_{G}(x, y) &amp;= \\sum(2\\Theta_{xy})^n \\Delta^{m}_{xy}\\sigma^2_{A^{n}D^{m}} \\\\ &amp; = 2\\Theta_{xy}\\sigma^2_A + \\Delta_{xy}\\sigma^2_{D} + (2\\Theta_{xy})^2\\sigma^2_{AA} + 2\\Theta_{xy}\\Delta_{xy}\\sigma^2_{AD} + \\Delta^2_{xy}\\sigma^2_{DD} + (2\\Theta_{xy})^3\\sigma^2_{AAA} + ... \\end{split} \\tag{7.11} \\end{equation}\\] Using values from Table 7.1 in the book you can then derive explicit expressions for the genetic covariances among common types of relative - these are in Table 7.2! These values show some interesting things: Gene action involving dominance only rarely contributes to the covariance between relatives. It requires that each parent of \\(x\\) be related to a different parent of \\(y\\). Such relationships (full sibs, double first cousins, and monozygotic twins) are said to be collateral. The coefficient for ^2_{AA} declines more rapidly with the distance of the relationship than does that for ^2_{A}. In the same way that the lack of variance that dominance and epistatic effects contribute to total genetic variance, doesn’t tell us anything about how the genes being measured function, the lack of variance dominance and epistatic effects contribute to the genetic covariance between relatives tells us nothing about how the function of the measured genes contributes to resemblance between relatives. WRITE THESE DOWN! 7.3 The effect of linkage and gametic phase disequilibrium Under the assumption of linkage equilibrium, linkage influences on the epistatic components of genetic covariance. COME BACK TO THIS SECTION… if only epistatic components relavent, then I don’t really care tbh 7.3.1 Gametic phase disequilibrium Somehow (work this out later), the book extrapolates from equations (5.11) and (5.12) to give the total genetic variance in the presence of ld: \\[\\begin{equation} \\sigma^2_{G} = \\sigma^2_{A} + \\sigma_{A,A} + \\sigma^2_{D} + \\sigma_{D,D} \\tag{7.12} \\end{equation}\\] where \\(\\sigma_{A,A}\\) is the contribution due to covariance of additive effects of nonalleles within gametes (the additive disequilibrium covariance), and \\(\\sigma_{D,D}\\) is the contribution of covariance due to dominance effects of different loci within individuals (the dominance disequilibrium covariance). For situations in which the study population is being maintained in a steady state of disequilibrium from generation to generation (by processes such as natural selection, migration, and/or non-random mating), the modifications needed to estimate genetic covariance between relatives is simple - just reokace \\(\\sigma^2_{A}\\) with \\(\\sigma^2_{A} + \\sigma_{A,A}\\) and \\(\\sigma^2_{D}\\) with \\(\\sigma^2_{D} + \\sigma_{D,D}\\). However, when mating is random and forces acting on the maintenance of disequilibria are dampened, then covariance between relatives changes with time as the recombination frequency (REMEMBER THAT BAD BOY FROM CHAPTER 5) causes a gradual decay of disequilibrium. Now we consider two loci, remember the pair of alleles (here \\(i\\) and \\(j\\) or \\(k\\) and \\(l\\)) are inherited from different gametes. The additive genetic variance is simply: \\[\\begin{equation} \\sigma^2_{A} = E(\\alpha^2_i) + E(\\alpha^2_j) + E(\\alpha^2_k) + E(\\alpha^2_l) \\tag{7.13} \\end{equation}\\] additive disequilibrium covariance in the base population is: \\[\\begin{equation} \\sigma_{A,A}(0) = 2E(\\alpha_i\\alpha_k) + 2E(\\alpha_j\\alpha_l) \\tag{7.14} \\end{equation}\\] Under the assumption of random mating \\(E(\\alpha_i\\alpha_j)\\) and \\(E(\\alpha_k\\alpha_l)\\) would be zero because there should be no correlation between the gametes of random, unrelated mates. As we’re assuming no forces are acting to maintain ld between alleles, we can assume it decays as given by (5.9). Thus, the decay of the additive disequilibrium covariance can be given by: \\[\\begin{equation} \\sigma_{A,A}(t) = (1 - c)^{t}\\sigma_{A,A}(0) \\tag{7.15} \\end{equation}\\] Unclear how the book then gets equation 7.15D, but then goes on to say that the additive disequilibrium covariance between relatives is given by: \\[\\begin{equation} \\sigma_{A,A}(x, y, t) = 2\\Theta_{xy}(1 - c)^{t}\\sigma_{A,A}(0) \\tag{7.16} \\end{equation}\\] where \\(t\\) denotes the number of generations that the common ancestors are removed from the base population. Thus, for example, the parent-offspring covariance (_{xy} = 1/4) resulting from additive disequilibrium covariance is \\(\\sigma_{A,A}(0)/2\\) if the parents are from the base population. Then for each generation down the parents are thne multiply that by \\((1 - c)\\), so if they were fourth generation it’d be \\((1-c)^3\\sigma_{A,A}(0)/2\\). Derivation of the covariance between relatives resulting from dominance disequilibrium covariance follows the same logic just presented. However, in this case, the dominance disequilibrium covariance declines each generation to \\((1 - c)^2\\) it’s previous value. See book for why COME BACK TO THIS! See Table 7.3 in the book for disequilibrium covariances of the base-population among relatives. COME BACK TO FINAL PAGE OF THIS SECTION! 7.4 Assortative mating Assortative mating isn’t uncommon, especially positive assortative mating, whereby individuals choose mates that have phenotypes that resemble their own. Lots of examples of this in human populations. Many assortative mating systems are selective, such that some phenotypes of one or both sexes have a greater ability to attract mates than others, but this is beyond the scope of this book. Here we assume all individuals have an equal opportunity to reproduce, the only limitation being the phenotypic distribution of available mates (i.e. nonselective assortative mating). Difference between inbreeding and assortative mating: inbreeding is choice of mates by similar genotypes and assortative mating is choice of mate by similar phenotypes. Of course, if the trait being assorted on has some genetic component (i.e. every fucking trait), then assortative mating will increase the chances that mates share identical alleles, BUT in all practical scenarios (especially for humans), it won’t cause extreme genetic structures like a completely homozygous population, like inbreeding can. Inbreeding can inflate the genetic variance of a population up to twofold in the abscence of epistasis (Properties of single loci), but strong assortative mating can cause an even larger increase (of course disassortative mating induces a reduction of the genetic variance). Positive assortative mating increases the coupling of genes with similar effects. How assortative mating influences additive genetic variance: Let’s define \\(\\rho_z\\) and \\(\\rho_g\\) to be the phenotypic and genetic correlations between mates and assume the regression of phenotypes of mates is linear. We also suppose that there are \\(n\\) loci, each contributing equally to the genetic variance of the trait and define the parameter \\(\\gamma = 1 - [1/(2n)]\\). With a random-mating base population with no LD and additive genetic variance \\(\\sigma^2_{A}\\), a single generation of assortative mating will shift the additive genetic variance to: \\[\\begin{equation} \\sigma^2_A(1) = (1 + \\frac{\\rho_z h^2} {2}) \\sigma^2_A \\tag{7.17} \\end{equation}\\] where \\(h^2 = \\frac{\\sigma^2_A} {\\sigma^2_z}\\). With continued assortative mating the variance asymptotically approaches the equilibrium: \\[\\begin{equation} \\hat{\\sigma}^2_A = \\frac{\\sigma^2_A} {1 - \\gamma\\hat{\\rho_g}} \\tag{7.17} \\end{equation}\\] where \\(\\hat{\\rho_g} = \\rho_{z}\\hat{\\sigma}^2_A/\\hat{\\sigma}^2_z\\) is the equilibrium genetic correlation between mates. This can be re-written as: \\[\\begin{equation} TO-DO!!! \\end{equation}\\] which gives the inflation of the additive equilibrium genetic variance relative to that in the random-mating base population (RECREATE FIGURE 7.7!). The difference \\(\\hat{\\sigma}^2_A - \\sigma^2_A\\) is the additive disequilibrium covariance, _{A,A}, maintained by assortative mating. (UNCLEAR AS TO WHY!) You can see from the figure, assortative mating must be fairly strong (\\(\\hat{\\rho_g} \\ge 0.2\\)) and combined with high \\(h^2\\) to induce much change in the variance. Also, the effective number of loci has a negligible effect unless it is very small. When only two loci contribute to the trait, \\(\\sigma^2_D\\) can change with assortative mating, but if the number of loci is even “moderate” the effect will be negligible so can be ignored for all complex human phenotypes for sure! In addition to creating LD, assortative mating causes genotype frequencies for the selected trait to deviate from the Hardy-Weinberg expectations. Positive assortative mating leads to an increase in homozygosity and negative assortative mating has the opposite effect. However, \\(\\rho_z\\) and \\(h^2\\) need to be very large to cause any major deviations from HWE. This implies the vast majority of genetic variance change induced by assortative mating comes in the form of increased LD, not changes in HWE, i.e. allelic association within rather than between gametes. Table 7.4 in the book gives changes in additive and dominance components of covariance between relatives for an equilibrium population undergoing assortative mating. 7.5 Polyploidy BORWANG!!! Ignoring! 7.6 Environmental sources of covariance between relatives This section essentially uses logic derived from path analysis to try and derive expressions for the environmental sources of covariance between various relatives including twins, full sibs and half sibs. To best follow the logic a deeper a look at path analysis and more time on the maths is likely required. COME BACK TO THIS! 7.7 The heritability concept Recalling that the first term in any genetic covariance expression is \\(2\\Theta_{xy}\\sigma^2_{A}\\). Thus, under the assumption that additive genetic variance is the dominant source of phenotypic covariance, \\[\\begin{equation} h^2 = \\frac{\\sigma(z_x, z_y)} {2\\Theta_{xy} \\sigma^2_z} \\tag{7.18} \\end{equation}\\] should provide a good approximation to the heritability. Violations of assumptions usually cause \\(\\sigma(z_x, z_y) / 2\\Theta_{xy}\\) to be an upwardly biased estimator of \\(\\sigma^2_A\\). A simple means of evaluating the likelihood of bias in heritability estimates arises when estimates of the phenotypic covariance are available for more than one type of relative (the book provides a couple of examples). The book also discusses lab vs. natural population estimates of heritability and gives examples. Conclusion: heritabilities respond to environmental change, and that substantial care should be taken in extrapolating results beyond the environment in which they are obtained! EVERY TRAIT IS HERITABLE INIT! (pretty much) In general, morphological characteristics tend to have higher heritabilities than life-history traits, with behavioural and physiological traits having intermediate heritabilites. Although there are plenty of exceptions of course. 7.7.1 Evolvability Useful to have a dimensionless parameter to compare evolvability of traits/species, and one such measure is the heriability. Remember equation (3.14). The change in the mean relative to the phenotypic standard deviation provides another useful descriptor of evolvability, \\[\\begin{equation} \\frac{\\Delta\\mu} {\\sigma_z} = h^2i \\tag{7.19} \\end{equation}\\] where \\(i = S / \\sigma_z\\) is the standardised selection differential, i.e. the change in the mean caused by selection in units of phenotypic standard deviations. Like in equation (3.14), the heritability here provides a measure of the efficiency of response to selection. There are several ways to measure evolvability, each of which has its own merits in particular contexts. All are interchangable given phenotypic variance, additive genetic variance, and mean phenotype. Here, Fisher’s fundamental theorem of natural selection is derived by dividing both sides of equation (3.14) by \\(\\mu\\) and substituting in \\(i\\), and considering the case in which the trait of interest is fitness (\\(W\\)). Remember, the selection differential is equivalent to the phenotypic covariance between the character and relative fitness ($w = W/{W}), i.e. (3.10), where \\(\\bar{W}\\) is mean fitness on an absolute scale. If \\(z\\) is fitness then \\(S = \\sigma^2_z(W)\\), where \\(\\sigma^2_z(W)\\) is the phenotypic variance of fitness and we can say: \\[\\begin{equation} \\frac{\\Delta\\bar{W}} {\\bar{W}} = \\frac{\\sigma^2_A(W)} {\\bar{W}^2} = \\sigma^2_A(w) \\tag{7.20} \\end{equation}\\] where \\(\\sigma^2_A(W)\\) and \\(\\sigma^2_A(w)\\) are the additive genetic variances of absolute and relative fitness. Thus, the proportional rate of evolution in mean fitness is equal to the squared coefficient of additive genetic variation of absolute fitness, or equivalently, to the additive genetic variance of relative fitness. "],
["introduction-to-matrix-algebra-and-linear-models.html", "Chapter 8 Introduction to Matrix Algebra and Linear Models 8.1 Multiple regression 8.2 Elementary matrix algebra 8.3 Expectations of random vectors and matrices 8.4 Covariance matrices of transformed vectors 8.5 The multivariate normal distribution 8.6 Overview of linear models 8.7 Generalised least squares", " Chapter 8 Introduction to Matrix Algebra and Linear Models \\[\\newcommand{\\mx}[1]{\\mathbf{#1}}\\] 8.1 Multiple regression Simple multiple regression equation: \\[\\begin{equation} y = \\alpha + \\beta_1z_1 + \\beta_2z_2 + \\dots + \\beta_nz_n + e \\tag{8.1} \\end{equation}\\] \\(y\\) = dependent/response variable, \\(z_1, z_2, z_n\\) = predictors, \\(e\\) = residual error, \\(\\alpha\\) is a constant as are \\(\\beta_1, \\beta_2, \\beta_n\\) to be estimated. Recall from Chapter 3 that the goal of least-squares regression is to find a set of constants (\\(\\alpha\\) and the \\(\\beta\\)s) that minimise the squared differences between observed and expected values, with expected values is anything that fits on the “line of best fit”. Also, recall equation (3.6), to see the relationship between \\(y\\), \\(z_n\\) (\\(x\\) in the equation) and \\(b\\). For multiple regression there are many “\\(b\\)” terms and each of them can be estimated by dividing the covariance of the dependent variable and the predictor (\\(\\sigma(y, z_n)\\)) by the covariance of the predictor with all other predictors in the model. When \\(n = 1\\), the model reduces to a simple linear regression and we return to equation (3.6). This can be represented in matrix form like so: \\[\\begin{equation} \\begin{pmatrix} \\sigma^2(z_1) &amp; \\sigma(z_1, z_n) &amp; \\dots &amp; \\sigma(z_1, z_n) \\\\ \\sigma(z_1, z_1) &amp; \\sigma^2(z_2) &amp; \\dots &amp; \\sigma(z_2, z_n) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma(z_1, z_n) &amp; \\sigma(z_2, z_n) &amp; \\dots &amp; \\sigma^2(z_n) \\end{pmatrix} \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_n \\end{pmatrix} = \\begin{pmatrix} \\sigma(y, z_1) \\\\ \\sigma(y, z_2) \\\\ \\vdots \\\\ \\sigma(y, z_n) \\end{pmatrix} \\tag{8.2} \\end{equation}\\] When estimating each response variable-predictor covariance term, it is the sum of predictor covariance multiplied by beta. If the covariance matrix and the vectors of (8.2) are written as \\(\\mx{V}\\), \\(\\mx{\\beta}\\) and \\(\\mx{c}\\) respectively, then the equation can be re-written as: \\[\\begin{equation} \\mx{V\\beta} = \\mx{c} \\tag{8.3} \\end{equation}\\] NOTE: It is standard procedure to denote matrices as bold capital letters and vectors as bold lower case letters. Before going onto matrix methods in more detail, here is an application of (8.1) in quantitative genetics. 8.1.1 An application to multivariate selection Suppose that a large number of individuals in a population have been measured for \\(n\\) characters and for fitness. Individual fitness can then be approximated by the linear model \\[\\begin{equation} w = \\alpha + \\beta_1z_1 + \\beta_2z_2 + ... + \\beta_nz_n + e \\tag{8.4} \\end{equation}\\] where \\(w\\) is the relative fitness (observed fitness divided by the mean fitness in the population). In Chapter 3, we learnt that the selection differential for the \\(i\\)th trait is defined as the covariance between phenotype and relative fitness, \\(S_i = \\sigma(z_i, w)\\). Therefore, if we use multiple regression to estimate \\(S_i\\) we’d end up with: \\[\\begin{equation} S_i = \\beta_i\\sigma^2(z_i) + \\sum^n_{j \\neq i} {\\beta_j\\sigma(z_i, z_j)} \\tag{8.5} \\end{equation}\\] Simple! 8.2 Elementary matrix algebra 8.2.1 Basic notation Vectors and matrices in mathematics are just like those in R. A matrix with the same number of rows and columns is called a square matrix. Vectors written vertically are called column vectors, e.g. \\[\\begin{equation} \\mx{a} = \\begin{pmatrix} 12 \\\\ 13 \\\\ 47 \\end{pmatrix} \\notag \\end{equation}\\] and those that are written horizontally are called row vectors, e.g. \\[\\begin{equation} \\mx{b} = \\begin{pmatrix} 12 &amp; 13 &amp; 47 \\end{pmatrix} \\notag \\end{equation}\\] Single numbers by themselves are often referred to as scalars. A matrix can be described by the elements that comprise it, with \\(M_ij\\) denoting the element in the \\(i\\)th row and \\(j\\)th column of matrix \\(\\mx{M}\\). 8.2.2 Partitioned matrices It is often useful to work with partitioned matrices wherein each element in a matrix is itself a matrix. There are several ways to partition a matrix, for example: \\[\\begin{equation} \\mx{c} = \\begin{pmatrix} 3 &amp; 1 &amp; 2 \\\\ 2 &amp; 5 &amp; 4 \\\\ 1 &amp; 1 &amp; 2 \\end{pmatrix} = \\begin{pmatrix} 3 &amp; \\vdots &amp; 1 &amp; 2 \\\\ \\dots &amp; \\dots &amp; \\dots &amp; \\dots \\\\ 2 &amp; \\vdots &amp; 5 &amp; 4 \\\\ 1 &amp; \\vdots &amp; 1 &amp; 2 \\end{pmatrix} = \\begin{pmatrix} \\mx{a} &amp; \\mx{b} \\\\ \\mx{d} &amp; \\mx{B} \\end{pmatrix} \\notag \\end{equation}\\] where \\[\\begin{equation} \\mx{a} = \\begin{pmatrix} 3 \\end{pmatrix} ,\\ \\mx{b} = \\begin{pmatrix} 1 &amp; 2 \\end{pmatrix} ,\\ \\mx{d} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} ,\\ \\mx{B} \\begin{pmatrix} 5 &amp; 4 \\\\ 1 &amp; 2 \\end{pmatrix} \\notag \\end{equation}\\] There are also other simple ways to partition matrices. 8.2.3 Addition and subtraction To add matrices, they must have the same dimensions. Just add the corresponding elelments, so for adding matrices \\(\\mx{A}\\), and \\(\\mx{B}\\) to make \\(\\mx{C}\\), it is simply \\(C_{ij} = A_{ij} + B_{ij}\\). Subtraction is defined similarly. Can add examples here if you really want to 8.2.4 Multiplication To multiply matrix \\(\\mx{M}\\) by scalar \\(a\\), then just multiply each element of \\(\\mx{M}\\) by \\(a\\). Dot product of two vectors, \\(\\mx{a} \\cdot \\mx{b}\\), is a scalar given by \\[\\begin{equation} \\mx{a} \\cdot \\mx{b} = \\sum^n_{i=1} {a_ib_i} \\notag \\end{equation}\\] For example, for the two vectors given by \\[\\begin{equation} \\mx{a} = \\begin{pmatrix} 1 &amp; 2 &amp; 3 &amp; 4 \\end{pmatrix} \\ \\ \\ \\mathrm{and} \\ \\ \\ \\mx{b} = \\begin{pmatrix} 4 \\\\ 5 \\\\ 7 \\\\ 9 \\end{pmatrix} \\notag \\end{equation}\\] the dot product \\(\\mx{a} \\cdot \\mx{b}\\) = (1 x 4) + (2 x 5) + (3 x 7) + (4 x 9) = 71. Dot product is not defined if the vectors have different lengths. Now consider the matrix \\(\\mx{L} = \\mx{M}\\mx{N}\\) produced by multiplying the \\(r\\) x \\(c\\) matrix \\(\\mx{M}\\) by the \\(c\\) x \\(b\\) matrix \\(\\mx{N}\\). Partitioning \\(\\mx{M}\\) as a column vector of \\(r\\) row vectors, \\[\\begin{equation} \\mx{M} = \\begin{pmatrix} \\mx{m_1} \\\\ \\mx{m_2} \\\\ \\vdots \\\\ \\mx{m_r} \\end{pmatrix} \\ \\ \\ \\mathrm{where}\\ \\ \\ \\mx{m_i} = \\begin{pmatrix} M_{i1} &amp; M_{i2} &amp; \\dots &amp; M_{ic} \\end{pmatrix} \\notag \\end{equation}\\] and \\(\\mx{N}\\) as a row vector of \\(b\\) column vectors, \\[\\begin{equation} \\mx{N} = \\begin{pmatrix} \\mx{n_1} &amp; \\mx{n_2} &amp; \\dots &amp; \\mx{n_b} \\end{pmatrix} \\ \\ \\ \\mathrm{where}\\ \\ \\ \\mx{n_j} = \\begin{pmatrix} N_{1j} \\\\ N_{2j} \\\\ \\vdots \\\\ N_{cj} \\\\ \\end{pmatrix} \\notag \\end{equation}\\] the \\(ij\\)th element of \\(\\mx{L}\\) is given by the dot product \\[\\begin{equation} L_{ij} = \\mx{m_i} \\cdot \\mx{n_j} = \\sum^c_{k=1} {M_{ik}N_{kj}} \\tag{8.6} \\end{equation}\\] Use this to write out matrix \\(\\mx{L}\\) – cba right now… To be definied, the number of columns in the first matrix must equal the number of rows in the second matrix. This means that unless the two matrices are square, it is only possible to multiply them together one way round and not the other (e.g. \\(\\mx{M}\\mx{N}\\) may be defined, but \\(\\mx{N}\\mx{M}\\) won’t). Writing \\(\\mx{M}_{r \\times c}\\mx{N}_{c \\times b} = \\mx{L}_{r \\times b}\\) shows the inner indices must match, while the outer indices give the number of rows and columns of the resulting matrix. Even if the matrices being multiplied are square, the order in which they are multiplied is important, i.e. multiplying \\(\\mx{A}\\) by \\(\\mx{B}\\) is not the same as vice versa. So there is terminology to help differentiate what is being multiplied by what. In the example just given, one would say matrix \\(\\mx{B}\\) is premultiplied by matrix \\(\\mx{A}\\), or that matrix \\(\\mx{A}\\) is postmultiplied by matrix \\(\\mx{B}\\). 8.2.5 Transposition The transpose of a matrix \\(\\mx{A}\\) is written as \\(\\mx{A}^T\\) or \\(\\mx{A}&#39;\\). It is obtained by simply switching the rows and columns of the matrix. Add example here if you want A useful identity is \\[\\begin{equation} (\\mx{A}\\mx{B}\\mx{C})^T = \\mx{C}^T\\mx{B}^T\\mx{A}^T \\tag{8.7} \\end{equation}\\] which holds for any number of matrices. Vectors tend to be written as column vectors, and therefore will be written as such henceforth, and as lowercase bold letters, e.g. \\(\\mx{a}\\), for a column vector and \\(\\mx{a}^T\\) for the corresponding row vector. Further, when multiplying vectors we can assess the inner product (dot product), which yields a scalar and the outer product, which yields a matrix. For the two \\(n\\)-dimensional column vectors \\(\\mx{a}\\) and \\(\\mx{b}\\), \\[\\begin{equation} \\mx{a} = \\begin{pmatrix} a_1 \\\\ \\vdots \\\\ a_n \\end{pmatrix} \\ \\ \\ \\mx{b} = \\begin{pmatrix} b_1 \\\\ \\vdots \\\\ b_n \\end{pmatrix} \\notag \\end{equation}\\] the inner product is given by \\[\\begin{equation} \\begin{pmatrix} a_1 &amp; \\dots &amp; a_n \\end{pmatrix} \\begin{pmatrix} b_1 \\\\ \\vdots \\\\ b_n \\end{pmatrix} = \\mx{a}^T\\mx{b} = \\sum^n_{i=1} {a_ib_i} \\tag{8.8} \\end{equation}\\] while the outer product yields an \\(n \\times n\\) matrix \\[\\begin{equation} \\begin{pmatrix} a_1 \\\\ \\vdots \\\\ a_n \\end{pmatrix} \\begin{pmatrix} b_1 &amp; \\dots &amp; b_n \\end{pmatrix} = \\mx{a}\\mx{b}^T = \\begin{pmatrix} a_1b_1 &amp; a_1b_2 &amp; \\dots &amp; a_1b_n \\\\ a_2b_1 &amp; a_2b_2 &amp; \\dots &amp; a_2b_n \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_nb_1 &amp; a_nb_2 &amp; \\dots &amp; a_nb_n \\end{pmatrix} \\tag{8.9} \\end{equation}\\] 8.2.6 Inverses and solutions to systems of equations Inverting a matrix is an operation similar to division of scalars. Multiplying an matrix by its inverse gives the identity matrix, \\(\\mx{I}\\), which is a matrix where the diagonals are 1 and all other elements are 0. This plays the same role as the number 1 in scalar multiplication and division. The notation for the inverse of matrix \\(\\mx{A}\\) would be \\(\\mx{A}^{-1}\\) and \\(\\mx{A}\\mx{A}^{-1} = \\mx{I}\\). This is useful for solving equations containing matrices, e.g. if you want to divide both sides of the equation by a matrix. A matrix is called nonsingular if its inverse exists. A useful property of inverses is that if the matrix product \\(\\mx{AB}\\) is a square matrix (where \\(\\mx{A}\\) and \\(\\mx{B}\\) are both square), then \\[\\begin{equation} (\\mx{AB})^{-1} = \\mx{B}^{-1}\\mx{A}^{-1} \\tag{8.10} \\end{equation}\\] When you have a non-square or singular matrix you can still obtain solutions to equations containing the matrix by using generalised inverses, but such solutions are not unique. See appendix 3 in the book for deets. Recalling equation (8.3), we can see that the solution (\\(\\mx{\\beta}\\)) can be expressed as \\[\\begin{equation} \\mx{\\beta} = \\mx{V}^{-1}\\mx{c} \\tag{8.11} \\end{equation}\\] Likewise, for the Pearson-Lande-Arnold regression giving the best linear prediction of fitness, \\[\\begin{equation} \\mx{\\beta} = \\mx{P}^{-1}\\mx{s} \\tag{8.12} \\end{equation}\\] where \\(\\mx{P}\\) is the covariance matrix for phenotypic measures \\(z_1, ..., z_n\\), and \\(\\mx{s}\\) is the vector of selection differentials for the \\(n\\) characters. Before going on to the formal expression for inverting a matrix, let’s have a look at some sweet sweet examples of extreme, but hella useful cases that lead to simple expressions for the inverse, shall weeeeee? First up, if the matrix is diagonal (all off-diagonal elements are zero), then the matrix inverse is also diagonal, with \\(\\mx{A}^{-1}_{ii} = 1/A_{ii}\\), i.e. just raise the diagonal elements to the power of negative one. Simple! Of course, if any diagonal elements are el zilcho, then it wonee work as you canee divide by zero (inverse is undefined). Second, for any 2 x 2 matrix \\(\\mx{A}\\), \\[\\begin{equation} \\mx{A} = \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix} \\ \\ \\ \\mathrm{then}\\ \\ \\ \\mx{A}^{-1} = \\frac{1} {ad - bc} \\begin{pmatrix} d &amp; -b \\\\ -c &amp; a \\end{pmatrix} \\tag{8.13} \\end{equation}\\] If \\(ad = bc\\), the inverse doesn’t exist as division by zero is undefined! 8.2.7 Determinants and minors F0r a \\(2 \\times 2\\) matrix, the quantity \\[\\begin{equation} |\\mx{A}| = A_{11}A_{22} - A_{12}A_{21} \\tag{8.14} \\end{equation}\\] is called the determinant, which more generally is denoted by det(\\(\\mx{A}\\)) or \\(|\\mx{A}|\\). \\(\\mx{A}^{-1}\\) only exists for a square matrix \\(\\mx{A}\\) if \\(|\\mx{A}| \\neq 0\\). For square matrices greater than 2, the determinant is obtained recursively from the general expression \\[\\begin{equation} |\\mx{A}| = \\sum^n_{j=1} {A_ij}(-1)^{i+j}|\\mx{A}_{ij}| \\tag{8.15} \\end{equation}\\] where \\(i\\) is any fixed row of the matrix and \\(\\mx{A}\\) and \\(\\mx{A}_{ij}\\) is a submatrix obtained by deleting the \\(i\\)th row and \\(j\\)th column from \\(\\mx{A}\\). Such a submatrix is known as a minor. Essentially you should be left with some \\(2 \\times 2\\) matrices at the end of all this. If the matrix is a diagonal, then the determinant is simply the product of the diagonal elements of that matrix, i.e. if \\[\\begin{equation} A_{ij} = \\begin{cases} a_i, &amp; i = j\\\\ 0, &amp; i \\neq j \\end{cases} \\ \\ \\ \\mathrm{then} \\ \\ \\ |\\mx{A}| = \\prod^n_{i=1}a_{i} \\notag \\end{equation}\\] 8.2.8 Computing inverses The general solution of a matrix inverse is \\[\\begin{equation} A^{-1}_{ij} = \\left[\\frac{(-1)^{i+j}|\\mx{A}_{ij}|} {|\\mx{A}|} \\right]^T \\tag{8.16} \\end{equation}\\] where \\(A^{-1}_{ij}\\) denotes the \\(ij\\)th element of \\(\\mx{A}^{-1}\\) and \\(\\mx{A}_{ij}\\) denotes the \\(ij\\)th minor of \\(\\mx{A}\\). From equation (8.16), division by the determinant is required for inversion, therefore the determinant of a matrix must be nonzero if that matrix can be inverted. Thus, a matrix is singular if its determinant is zero. This occurs whenever a matrix contains a row (or column) that can be written as a weighted sum of any other rows (or columns). In the context of our linear model, (8.2), this happens if one of the \\(n\\) equations can be written as a combination of the others, a situation that is equivalent to there being \\(n\\) unknowns but less than \\(n\\) independent equations. 8.3 Expectations of random vectors and matrices Matrix algebra provides a powerful approach for analysing linear combinations of random variables. Let \\(\\mx{x}\\) be a column vector containing \\(n\\) random variables, \\(\\mx{x} = (x_1, x_2, \\dots, x_n)^T\\). We may want to construct a new univariate (scalar) random variable \\(y\\) by taking some linear combination of the elements of \\(x\\), \\[\\begin{equation} y = \\sum^n_{i=1} {a_ix_i} = \\mx{a}^T \\mx{x} \\notag \\end{equation}\\] where \\(\\mx{a} = (a_1, a_2, \\dots, a_n)^T\\) is a column vector of constants. Good example of this is when we want to make a weighted genetic score. Likewise, we can construct a new \\(k\\)-dimensional vector \\(\\mx{y}\\) by premultiplying \\(\\mx{x}\\) by a \\(k \\times n\\) matrix \\(\\mx{A}\\) of constants, \\(\\mx{y} = \\mx{A}\\mx{x}\\). More generally, an (\\(n \\times k\\)) matrix \\(\\mx{X}\\) of random variables can be transformed into a new \\(m \\times l\\) dimensional matrix \\(\\mx{Y}\\) of elements consisting of linear combinations of the elements of \\(\\mx{X}\\) by \\[\\begin{equation} \\mx{Y}_{m \\times l} = \\mx{A}_{m \\times n} \\mx{X}_{n \\times k} \\mx{B}_{k \\times l} \\tag{8.17} \\end{equation}\\] where the matrices \\(\\mx{A}\\) and \\(\\mx{B}\\) are constants with dimensions as subscripted. If \\(\\mx{X}\\) is full of random variables, the expected value of \\(\\mx{X}\\) is \\(E(\\mx{X})\\) containing the expected value of each element of \\(\\mx{X}\\). If \\(\\mx{X}\\) and \\(\\mx{Z}\\) are matrices of the same dimension, then \\[\\begin{equation} E(\\mx{X} + \\mx{Z}) = E(\\mx{X}) + E(\\mx{Z}) \\tag{8.18} \\end{equation}\\] Similarly, just remember other rules of expectations to obtain an expression for the expectation of \\(\\mx{Y}\\) from (8.17). The important point is of course that the dimensions of matrices need to match up! 8.4 Covariance matrices of transformed vectors If we have an \\(n \\times n\\) square matrix \\(\\mx{A}\\) and an \\(n \\times 1\\) column vector \\(\\mx{x}\\), then: \\[\\begin{equation} \\mx{x}^T\\mx{Ax} = \\sum^n_{i=1} \\sum^n_{j=1} {a_{ij}}x_ix_j \\tag{8.19} \\end{equation}\\] This expression is called a quadratic form (or quadratic product) and yield a scalar. A generalisation of a quadratic form is the bilinear form, \\(\\mx{b^TAa}\\), where \\(\\mx{b}\\) and \\(\\mx{a}\\) are, respectively \\(n \\times 1\\) and \\(m \\times 1\\) column vectors and \\(\\mx{A}\\) is an \\(n \\times m\\) matrix. Index the matrices and you can tell what the resulting product is! Hint: it’s a scaler, figure out why! As scalars, bilinear forms equal their transposes, giving the useful identity: \\[\\begin{equation} \\mx{b^TAa} = (\\mx{b^TAa})^T = \\mx{a^TA^Tb} \\tag{8.20} \\end{equation}\\] If \\(\\mx{x}\\) is a vector of \\(n\\) random variables, then you can express the \\(n\\) variances and \\(n(n-1)/2\\) covariances associated with the elements of \\(x\\) as the matrix \\(\\mx{V}\\), where \\(V_{ij} = \\sigma(x_i, x_j)\\) is the covariance between the random variables \\(x_i\\) and \\(x_j\\). This is a covariance matrix (or variance-covariance matrix)! Remember: diagonals = variances and off-diagonals = covariances! The \\(\\mx{V}\\) matrix is symmetric such that: \\[\\begin{equation} V_{ij} = \\sigma(x_i, x_j) = \\sigma(x_j, x_i) = V_{ji} \\notag \\end{equation}\\] If we have a univariate random variable \\(y = \\sum{c_{k}x_{k}}\\) generated from a linear combination of the elements of \\(\\mx{x}\\), in matrix notation we have \\(y = \\mx{c}^T\\mx{x}\\), where \\(\\mx{c}\\) is a column vector of constants. The variance of \\(y\\) can be expressed as a quadratic form involving the covariance matrix \\(\\mx{V}\\) for the elements of \\(\\mx{x}\\), \\[\\begin{equation} \\mx{c}^T\\mx{V}\\mx{c} \\tag{8.21} \\end{equation}\\] (See book for derivation, (8.21) is the final result) Likewise, the covariance between two univariate random variables created from different linear combinations of \\(\\mx{x}\\) is given by the bilinear form \\[\\begin{equation} \\sigma(\\mx{a^Tx}, \\mx{b^Tx}) = \\mx{a^TVb} \\tag{8.22} \\end{equation}\\] If we transform \\(\\mx{x}\\) to two new vectors \\(\\mx{y}_{x \\times 1} = \\mx{A}_{l \\times n}\\mx{x}_{n \\times 1}\\) and \\(\\mx{z}_{m \\times 1} = \\mx{B}_{m \\times n}\\mx{x}_{n \\times 1}\\), then instead of a single covariance we have an \\(l \\times m\\) dimensional covariance matrix, denoted \\(\\mx{\\sigma(y, z)}\\). Letting \\(\\mx{\\mu_{y}} = \\mx{A_\\mu}\\) and \\(\\mx{\\mu_{z}} = \\mx{B_\\mu}\\), with \\(E(\\mx{x}) = \\mu\\), then \\(\\mx{\\sigma(y, z)}\\) can be expressed in terms of \\(\\mx{V}\\), the covariance matrix of \\(\\mx{x}\\), \\[\\begin{equation} \\begin{split} \\mx{\\sigma(y, z)} &amp;= \\mx{\\sigma(Ax, Bx)} \\notag \\\\ &amp;= E\\left[(\\mx{y} - \\mx{\\mu_{y}})(\\mx{z} - \\mx{\\mu_{z}})^T \\right] \\notag \\\\ &amp;= E\\left[\\mx{A}(\\mx{x} - \\mx{\\mu})(\\mx{x} - \\mx{\\mu})^T \\mx{B}^T \\right] \\notag \\\\ &amp;= \\mx{AVB}^T \\end{split} \\tag{8.23} \\end{equation}\\] In particular, the covariance matrix for \\(\\mx{y} = \\mx{Ax}\\) is \\[\\begin{equation} \\mx{\\sigma(y, y)} = \\mx{AVA}^T \\tag{8.24} \\end{equation}\\] so that the covariance between \\(y_i\\) and \\(y_\\) is given by the \\(ij\\)th element of the matrix product \\(\\mx{AVA}^T\\). Finally, note that if \\(\\mx{x}\\) is a vector of random variables with expected value \\(\\mu\\), then the expected value of the scalar quadratic product \\(\\mx{x}^T\\mx{Ax}\\) is \\[\\begin{equation} \\mx{x}^T\\mx{Ax} = tr(\\mx{AV}) + \\mx{\\mu}^T\\mx{A\\mu} \\tag{8.25} \\end{equation}\\] where \\(\\mx{V}\\) is the covariance matrix for the elements of \\(\\mx{x}\\), and the trace of a square matrix, \\(tr(\\mx{M}) = \\sum{M_{ii}}\\) is the sum of its diagonal values. 8.5 The multivariate normal distribution Multivariate normal distribution = MVN. Consider the probability density function for \\(n\\) independent normal random variables, where \\(x_i\\) is normally distributed with mean \\(\\mu_i\\) and variance \\(\\sigma^2_i\\). In this case, because the variables are independent, the joint probability density function is simply the product of each univariate density, \\[\\begin{equation} TO-DO \\tag{8.26} \\end{equation}\\] Let’s express this in matrix form, because we’re cool! (also it compacts things a lot). To do this, we need to defin these matrices \\[\\begin{equation} \\mx{V} = \\begin{pmatrix} \\sigma^2_1 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; \\sigma^2_2 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma^2_n \\end{pmatrix} \\ \\ \\ \\mathrm{and}\\ \\ \\ \\mx{\\mu} = \\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_n \\end{pmatrix} \\notag \\end{equation}\\] Since \\(\\mx{V}\\) is diagonal, its determinant is simply the product of the diagonal elements \\[\\begin{equation} |\\mx{V}| = \\prod^n_{i=1}\\sigma^2_i \\notag \\end{equation}\\] Likewise, using quadratic products, note that \\[\\begin{equation} \\sum^n_{i=1} \\frac{(x_i - \\mu_i)^2} {\\sigma^2_i} = (\\mx{x} - \\mx{\\mu}^T)\\mx{V}^{-1} (\\mx{x} - \\mx{\\mu}) \\notag \\end{equation}\\] putting these together, equation (8.26) can be written as \\[\\begin{equation} TO-DO \\tag{8.27} \\end{equation}\\] This can also be written as \\(p(\\mx{x}, \\mx{\\mu}, \\mx{V}\\) to stress that it is a function of the mean vector \\(\\mx{\\mu}\\) and the covariance matrix \\(\\mx{V}\\). More generally, when the elements of \\(\\mx{x}\\) are correlated, equation (8.27) gives the probability density function for a vector of multivariate normally distributed random variables, with mean vector \\(\\mx{\\mu}\\) and covariance matrix \\(\\mx{V}\\). We denote this by \\[\\begin{equation} \\mx{x} \\sim \\mathrm{MVN_n}(\\mx{\\mu, V}) \\notag \\end{equation}\\] where the subscript indicating the dimensionality of \\(\\mx{x}\\) is usually omitted. MVN also called Gaussian distribution. 8.5.1 Properties of the MVN Like the univariate normal distribution, the MVN is expected to arise naturally when the trait of interest result from a large number of underlying variables, so fits many human traits! Prepare your butts for some useful properties of MVN, because here they come!!! If \\(\\mx{x} \\sim \\mathrm{MVN}\\), then the distribution of any subset of the variables in \\(\\mx{x}\\) is also MVN. For example, each \\(x_i\\) is normally distributed and each pair \\((x_i, x_j)\\) is bivariate normally distributed. If \\(\\mx{x} \\sim \\mathrm{MVN}\\), then any linear combination of the elements of \\(\\mx{x}\\) is also MVN. Specifically, if \\(\\mx{x} \\sim \\mathrm{MVN_n}(\\mx{\\mu}, \\mx{V})\\), \\(\\mx{a}\\) is a vector of constants, and \\(\\mx{A}\\) is a matrix of constants, then \\[\\begin{equation} \\mathrm{for}\\ \\ \\ \\mx{y} = \\mx{x} + \\mx{a}, \\ \\ \\ \\ \\ \\mx{y} \\sim \\mathrm{MVN_n}(\\mx{\\mu} + \\mx{a}, \\mx{V}) \\\\ \\mathrm{for}\\ \\ \\ \\mx{y} = \\mx{a}^T \\mx{x} = \\sum^n_{k=1} {a_ix_i}, \\ \\ \\ \\ \\ y \\sim \\mathrm{N}(\\mx{a}^{T}\\mx{\\mu}, \\mx{a}^{T}\\mx{V}\\mx{a}) \\\\ \\mathrm{for}\\ \\ \\ \\mx{y} = \\mx{A}\\mx{x}, \\ \\ \\ \\ \\ \\mx{y} \\sim \\mathrm{MVN_m}(\\mx{A\\mu}, \\mx{A^{T}VA}) \\end{equation}\\] Conditional distributions associated with MVN are also multivariate normal. ADD IN BOOK PART HERE!! If \\(\\mx{x} \\sim \\mathrm{MVN}\\), the regression of any subset of \\(\\mx{x}\\) on another subset is linear and homoscedastic. ADD IN BOOK PART HERE!! 8.6 Overview of linear models Consider a multivariate linear model where all the covariates are binary. For example, consider the half-sib design wherein each of \\(p\\) unrelated sires is mated at random to a number of unrelated dams and a single offspring is measured from each cross. The simplest model for this design is \\[\\begin{equation} y_{ij} = \\mu + s_i + e_{ij} \\notag \\end{equation}\\] where \\(y_{ij}\\) is the phenotype of the \\(j\\)th offspring from sire \\(i\\), \\(\\mu\\) is the population mean, \\(s_i\\) is the sire effect, and \\(e_{ij}\\) is the residual error. DON’T UNDERSTAND THIS SENTENCE Although this is clearly a linear model, it differs significantly from the regression model described above in that while there are parameters to estimate (the sire effects, \\(s_i\\)), the only measured values are the \\(y_{ij}\\). Nevertheless, we can express this model in a form that is essentially identical to the standard regression model by using \\(p\\) binary variables to classify the sires of the offspring. The resulting linear model becomes \\[\\begin{equation} y_{ij} = \\mu + \\sum^p_{k=1}s_{k}x_{ik} + e_{ij} \\notag \\end{equation}\\] where \\[\\begin{equation} x_{ik} = \\begin{cases} 0, &amp; \\text{ if k = i } \\\\ 1, &amp; \\text{ otherwise } \\end{cases} \\end{equation}\\] By using binary variables, an wide class of problems can be handled by linear models. Models containing only binary variables are usually called ANOVA (analysis of variance) models! Whether predictor variables are a continuous (as in regression) or not (as in ANOVA), the procedures are special cases of the general linear model (GLM), wherein each observation (\\(\\mx{y}\\)) is assumed to be a linear function of \\(p\\) observed and / or binary variables plus a residual error (\\(e\\)), \\[\\begin{equation} y_i = \\sum^p_{k=1} \\beta_{k}x_{ik} + e_{i} \\tag{8.28} \\end{equation}\\] where \\(x_{i1}, \\dots, x_{ip}\\) are the values of the \\(p\\) predictor variables for the \\(i\\)th individual. For a vector of \\(n\\) observations, the GLM can be written in matrix form as \\[\\begin{equation} \\mx{y} = \\mx{X\\beta} + \\mx{e} \\tag{8.29} \\end{equation}\\] where the design or incidence matrix \\(\\mx{X}\\) is \\(n \\times p\\), and \\(\\mx{e}\\) is the vector of residual errors. \\(\\mx{\\beta}\\) is to be estimated obvs. 8.6.1 Ordinary least squares Estimates of \\(\\mx{\\beta}\\) are usually obtained using least-squares that make assumptions about the residuals. Ordinary least squares assumes that the residual errors are homoscedastic and uncorrelated, i.e., \\(\\sigma^2(e_i) = \\sigma^2_e\\) for all \\(i\\) and \\(\\sigma(e_i, e_j) = 0\\) for \\(i \\neq j\\). SOME MATHS HERE \\[\\begin{equation} \\mx{b} = (\\mx{X}^T\\mx{X})^{-1}\\mx{X}^T\\mx{y} \\tag{8.30} \\end{equation}\\] where \\(\\mx{b}\\) is an estimate of \\(\\mx{\\beta}\\). Under the assumption the residual errors are uncorrelated and homoscedastic, the covariance matrix of the elements of \\(\\mx{b}\\) is \\[\\begin{equation} \\mx{V_b} = (\\mx{X}^T\\mx{X})^{-1}\\sigma^{2}_{e} \\tag{8.31} \\end{equation}\\] If the residuals follow a multivariate distribution with \\(\\mx{e} \\sim \\mathrm{MVN}(0, \\sigma^{2}_{e} \\dot \\mx{I}\\), the OLS estimate is also the maximum-likelihood estimate. 8.7 Generalised least squares "],
["questions.html", "Questions Chapter 4 Chapter 5", " Questions Have fun answering these Gib! Chapter 4 What the fuck are they talking about with the molecular basis of dominance? - page 63-64 Chapter 5 How do they calculate the variance of a phenotype explained by just the dominance effects? - page 91 "]
]

[
["introduction-to-matrix-algebra-and-linear-models.html", "Chapter 8 Introduction to Matrix Algebra and Linear Models 8.1 Multiple regression 8.2 Elementary matrix algebra", " Chapter 8 Introduction to Matrix Algebra and Linear Models \\[\\newcommand{\\mx}[1]{\\mathbf{#1}}\\] 8.1 Multiple regression Simple multiple regression equation: \\[\\begin{equation} y = \\alpha + \\beta_1z_1 + \\beta_2z_2 + \\dots + \\beta_nz_n + e \\tag{8.1} \\end{equation}\\] \\(y\\) = dependent/response variable, \\(z_1, z_2, z_n\\) = predictors, \\(e\\) = residual error, \\(\\alpha\\) is a constant as are \\(\\beta_1, \\beta_2, \\beta_n\\) to be estimated. Recall from Chapter 3 that the goal of least-squares regression is to find a set of constants (\\(\\alpha\\) and the \\(\\beta\\)s) that minimise the squared differences between observed and expected values, with expected values is anything that fits on the “line of best fit”. Also, recall equation (??), to see the relationship between \\(y\\), \\(z_n\\) (\\(x\\) in the equation) and \\(b\\). For multiple regression there are many “\\(b\\)” terms and each of them can be estimated by dividing the covariance of the dependent variable and the predictor (\\(\\sigma(y, z_n)\\)) by the covariance of the predictor with all other predictors in the model. When \\(n = 1\\), the model reduces to a simple linear regression and we return to equation (??). This can be represented in matrix form like so: \\[\\begin{equation} \\begin{pmatrix} \\sigma^2(z_1) &amp; \\sigma(z_1, z_n) &amp; \\dots &amp; \\sigma(z_1, z_n) \\\\ \\sigma(z_1, z_1) &amp; \\sigma^2(z_2) &amp; \\dots &amp; \\sigma(z_2, z_n) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma(z_1, z_n) &amp; \\sigma(z_2, z_n) &amp; \\dots &amp; \\sigma^2(z_n) \\end{pmatrix} \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_n \\end{pmatrix} = \\begin{pmatrix} \\sigma(y, z_1) \\\\ \\sigma(y, z_2) \\\\ \\vdots \\\\ \\sigma(y, z_n) \\end{pmatrix} \\tag{8.2} \\end{equation}\\] When estimating each response variable-predictor covariance term, it is the sum of predictor covariance multiplied by beta. If the covariance matrix and the vectors of (8.2) are written as \\(\\mx{V}\\), \\(\\mx{\\beta}\\) and \\(\\mx{c}\\) respectively, then the equation can be re-written as: \\[\\begin{equation} \\mx{V\\beta} = \\mx{c} \\tag{8.3} \\end{equation}\\] NOTE: It is standard procedure to denote matrices as bold capital letters and vectors as bold lower case letters. Before going onto matrix methods in more detail, here is an application of (8.1) in quantitative genetics. 8.1.1 An application to multivariate selection Suppose that a large number of individuals in a population have been measured for \\(n\\) characters and for fitness. Individual fitness can then be approximated by the linear model \\[\\begin{equation} w = \\alpha + \\beta_1z_1 + \\beta_2z_2 + ... + \\beta_nz_n + e \\tag{8.4} \\end{equation}\\] where \\(w\\) is the relative fitness (observed fitness divided by the mean fitness in the population). In Chapter 3, we learnt that the selection differential for the \\(i\\)th trait is defined as the covariance between phenotype and relative fitness, \\(S_i = \\sigma(z_i, w)\\). Therefore, if we use multiple regression to estimate \\(S_i\\) we’d end up with: \\[\\begin{equation} S_i = \\beta_i\\sigma^2(z_i) + \\sum^n_{j \\neq i} {\\beta_j\\sigma(z_i, z_j)} \\tag{8.5} \\end{equation}\\] Simple! 8.2 Elementary matrix algebra 8.2.1 Basic notation Vectors and matrices in mathematics are just like those in R. A matrix with the same number of rows and columns is called a square matrix. Vectors written vertically are called column vectors, e.g. \\[\\begin{equation} \\mx{a} = \\begin{pmatrix} 12 \\\\ 13 \\\\ 47 \\end{pmatrix} \\notag \\end{equation}\\] and those that are written horizontally are called row vectors, e.g. \\[\\begin{equation} \\mx{b} = \\begin{pmatrix} 12 &amp; 13 &amp; 47 \\end{pmatrix} \\notag \\end{equation}\\] Single numbers by themselves are often referred to as scalars. A matrix can be described by the elements that comprise it, with \\(M_ij\\) denoting the element in the \\(i\\)th row and \\(j\\)th column of matrix \\(\\mx{M}\\). 8.2.2 Partitioned matrices It is often useful to work with partitioned matrices wherein each element in a matrix is itself a matrix. There are several ways to partition a matrix, for example: \\[\\begin{equation} \\mx{c} = \\begin{pmatrix} 3 &amp; 1 &amp; 2 \\\\ 2 &amp; 5 &amp; 4 \\\\ 1 &amp; 1 &amp; 2 \\end{pmatrix} = \\begin{pmatrix} 3 &amp; \\vdots &amp; 1 &amp; 2 \\\\ \\dots &amp; \\dots &amp; \\dots &amp; \\dots \\\\ 2 &amp; \\vdots &amp; 5 &amp; 4 \\\\ 1 &amp; \\vdots &amp; 1 &amp; 2 \\end{pmatrix} = \\begin{pmatrix} \\mx{a} &amp; \\mx{b} \\\\ \\mx{d} &amp; \\mx{B} \\end{pmatrix} \\notag \\end{equation}\\] where \\[\\begin{equation} \\mx{a} = \\begin{pmatrix} 3 \\end{pmatrix} , \\mx{b} = \\begin{pmatrix} 1 &amp; 2 \\end{pmatrix} , \\mx{d} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} , \\mx{B} \\begin{pmatrix} 5 &amp; 4 \\\\ 1 &amp; 2 \\end{pmatrix} \\notag \\end{equation}\\] There are also other simple ways to partition matrices. 8.2.3 Addition and subtraction To add matrices, they must have the same dimensions. Just add the corresponding elelments, so for adding matrices \\(\\mx{A}\\), and \\(\\mx{B}\\) to make \\(\\mx{C}\\), it is simply \\(C_ij = A_ij + B_ij\\). Subtraction is defined similarly. Can add examples here if you really want to 8.2.4 Multiplication To multiply matrix \\(\\mx{M}\\) by scalar \\(a\\), then just multiply each element of \\(\\mx{M}\\) by \\(a\\). Dot product of two vectors, \\(\\mx{a} \\cdot \\mx{b}\\), is a scalar given by \\[\\begin{equation} \\mx{a} \\cdot \\mx{b} = \\sum^n_{i=1} {a_ib_i} \\notag \\end{equation}\\] For example, for the two vectors given by \\[\\begin{equation} \\mx{a} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{pmatrix} \\notag \\end{equation}\\] and \\[\\begin{equation} \\mx{b} = \\begin{pmatrix} 4 &amp; 5 &amp; 7 &amp; 9 \\end{pmatrix} \\notag \\end{equation}\\] the dot product \\(\\mx{a} \\cdot \\mx{b}\\) = (1 x 4) + (2 x 5) + (3 x 7) + (4 x 9) = 71. Dot product is not defined if the vectors have different lengths. Now consider the matrix \\(\\mx{L} = \\mx{M}\\mx{N}\\) produced by multiplying the \\(r\\) x \\(c\\) matrix \\(\\mx{M}\\) by the \\(c\\) x \\(b\\) matrix \\(\\mx{N}\\). Partioning \\(\\mx{M}\\) as a column vector of \\(r\\) row vectors, \\[\\begin{equation} \\mx{M} = \\begin{pmatrix} \\mx{m_1} \\\\ \\mx{m_2} \\\\ \\vdots \\\\ \\mx{m_r} \\end{pmatrix} where \\mx{m_i} = \\begin{pmatrix} M_{i1} &amp; M_{i2} &amp; \\dots &amp; M_{ic} \\end{pmatrix} \\notag \\end{equation}\\] and \\(\\mx{N}\\) as a row vector of \\(b\\) column vectors, \\[\\begin{equation} \\mx{N} = \\begin{pmatrix} \\mx{n_1} &amp; \\mx{n_2} &amp; \\dots &amp; \\mx{n_b} \\end{pmatrix} where \\mx{n_j} = \\begin{pmatrix} N_{1j} \\\\ N_{2j} \\\\ \\vdots \\\\ N_{cj} \\\\ \\end{pmatrix} \\notag \\end{equation}\\] the \\(ij\\)th element of \\(\\mx{L}\\) is given by the dot product \\[\\begin{equation} L_{ij} = \\mx{m_i} \\cdot \\mx{n_j} = \\sum^c_{k=1} {M_{ik}N_{kj}} \\tag{8.6} \\end{equation}\\] Use this to write out matrix \\(\\mx{L}\\) – cba right now… To be definied, the number of columns in the first matrix must equal the number of rows in the second matrix. This means that unless the two matrices are square, it is only possible to multiply them together one way round and not the other (e.g. \\(\\mx{M}\\mx{N}\\) may be defined, but \\(\\mx{N}\\mx{M}\\) won’t). Writing \\(\\mx{M}_{r \\times c}\\mx{N}_{c \\times b} = \\mx{L}_{r \\times b}\\) shows the inner indices must match, while the outer indices give the number of rows and columns of the resulting matrix. Even if the matrices being multiplied are square, the order in which they are multiplied is important, i.e. multiplying \\(\\mx{A}\\) by \\(\\mx{B}\\) is not the same as vice versa. So there is terminology to help differentiate what is being multiplied by what. In the example just given, one would say matrix \\(\\mx{B}\\) is premultiplied by matrix \\(\\mx{A}\\), or that matrix \\(\\mx{A}\\) is postmultiplied by matrix \\(\\mx{B}\\). 8.2.5 Transposition The transpose of a matrix \\(\\mx{A}\\) is written as \\(\\mx{A}^T\\) or \\(\\mx{A}&#39;\\). It is obtained by simply switching the rows and columns of the matrix. Add example here if you want A useful identity is \\[\\begin{equation} (\\mx{A}\\mx{B}\\mx{C})^T = \\mx{C}^T\\mx{B}^T\\mx{A}^T \\tag{8.7} \\end{equation}\\] which holds for any number of matrices. Vectors tend to be written as column vectors, and therefore will be written as such henceforth, and as lowercase bold letters, e.g. \\(\\mx{a}\\), for a column vector and \\(\\mx{a}^T\\) for the corresponding row vector. Further, when multiplying vectors we can assess the inner product (dot product), which yields a scalar and the outer product, which yields a matrix. For the two \\(n\\)-dimensional column vectors \\(\\mx{a}\\) and \\(\\mx{b}\\), \\[\\begin{equation} \\mx{a} = \\begin{pmatrix} a_1 \\\\ \\vdots \\\\ a_n \\end{pmatrix} \\mx{b} = \\begin{pmatrix} b_1 \\\\ \\vdots \\\\ b_n \\end{pmatrix} \\notag \\end{equation}\\] the inner product is given by \\[\\begin{equation} \\begin{pmatrix} a_1 &amp; \\dots &amp; a_n \\end{pmatrix} \\begin{pmatrix} b_1 \\\\ \\vdots \\\\ b_n \\end{pmatrix} = \\mx{a}^T\\mx{b} = \\sum^n_{i=1} {a_ib_i} \\tag{8.8} \\end{equation}\\] while the outer product yields an \\(n \\times n\\) matrix \\[\\begin{equation} \\begin{pmatrix} a_1 \\\\ \\vdots \\\\ a_n \\end{pmatrix} \\begin{pmatrix} b_1 &amp; \\dots &amp; b_n \\end{pmatrix} = \\mx{a}\\mx{b}^T = \\begin{pmatrix} a_1b_1 &amp; a_1b_2 &amp; \\dots &amp; a_1b_n \\\\ a_2b_1 &amp; a_2b_2 &amp; \\dots &amp; a_2b_n \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_nb_1 &amp; a_nb_2 &amp; \\dots &amp; a_nb_n \\end{pmatrix} \\tag{8.9} \\end{equation}\\] 8.2.6 Inverses and solutions to systems of equations Inverting a matrix is an operation similar to division of scalars. Multiplying an matrix by its inverse gives the identity matrix, \\(\\mx{I}\\), which is a matrix where the diagonals are 1 and all other elements are 0. This plays the same role as the number 1 in scalar multiplication and division. The notation for the inverse of matrix \\(\\mx{A}\\) would be \\(\\mx{A}^-1\\) and \\(\\mx{A}\\mx{A}^-1 = \\mx{I}\\). This is useful for solving equations containing matrices, e.g. if you want to divide both sides of the equation by a matrix. A matrix is called nonsingular if its inverse exists. A useful property of inverses is that if the matrix product \\(\\mx{AB}\\) "]
]

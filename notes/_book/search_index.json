[
["index.html", "Notes for: Walsh and Lynch. Genetics and Analysis of Quantitative Traits Preface", " Notes for: Walsh and Lynch. Genetics and Analysis of Quantitative Traits Thomas Battram 2020-05-14 Preface This is a good book, but if I make it through the whole thing I deserve several medals and some cake. "],
["an-overview-of-quantitative-genetics.html", "Chapter 1 An overview of quantitative genetics", " Chapter 1 An overview of quantitative genetics BORWANG!!! This chapter just introduces the book and some simple concepts. "],
["properties-of-distributions.html", "Chapter 2 Properties of distributions", " Chapter 2 Properties of distributions ALSO BORWANG! You can guess what this chapter was on and also how much of a hoot it was… "],
["covariance-regression-and-correlation.html", "Chapter 3 Covariance, regression, and correlation 3.1 Covariance 3.2 Least squares linear regression 3.3 Correlation 3.4 Differential selection (brief intro) 3.5 Correlation between genotype and phenotype (brief intro) 3.6 End of chapter questions", " Chapter 3 Covariance, regression, and correlation 3.1 Covariance Covariance is a measure of association and the covariance between x and y would be denoted by \\(\\sigma(x, y)\\). If \\(x\\) and \\(y\\) are independent then \\(\\sigma(x, y) = 0\\), BUT if \\(\\sigma(x, y) = 0\\), \\(x\\) and \\(y\\) aren’t necessarily independent. 3.1.1 Useful identities for covariance Covariance of \\(x\\) with itself = variance of \\(x\\): \\[\\begin{equation} \\sigma(x, x) = \\sigma^2(x) \\tag{3.1} \\end{equation}\\] For constants (here represented by \\(a\\)) see (3.2) below \\[\\begin{align} \\sigma(a, x) &amp;= 0 \\notag \\\\ \\sigma(ax, y) &amp;= a\\sigma(x, y) \\notag \\\\ \\sigma^2(a, x) &amp;= a^2\\sigma^2(x) \\notag \\\\ \\sigma[(a + x), y] &amp;= \\sigma(x, y) \\tag{3.2} \\end{align}\\] The covariance of 2 sums can be written as the sum of covariances, i.e. just multiply out the brackets: \\[\\begin{equation} \\sigma[(x + y),(w + z)] = \\sigma(x, w) + \\sigma(x, z) + \\sigma(y, w) + \\sigma(y, z) \\tag{3.3} \\end{equation}\\] Variance of a sum is sum of variances and covariances: \\[\\begin{equation} \\sigma^2(x + y) = \\sigma^2(x) + 2\\sigma(x, y) + \\sigma^2(y) \\tag{3.4} \\end{equation}\\] 3.2 Least squares linear regression Linear model: \\[\\begin{equation} y = \\alpha + \\beta{x} + e \\tag{3.5} \\end{equation}\\] Continuing on, \\(\\alpha\\) and \\(\\beta\\) will be the true population values and \\(a\\) and \\(b\\) will be the intercept and slope for the line of best fit derived from observed data. The derivation of \\(a\\) and \\(b\\) using the least-squares model can be found on pages 39-41. Buuut, who cares about that, here are the results: \\[\\begin{align} a &amp;= \\bar{y} - b\\bar{x} \\notag \\\\ b &amp;= \\frac{Cov(x, y)} {Var(x)} \\tag{3.6} \\end{align}\\] 3.2.1 Properties of least squares 6 in the book, just writing down important/not obvious ones. The mean residual (\\(\\bar{e}\\)) is 0 Residual errors are uncorrelated with predictor variable \\(x\\) (see book for why) BUT \\(e\\) and \\(x\\) may not be independent if the relationship between \\(x\\) and \\(y\\) is non-linear. If it is truly non-linear \\(E(e|x) != 0\\) Variance of \\(e\\) can vary with \\(x\\), in this situation the the regression is said to display heteroscedasticity (see Figure 3.4 for great illustration) The regression of \\(y\\) on \\(x\\) is different to the regression of \\(x\\) on \\(y\\)! 3.3 Correlation Correlation coefficient between \\(x\\) and \\(y\\): \\[\\begin{equation} r(x, y) = \\frac{Cov(x, y)} {\\sqrt{Var(x) Var(y)}} \\tag{3.7} \\end{equation}\\] The correlation coefficient is a dimensionless measure of association and it is symmetrical (i.e. \\(r(x, y) = r(y, x)\\)). Scaling \\(x\\) or \\(y\\) by constants does not change the correlation coefficient, but it does affect variances and covariances. The correlation coefficient is a standardised regression coefficient -&gt; the regression coefficient resulting from rescaling \\(x\\) and \\(y\\) such that each has unit variance). \\(r^2\\) assumes \\(E(y|x)\\) is linear! 3.4 Differential selection (brief intro) The directional selection differential, \\(S\\), is the difference between the mean phenotype within that generation before selection (\\(\\mu_s\\)) and the mean phenotype within that generation after (\\(\\mu\\)) selection. \\[\\begin{equation} S = \\mu_s - \\mu \\tag{3.8} \\end{equation}\\] If all individuals have equal fertility and viability then selecting individuals won’t change anything so \\(\\mu_s = \\mu\\) and \\(S = 0\\). If \\(W(z)\\) is the probability that individuals with phenotype \\(z\\) survive to reproduce and \\(p(z)\\) is the density of \\(z\\) (pretty much means distribution) before selection, then the density after selection is: \\[\\begin{equation} p_{s}(z) = \\frac{W(z)p(z)} {\\int W(z)p(z)dz} \\tag{3.9} \\end{equation}\\] The denominator here is the mean individual fitness (\\(\\bar{W}\\)). The relative fitness of \\(z\\) is \\(w(z) = \\frac{W(z)} {\\bar{W}}\\). After some sweet derivation (see page 46), you finish with: \\[\\begin{equation} S = \\sigma[z, w(z)] \\tag{3.10} \\end{equation}\\] Therefore the directional selection is equivalent to the covariance of the phenotype and the relative fitness. If you regress offspring phenotype on the midparent phenotype and that relationship is linear with slope \\(\\beta\\), a change in mean midparent phenotype induces an expected change in mean phenotype across generations equal to: \\[\\begin{equation} \\begin{split} \\Delta\\mu &amp;= \\mu_0 - \\mu \\\\ &amp;= \\beta(\\mu_s - \\mu) \\\\ &amp;= \\beta{S} \\end{split} \\tag{3.11} \\end{equation}\\] This is the breeders’ equation! 3.5 Correlation between genotype and phenotype (brief intro) Only when there is no gene-environment interaction is the variance explained by genetics (broad-sense heritability) the equation below: \\[\\begin{equation} H^2 = \\frac{\\sigma^2_G} {\\sigma^2_z}, \\tag{3.12} \\end{equation}\\] where \\(z\\) is the phenotype and \\(G\\) is the sum of the total effects (not just additive) at all loci on the trait. The slope of a midparent-offspring regression provides an estimate of the proportion of the phenotypic variance that is attributable to additive genetic factors (the narrow-sense heritability). \\[\\begin{equation} h^2 = \\frac{\\sigma^2_A} {\\sigma^2_z} \\tag{3.13} \\end{equation}\\] So as \\(h^2\\) is just the regression of offspring phenotype on midparent phenotype it can actually be used in the breeders’ equation! \\[\\begin{equation} \\Delta\\mu = h^2S \\tag{3.14} \\end{equation}\\] So the narrow-sense heritability can be thought of as the efficiency of the response to selection. If \\(h^2 = 0\\) there can be no evolutionary change regardless of strength of selection. Although this should be obvious because if \\(h^2\\) is 0 then there is clearly no passing of genetic material onto the next generation that is influencing that trait. 3.6 End of chapter questions True of false, if \\(\\sigma(x,y) = 0\\), \\(x\\) and \\(y\\) are independent Finish these equations: \\[\\begin{align} \\sigma[(x + y),(w + z)] &amp;= ... \\notag \\\\ \\sigma^2(a, x) &amp;= ... \\notag \\\\ \\sigma[(a + x), y] &amp;= ... \\notag \\\\ \\sigma^2(x + y) &amp;= ... \\notag \\\\ \\end{align}\\] Name 4 important properties of least-squares regression Give an assumption of the correlation coefficient How can you work out \\(h^2\\) from trio data? Give two definitions of the breeders’ equation. "],
["properties-of-single-loci.html", "Chapter 4 Properties of single loci 4.1 Introduction 4.2 Allele and genotype frequencies 4.3 The transmission of genetic information 4.4 Characterising the influence of a locus on the phenotype 4.5 The basis of dominance 4.6 Fisher’s decomposition of the genotypic value 4.7 Partioning the genetic variance. 4.8 Additive effects, average excesses and breeding values 4.9 Extensions for multiple alleles and non random mating 4.10 End of chapter questions", " Chapter 4 Properties of single loci 4.1 Introduction too easy 4.2 Allele and genotype frequencies too easy 4.3 The transmission of genetic information 4.3.1 The Hardy-Weinberg principle \\[\\begin{equation} p^2 + 2pq + q^2 = 1 \\tag{4.1} \\end{equation}\\] where \\(p\\) = allele frequency of first allele at a locus and \\(q\\) = allele frequency of the second allele at that same locus. Assumptions of H-W: No selection No mutation Random mating No differential migration No random drift Even though these assumptions will never be met completely in the real world, for the majority of the time the H-W prinicple holds regardless. Assuming assumptions are met, 2 important points from H-W: It takes no more than a single generation to equilibriate and stabilize gene frequencies in the two sexes. Only one additional generation is required for the stabilisation of the genotype frequencies into the predictible Hardy-Weinberg proportions. 4.3.2 Sex-linked loci Alleles on sex chromosomes in diploid organisms are obviously different. Sons can only receive an X chromosome from their mother so the frequency of X linked loci in the sons is equal to that of their mothers. Daughters receive both an X chromosome from Mum + from Dad. Overall this means allele frequencies oscilate around an equilibrium state, but continually get closer to that state over the generations (see Figure 4.2 and page 56 for equation). 4.3.3 Polyploidy Skipped over this section because it’s not relevant to human quant gen. Buuut, essentially it just details how to derive allele frequencies under a certain case of polyploidy. Also, it should be noted that of course H-W does not hold under polyploidy! 4.3.4 Age structure Age structure also complicates our idealised model of H-W. In populations composed of several age classes, the generations overlap, and this causes the approach of genotype frequencies towards the H-W expectations to be gradual (rather than just by 1 or 2 generations), even in the case of an autosomal locus. Doesn’t explain this very much, but it’s covered elsewhere. Importantly, when newly founded populations have significant age structure, fluctuations in both gene and genotoype frequencies may occur for a substantial period of time even in the abscence of selection! 4.3.5 Testing for Hardy-Weinberg proportions Says in the book that LRT can be used to test for departures from HWE and it can, but another common method is the chi-squared test and in PLINK they used Haldane’s exact test which is apparently analogus to Fisher’s exact test (papier on it) (can also use Fisher’s exact test if the sample size is tiny and the allele is rare.). Essentially, in a population, at a specific locus, you can calculate the allele frequencies (and from that expected genotype frequencies) from the observed genotype frequencies then test if there is a difference between the observed and expected values. LRT equation for it given on page 60. See code for some comparisons. Should remember (as pointed out above), that just because some assumptions are violated, doesn’t mean you’d get a departure from HWE! 4.4 Characterising the influence of a locus on the phenotype If a trait is entirely influenced by a single locus then the genetic effect on that trait can be characterised pretty easily and the dominance and additive effects of the alleles can be calculated. So if a locus has genotypes \\(B_1B_1\\), \\(B_1B_2\\), \\(B_2B_2\\), then the values given to these genotypes can be said to be: \\(-a\\), \\((1 + k)a\\) and \\(+a\\). Now if you have genotype data at that locus and data on the trait you can work out the effect of the \\(B_2\\) allele by taking the mean phenotypic value of individuals with \\(B_2B_2\\) and subtracting the mean phenotypic value of individuals with \\(B_1B_1\\) and dividing by 2 i.e. \\[\\begin{equation} B_{2eff} = \\frac{p_{B2} - p_{B1}} {2} \\tag{4.2} \\end{equation}\\] where \\(B_{2eff}\\) is the effect of allele \\(B_2\\), \\(p_{B2}\\) is the mean phenotypic value of individuals with \\(B_2B_2\\) and \\(p_{B1}\\) is the mean phneotypic value of individuals with \\(B_1B_1\\). As \\(B_{2eff} = a\\) you can then substitute this in to \\((1 + k)a\\) to get the dominance coefficient \\(k\\). Of course if \\(k = 0\\) then there is no dominance (in reality you would calculate probability of dominance). 4.5 The basis of dominance Confusing part… Don’t really get the enzyme activity bit… Main point (I think) is that new deleterious mutations are very likely to be recessive and new mutations with a slight deleterious effect interact in an almost entirely additive fashion (no dominance!). 4.6 Fisher’s decomposition of the genotypic value Recalling that the phenotypic value can be partitioned like so: \\[\\begin{equation} z = G + E \\tag{4.3} \\end{equation}\\] where \\(z\\) is the phenotype, \\(G\\) is the genotypic value and \\(E\\) is the environmental value. The genotypic value of a specific locus can be partitioned into it’s “expected” values based on there being only additive effects (\\(\\hat{G}\\)) and the deviations from the expected values or dominance effects (\\(\\delta\\)). So for genotype \\(B_iB_j\\): \\[\\begin{equation} G_{ij} = \\hat{G_{ij}} + \\delta_{ij} \\tag{4.4} \\end{equation}\\] This can be formalised (whatever the fuck that means) by regressing the genotypic values on the number of \\(B_1\\) and \\(B_2\\) alleles in the genotype (\\(N_1\\) and \\(N_2\\)): \\[\\begin{equation} G_{ij} = \\hat{G_{ij}} + \\delta_{ij} = \\mu_G + \\alpha_1N_1 + \\alpha_2N_2 \\tag{4.5} \\end{equation}\\] \\(\\mu_G\\) = the mean genotypic value in the population, \\(\\alpha_1\\) and \\(\\alpha_2\\) are the slopes of the regression, \\(N_1\\) and \\(N_2\\) are the number of \\(B_1\\) and \\(B_2\\) alleles. So the regression is: \\[\\begin{equation} G_{ij} ~ N_2 + N_1 \\end{equation}\\] By noting that for any individual, \\(N_1 = 2 - N_2\\) you can reduce the multiple regression model into an easier to work with univariate model. Give it a go (use equation (4.5)): \\[\\begin{equation} \\begin{split} G_{ij} &amp;= \\mu_G + \\alpha_1(2 - N_2) + \\alpha2N_2 + \\delta_{ij} \\\\ &amp;= l + (\\alpha_2 - \\alpha_1)N_2 + \\delta_{ij} \\end{split} \\tag{4.6} \\end{equation}\\] where \\(l = \\mu_G + 2\\alpha_1\\) is the intercept and the slope is \\(\\alpha = \\alpha_2 - \\alpha_1\\) If you plotted the genotypic value (\\(G\\)) against gene content (\\(N_2\\) or number of \\(B_2\\) alleles) and calculated residuals these residuals would be \\(\\delta\\), the dominance deviation (see Figure 4.6). The rest of the chapter uses this regression and what we know about genotype frequencies to derive a formula for the average effect of allelic substitution: \\[\\begin{equation} \\alpha = a[1 + k(p_1 - p_2)] \\tag{4.7} \\end{equation}\\] where \\(a\\) = genotypic value of \\(B_2\\) (see above), \\(k\\) is the dominance coefficient and \\(p_1\\) and \\(p_2\\) are the frequencies of \\(B_1\\) and \\(B_2\\). This value \\(\\alpha\\) represents the average change in genotypic value that results when a \\(B_2\\) allele is randomly substituted for a \\(B_1\\) allele. If no dominance (\\(k = 0\\)) then \\(\\alpha = a\\). Except in the case of additivity, the average effect of allelic substitution is not simply a function of the inherent physiological properties of the allele. It can only be defined in the context of the population! 4.7 Partioning the genetic variance. Deriving variance of \\(G\\): \\[\\begin{align} \\begin{split} G &amp;= \\hat{G} + \\delta \\notag \\\\ \\sigma^2_G &amp;= \\sigma^2(\\hat{G} + \\delta) \\notag \\\\ &amp;= \\sigma^2(\\hat{G}) + 2\\sigma(\\hat{G} + \\delta) + \\sigma^2(\\delta) \\end{split} \\tag{4.8} \\end{align}\\] The top equation is just like a regression, with \\(\\delta\\) being the residual error and we know that for least-squares there is no correlation between the residual error and the predictor. So there is no correlation between \\(\\hat{G}\\) and \\(\\delta\\). Therefore: \\[ \\sigma^2_G = \\sigma^2(\\hat{G}) + \\sigma^2(\\delta) \\] OR more commonly \\[\\begin{equation} \\sigma^2_G = \\sigma^2_A + \\sigma^2_D \\tag{4.9} \\end{equation}\\] \\(\\sigma^2_A\\) is the variance of \\(G\\) explained by regression on \\(N_2\\) (or \\(N_1\\)), and \\(\\sigma^2_D\\) is the residual variance of that regression. The variance of the additive and dominance effects! For a diallelic locus we can do some rearranging of equations in Table 4.1 of the book and get these equations: \\[\\begin{align} \\sigma^2_A &amp;= 2p_1p_2\\alpha^2 \\tag{4.10} \\\\ \\sigma^2_D &amp;= (2p_1p_2ak)^2 \\tag{4.11} \\end{align}\\] From these we can clearly see that both components depend on allele frequencies, the dominance coefficient and the homozygous effect (remember \\(\\alpha\\) is just the slope of the \\(G ~ N_2\\) regression!). By plotting how genetic variance changes with gene frequency under different scenarios (see 4.1). You see some interesting patterns. Firstly, at a single diallelic locus, you see that \\(\\sigma^2_A\\) reaches it’s peak when \\(p_1 = p_2 = 0.5\\). Secondly, it’s clear that, even in the case of overdominance (which is rare!), additive genetic variance will almost always be much higher than genetic variance from dominance effects, even when the frequency of the dominant allele is high. Figure 4.1: The dependence of components of genetic variance at a locus on the frequency of the \\(B_2\\) allele. \\(a\\) is set to be one, which scales the vertical axes so that for any particular case, the actual variances are obtainable by multiplying by \\(a^2\\). 4.8 Additive effects, average excesses and breeding values The dominance deviation of a parent, which is a function of the interaction between the two parental alleles, is eliminated when gametes are produced. Thus, one can think of \\(\\hat{G}\\) and \\(\\delta\\) as the heritable and nonheritable components of an individual’s genotypic value. Fisher proposed two different measures of the effect of an allele: one being the additive effect (\\(\\alpha_i\\)) and then the average excess \\(\\alpha^x_i\\). The average excess \\(\\alpha^x_2\\) of allele \\(B_2\\) is the difference between the mean genotypic value of individuals carrying at least one copy of \\(B_2\\) and the mean genotypic value of a random individual from the entire population: \\[\\begin{equation} \\alpha^x_2 = (G_{12}P_{12|2} + G_{22}P_{22|2}) - \\mu_G \\tag{4.12} \\end{equation}\\] where \\(P_{ij}\\) is the conditional probability of a \\(B_iB_j\\) genotype given that one allele is \\(B_i\\). Under random mating \\(P_{ij|i} = p_j\\) (\\(p_j\\) = frequency of allele \\(B_j\\)). THINK ABOUT HARDY-WEINBERG AND IT MAKES SENSE! So under random mating, \\[\\begin{equation} \\alpha^x_2 = G_{12}p_1 + G_{22}p_2 - \\mu_G \\tag{4.13} \\end{equation}\\] \\(G_{12} = a(1+k)\\) and \\(G_{22} = 2a\\). By substituting these into the equation above for \\(\\alpha^x_1\\) and \\(\\alpha^x_2\\) and then calculating \\(\\alpha_1\\) and \\(\\alpha_2\\) (additive effects) by the method previously mentioned (regressing genotypic value \\(G\\) on the number of \\(B_2\\) alleles, \\(N_2\\)), we will see they’re equivalent (shown on page 72): \\[\\begin{align} \\alpha_2 &amp;= p_1\\alpha \\notag \\\\ \\alpha_1 &amp;= -p_2\\alpha \\end{align}\\] The breeding value of an individual (\\(A\\)) is the sum of the additive effects of its genes. So the breeding value of a \\(B_1B_1\\) homozygote is just \\(2\\alpha_1\\). In randomly mating populations the breeding value of a genotype is equivalent to twice the expected deviation of its offspring mean phenotype from the population mean. Soooo, no genotype information is needed to calculate the breeding value. All we have to do is mate an individual to many randomly chosen individuals from the population and taking twice the deviation of its offspring mean from the population mean. EASY IN HUMANS!!! In Chapter 13 this will be discussed wrt candidate gene studies. 4.9 Extensions for multiple alleles and non random mating So this section seems mostly unrelevant as we’re unlikely to deal with situations with more than 2 alleles. Non-random mating could be encountered if we’re interested in some phenotypes (e.g. alcohol intake). Buuuut, it’s still good to note some of the generalised equations for what we’ve been discussing so far in the chapter. 4.9.1 Average excess When \\(n\\) alleles are present, the average excess, \\(\\alpha^x_i\\), for any allele \\(B_i\\) is given by \\[\\begin{equation} \\alpha^x_i = \\sum_{j=1}^{n} P_{ij|i}G_{ij} - \\mu_G \\tag{4.14} \\end{equation}\\] Remember, under random mating \\(P_{ij|i} == p_j\\) 4.9.2 Additive effects The genotypic value can also be obtained using regression as before, but in it’s generalised form is a multivariate regression. For \\(n\\) alleles \\[\\begin{equation} G = \\mu_G + \\sum_{i=1}^{n} \\alpha_{i}N_{i} + \\delta \\tag{4.15} \\end{equation}\\] After some re-arranging can derive the regression coefficients and finally end with \\[\\begin{equation} \\alpha_i = \\sum_{j=1}^{n} p_jG_{ij} - \\mu_G \\tag{4.16} \\end{equation}\\] i.e. under random mating, the average effects (\\(\\alpha_i\\)) are equal to the conditional mean deviations from the mean genotypic value of the population (\\(\\mu_G\\)). For non-random mating we need the inbreeding coefficient, \\(f\\) to define our genotype frequencies: \\[\\begin{align} P_{ii} &amp;= (1 - f)p^2_i + fp_i \\notag \\\\ P_{ij} &amp;= 2(1 -f)p_ip_j \\tag{4.17} \\end{align}\\] Unsure of why, but this means \\[\\begin{equation} \\alpha_i = \\frac{\\alpha^x_i} {1 + f} \\tag{4.18} \\end{equation}\\] so \\(f\\) is the fractional reduction of heterozygote frequencies relative to those expected under random mating. This means you can kind of do a test for random mating by checking heterozygote and homozygote frequencies in a population! 4.9.3 Additive genetic variance The additive genetic variance across \\(n\\) alleles is \\[\\begin{equation} \\sigma^2_A = 2 \\sum_{i=1}^{n} p_i\\alpha_i\\alpha^x_i \\tag{4.19} \\end{equation}\\] In general inbreeding inflates the additive genetic variance by causing correlations among the effects of alleles within the same individuals. The broad sense heritability, even under scenarios of non-random mating can be given by \\[\\begin{equation} \\sigma^2_G = \\sigma^{2}(\\alpha_i + \\alpha_j) + \\sigma^{2}(\\delta_{ij}) \\end{equation}\\] although it should be noted that the definitions of \\(\\alpha_i\\) and \\(\\delta_ij\\) change with the degree of inbreeding! Random mating means \\(\\alpha_i\\) and \\(\\alpha_j\\) are uncorrelated so we get back to the good old equation \\[ \\sigma^2_G = \\sigma^2_A + \\sigma^2_D \\] Importantly, under random mating, \\(\\sigma^2_A\\) is equivalent to the variance of breeding values of individuals in the population. Summarising some key terms The homozygous effect, \\(a\\), and the dominance coefficient, \\(k\\), are intrinsic properties of allelic products. They are not functions of allele frequencies, but may vary with genetic background The additive effect, \\(\\alpha_i\\), and the average excess, \\(\\alpha^x_i\\), are properties of alleles in a particular population. They are functions of \\(a\\), \\(k\\) and genotype frequencies (\\(p_i\\)). The breeding value, \\(A\\), is a property of a particular individual in reference to a particular population. It’s equivalent to the sum of the additive effects of an individual’s alleles. The additive genetic variance, \\(\\sigma^2_A\\) is a property of a particular population. It is equivalent to the variance of the breeding values of individuals within the population. 4.10 End of chapter questions What is the Hardy-Weinberg principle and what are it’s assumptions? What does the H-W principle mean for gene and genotype frequencies across generations? What is age structure and how does it affect HWE? How can you test for HWE? Are deliterious mutations likely to be dominant or recessive? Assuming a trait was entirely influenced by a single locus, how could you calculate dominance and additive effects knowing the genotypic value and the number of effect alleles? What is the formula for the average effect of allelic substitution? For a diallelic locus, what does the additive genetic variance and dominance genetic variance depend on? How does the contribution of additive genetic variance contribute to total genetic variance when \\(k\\) varies? What is the breeding value of an individual? Define the additive genetic variance in the presence of \\(n\\) alleles Learn the definitions of the key terms! "],
["sources-of-genetic-variation-for-multilocus-traits.html", "Chapter 5 Sources of genetic variation for multilocus traits 5.1 Epistasis 5.2 A general least-squares model for genetic effects 5.3 Linkage 5.4 Effect of disequilibrium of the genetic variance 5.5 End of chapter questions", " Chapter 5 Sources of genetic variation for multilocus traits 5.1 Epistasis Epistasis describes the nonadditivity of effects between loci, i.e. the alleles of one loci influence the effects of another loci. The genotypic value, \\(G_{ijkl}\\), needs to take into account all the interaction terms that can arrive between loci, for two loci it’s additive x additive effects (\\(\\alpha\\alpha\\)), additive x dominance effects (\\(\\alpha\\delta\\)), and dominance x dominance effects (\\(\\delta\\delta\\)). As the number of loci increases the number of interaction terms increase steadily e.g. \\(\\alpha\\alpha\\alpha\\) will be there for three loci. 5.2 A general least-squares model for genetic effects This is just an extension of the one-locus linear model introduced in Chapter 4. For this section, imagine we are interested in measuring the genetic effects of two loci, \\(G_{ijkl}\\), which can easily be extended to more. The additive effect of an allele on a phenotype is just the phenotypic value in people with that allele minus the mean phenotypic value of the population. When considering epistatic effects we can define it in the same way. \\[\\begin{equation} \\alpha_{i} = G_{i...} - \\mu_{G} \\tag{5.1} \\end{equation}\\] \\(G_{i...}\\) is just the conditional mean phenotype of individuals with allele \\(i\\) at the first locus without regard to the other allele at that locus or to the genotype at the second locus. The other additive terms (for \\(\\alpha_{j}\\), \\(\\alpha_{k}\\), \\(\\alpha_{l}\\)) are defined in the same way. Within each locus, the mean value of average effects (weighted by allele frequency) = 0. Dominance effects can be defined in a similar way, complete these equations by recalling (4.15): \\[\\begin{align} \\delta_{ij} &amp;= G_{ij..} - ... \\tag{5.2} \\\\ \\delta_{lk} &amp;= G_{..lk} - ... \\tag{5.3} \\end{align}\\] Like with the additive effects, the mean dominance deviation at each locus is equal to zero. Epistatic effect terms proceed in a similar fashion. Letting \\(G_{i.k.}\\) be the mean phenotype of individuals with gene \\(i\\) at locus 1 and \\(k\\) at locus 2, without regard to the other two genes, the \\(ik\\)th additive x additive effect is: \\[\\begin{equation} \\left(\\alpha\\alpha\\right)_{ik} = G_{i.k.} - \\mu_{G} - \\alpha_{i} - \\alpha_{k} \\tag{5.4} \\end{equation}\\] So \\(\\left(\\alpha\\alpha\\right)_{ik}\\) is the deviation of the conditional mean \\(G_{i.k.}\\) from the expectation based on the population mean \\(\\mu_{G}\\) and the additive effects \\(\\alpha_{i}\\) and \\(\\alpha_{k}\\). An additive x dominance effect measures the interaction between an allele at one locus with a genotype of another locus (see equation 5.5 in book) and the dominance x dominance effect involves an interaction between the genotypes at each locus (see equation 5.6 in book). The complete genotypic value, \\(G_{ijkl...}\\) can be found in equation 5.7 in the book. These parameters depend on genotype frequencies in the population, but the mean value of each type of effect is always equal to zero. The genotypic value of an individual is often impossible to quantify because of variation in the phenotype due to the environment, but the genotypic value for an individual equation can be extended to populations. Providing mating is random and segregation of loci is independent, there is no statistical relationship between the genes found within or among loci. So the total genetic variance is just the sum of the variance of the individual effects, simplified this is: \\[\\begin{equation} \\sigma^{2}_{G} = \\sigma^{2}_{A} + \\sigma^{2}_{D} + \\sigma^{2}_{AA} + \\sigma^{2}_{AD} + \\sigma^{2}_{DD} + ... \\tag{5.5} \\end{equation}\\] … here and in other cases just symbolises more terms can be added if more than two loci are used. Epistatic effects are expected to be common throughout the genome and Wright thought they were the rule, rather than exception. See example two in the book for calculations of epistatic effects and how much variance they contribute to the overall genetic variance component. Overall, it is clear that even with large epistatic effects, additive genetic variance, \\(\\sigma^{2}_{A}\\) will pretty much always (if not always) contribute to the vast majority of overall genetic variance \\(\\sigma^{2}_{G}\\). This is important for two reasons: Variance components provide limited insight into the physiological mode of gene action, i.e. just because genetic variance is explained by additive effects (which means you essentially count each gene separately), it does not mean the interaction between genes is not important in terms of their function! When interested in the variance of a trait that is explained by genetics, you can expect the vast majority of that variance to be explained by additive genetic effects, which makes things like estimating heritability far easier. 5.2.1 Extension to haploids and polyploids Skipped this section as not relevant to humans. 5.3 Linkage Genes of the same chromosome tend to be inherited as a group, a tendency that declines with increasing distance between the loci. Crossing-over during meiosis is responsible for this decline. Difference between linkage and linkage disequilibrium Loci are linked if they tend to be inherited together. If loci are correlated for any reason (don’t need to be inherited together), they are in linkage disequilibrium. The census units for measuring linkage are gamete frequencies, so you can use an individual to estimate this. LD is measured across a population. Can get linkage without LD, can get linkage and LD, and can get two correlated loci (LD) that aren’t linked. Even though you can get correlated loci for reasons other than linkage, the LD between linked loci are more likely to persist over time as seen in (5.8). Under linkage equilibrium, the frequency of gametes is the product of allele frequencies, so for loci \\(A\\) and \\(B\\), \\[\\begin{equation} Freq(AB) = Freq(A) * Freq(B) \\tag{5.6} \\end{equation}\\] So A and B are independent of each other. Measure of disequilibrium is just the departure from this: \\[\\begin{equation} D_{AB} = Freq(AB) - Freq(A) * Freq(B) \\tag{5.7} \\end{equation}\\] \\(D_{AB}\\) can be positive or negative depending on whether \\(A\\) and \\(B\\) are in coupling (\\(AB\\) gametes are overrepresented) or repulsion (\\(AB\\) gametes are underrepresented) disequilibrium. \\(D\\) is often referred to as the coefficient of linkage disequilibrium (although can be non-zero without linkage!). Selection, migration, mutation and drift can help maintain LD. Even without these forces, once LD is established it can be maintained for many generations (especially if loci are more tightly linked!). Expected LD changes over time depend on the recombination fraction between loci, \\(c\\). This value ranges from 0 to 0.5, where 0 essentially means the loci are inherited together and 0.5 is free recombination between loci. If recombination frequency between the \\(A\\) and \\(B\\) loci is \\(c\\), the disequilibrium in generation \\(t\\) is given by: \\[\\begin{equation} D(t) = (1 - c)^{t}D(0) \\tag{5.8} \\end{equation}\\] This equation is graphically displayed in 5.1. Figure 5.1: The decline, under random mating, of linkage disequilibrium when the initial value [D(0)] is set to 1 as a function of the recombination frequency, c. To estimate \\(D\\) you can directly count gamete frequencies of an individual. This is not possible for most organisms though and usually you have to make-do with measured multilocus genotypes across a population. You can then workout gametes used to produce the genotypes (e.g. someone with an \\(AABb\\) genotype would be formed from a \\(AB\\) gamete and \\(aB\\) gamete). However, with double heterozygotes you can’t be sure if an \\(ABab\\) individual was formed from \\(AB\\) and \\(ab\\) gametes or from \\(Ab\\) and \\(aB\\) gametes. Under random mating, it’s not necessary to distinguish between coupling and repulsion heterozygotes so this doesn’t really matter. In this case an estimate of \\(D\\) is given by: \\[\\begin{equation} \\hat{D_{AB}} = \\frac{N} {N - 1} [\\frac{4N_{AABB} + 2(N_{AABb} + N_{AaBB}) + N_{AbBb}} {2N} -2\\hat{p_{A}}\\hat{p_{B}}] \\tag{5.9} \\end{equation}\\] The equation for the sampling variance of \\(D\\) is shown on page 99. Ideally you’d have 1000s of samples to achieve reasonable statistical power when estimating \\(D\\) using multilocus genotype frequencies. 5.4 Effect of disequilibrium of the genetic variance The aggregate effects of gametic phase disequilibrium might be extensive for quantitative traits whose expression is based on large numbers of loci, even if the average level of disequilibrium between pairs of loci is relatively small. If genes with a positive influence on a character tend to be associated on some chromosomes, and those with a negative influence on others (coupling disequilbrium), the observed genetic variation will be inflated relative to the expectation under random assortment. The opposite will occur if “plus” alleles at one locus tend to be associated with “minus” alleles at another (repulsion disequilibrium). This is illustrated nicely by figure 5.6 in the book. Think of it this way, you’re studying gene \\(A\\), gene \\(B\\) and phenotype \\(X\\). If upregulation of \\(A\\) leads to an increase in \\(X\\) and an upregulation of \\(B\\) leads to an increase in \\(X\\) by the same proportions, then if genetic variation always occurs so that whenever \\(A\\) is upregulated, \\(B\\) is downregulated by the same amount, then what will be observed at both of the loci is that variation at them is not associated with variation in \\(X\\). This situation described is complete repulsion linkage (again, see figure 5.6 in book). This is assuming additive effects across loci. When there is no disequilibrium between loci, the variance at each locus is just \\(2pqa^{2}\\), see (4.10). As always, dominance effects muddy the waters, but here are the formalized multilocus analogs of (4.10) and (4.11): \\[\\begin{align} \\sigma^2_A &amp;= 2\\sum_{i=1}^{n} \\alpha(i)^{2}p_{i}q_{i} + 2\\sum_{i=1}^{n} \\sum_{j\\ne1}^{n} \\alpha(i)\\alpha(j)D_{ij} \\tag{5.10} \\\\ \\sigma^2_D &amp;= 4\\sum_{i=1}^{n} (a_{i}k_{i}p_{i}q_{i})^{2} + 4\\sum_{i=1}^{n} \\sum_{j\\ne1}^{n} a_{i}a_{j}k_{i}k_{j}D^2_{ij} \\tag{5.11} \\end{align}\\] where \\(\\alpha(i)\\) is the average effect of allelic substitution at the \\(i\\)th locus (defined in equation (4.7)). Epistatic interactions make things crazy complicated! In summary, the componenets of expressed genetic variance for quantitative traits can be partitioned into expected values under gametic phase equilibrium and deviations from these caused by disequilibrium. From the book: “When the disequilibrium covariance is negative, we refer to it as hidden genetic variance because it is subject to conversion to expressed genetic variance via the breakdown of gametic phase disequilibria”. What I think this means: Negative disequilibrium covariance is just when loci correlated loci covary in a way so that when one increases a traits value, the other decreases it, this is what happens in repulsion linkage! The reason it’s not just called repulsion linkage is because you can have covarying loci without them being linked. Sooo, what the book is saying here, is that if linkage disequilibrium (correlation) between the two loci that negatively covary is reduced, the amount of genetic variance they explain in the trait will increase! 5.4.1 The evidence Hidden genetic variation is expected to be a natural consequence of stabilising selection, which favours linkage groups for their composite properties without regard to the alleles at individual loci. Theoretical work has suggested that stabilising selection encourages the development of substantial hidden genetic variance, potentially depressing the level of expressed genetic variance to 50% or less than its equilibrium expectation. Of course selection doesn’t always favour an increase in hidden genetic variance. Sometimes coupling selection is favoured, so that expressed genetic variance exceeds equilibrium expectations. In this case the disequilibrium covariance is positive, and recombination would be expected to result in a reduction in the expressed genetic variance. Here we’ve just considered one trait, but of course the same thinking applies to selection upon multiple traits simultaneously. For example, in populations of insects (LIKE BEES) that exploit multiple host plants, one might expect a genetic correlation to evolve such that individuals prefer to feed on the plant species upon which they perform best. Such correlations could result from LD between a set of genes influencing preference and another influencing performance. 5.5 End of chapter questions Define epistasis Describe the terms that will be needed to define the genotypic value \\(G_{ijkl}\\) Complete equations (5.2) and (5.3) Give two important inferences from the fact the total genetic variance will mostly be attributable to the additive genetic variance, even if there are large dominance and epistatic effects What is linkage and linkage disequilibrium? What is the coefficient of linkage disequilibrium? What can influence maintenance of LD? (5 things) What is the relationship between linkage and LD over time? Give two methods of estimating \\(D\\) What are coupling disequlibrium and repulsion disequilibrium and how do they effect genetic variance? Explain equations (5.10) and (5.11) What is hidden genetic variance? How is stabilising selection thought to influence hidden genetic variance? Why? "],
["questions.html", "Questions Chapter 4 Chapter 5", " Questions Have fun answering these Gib! Chapter 4 What the fuck are they talking about with the molecular basis of dominance? - page 63-64 Chapter 5 How do they calculate the variance of a phenotype explained by just the dominance effects? - page 91 "]
]

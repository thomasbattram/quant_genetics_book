# Introduction to Matrix Algebra and Linear Models

$$\newcommand{\mx}[1]{\mathbf{#1}}$$

## Multiple regression

Simple multiple regression equation:

\begin{equation}
	y = \alpha + \beta_1z_1 + \beta_2z_2 + \dots + \beta_nz_n + e
	(\#eq:simple-multiple-regression)
\end{equation}

$y$ = dependent/response variable, $z_1, z_2, z_n$ = predictors, $e$ = residual error, $\alpha$ is a constant as are $\beta_1, \beta_2, \beta_n$ to be estimated.

Recall from Chapter 3 that the goal of least-squares regression is to find a set of constants ($\alpha$ and the $\beta$s) that minimise the squared differences between observed and expected values, with expected values is anything that fits on the "line of best fit". Also, recall equation \@ref(eq:intercept-and-slope), to see the relationship between $y$, $z_n$ ($x$ in the equation) and $b$. For multiple regression there are many "$b$" terms and each of them can be estimated by dividing the covariance of the dependent variable and the predictor ($\sigma(y, z_n)$) by the covariance of the predictor with all other predictors in the model. When $n = 1$, the model reduces to a simple linear regression and we return to equation \@ref(eq:intercept-and-slope). This can be represented in matrix form like so:

\begin{equation}
	\begin{pmatrix}
		\sigma^2(z_1) & \sigma(z_1, z_n) & \dots & \sigma(z_1, z_n) \\
		\sigma(z_1, z_1) & \sigma^2(z_2) & \dots & \sigma(z_2, z_n) \\
		\vdots & \vdots & \ddots & \vdots \\
		\sigma(z_1, z_n) & \sigma(z_2, z_n) & \dots & \sigma^2(z_n)
	\end{pmatrix}
	\begin{pmatrix}
		\beta_1 \\
		\beta_2 \\
		\vdots \\
		\beta_n
	\end{pmatrix}
	=
	\begin{pmatrix}
		\sigma(y, z_1) \\
		\sigma(y, z_2) \\
		\vdots \\
		\sigma(y, z_n)
	\end{pmatrix}
	(\#eq:multiple-regression-matrix-form)
\end{equation}

When estimating each response variable-predictor covariance term, it is the sum of predictor covariance multiplied by beta.

If the covariance matrix and the vectors of \@ref(eq:multiple-regression-matrix-form) are written as $\mx{V}$, $\mx{\beta}$ and $\mx{c}$ respectively, then the equation can be re-written as:

\begin{equation}
	\mx{V\beta} = \mx{c}
	(\#eq:abbreviated-multiple-regression)
\end{equation}

__NOTE__: It is standard procedure to denote matrices as bold capital letters and vectors as bold lower case letters.

Before going onto matrix methods in more detail, here is an application of \@ref(eq:simple-multiple-regression) in quantitative genetics.

### An application to multivariate selection

Suppose that a large number of individuals in a population have been measured for $n$ characters and for fitness. Individual fitness can then be approximated by the linear model

\begin{equation}
	w = \alpha + \beta_1z_1 + \beta_2z_2 + ... + \beta_nz_n + e
	(\#eq:fitness-linear-model)
\end{equation}

where $w$ is the relative fitness (observed fitness divided by the mean fitness in the population). In Chapter 3, we learnt that the selection differential for the $i$th trait is defined as the covariance between phenotype and relative fitness, $S_i = \sigma(z_i, w)$. Therefore, if we use multiple regression to estimate $S_i$ we'd end up with:

\begin{equation}
	S_i = \beta_i\sigma^2(z_i) + \sum^n_{j \neq i} {\beta_j\sigma(z_i, z_j)}
	(\#eq:ith-selection-differential) 
\end{equation}

Simple!

## Elementary matrix algebra

### Basic notation
Vectors and matrices in mathematics are just like those in R. A matrix with the same number of rows and columns is called a square matrix. Vectors written vertically are called column vectors, e.g. 

\begin{equation}
	\mx{a} = 
	\begin{pmatrix}
		12 \\
		13 \\
		47
	\end{pmatrix}
	\notag
\end{equation}

and those that are written horizontally are called row vectors, e.g. 

\begin{equation}
	\mx{b} = 
	\begin{pmatrix}
		12 & 13 & 47
	\end{pmatrix}
	\notag
\end{equation}

Single numbers by themselves are often referred to as scalars.

A matrix can be described by the elements that comprise it, with $M_ij$ denoting the element in the $i$th row and $j$th column of matrix $\mx{M}$.

### Partitioned matrices

It is often useful to work with partitioned matrices wherein each element in a matrix is itself a matrix. There are several ways to partition a matrix, for example: 

\begin{equation}
	\mx{c} = 
	\begin{pmatrix}
		3 & 1 & 2 \\
		2 & 5 & 4 \\
		1 & 1 & 2
	\end{pmatrix}
	=
	\begin{pmatrix}
		3 & \vdots & 1 & 2 \\
		\dots & \dots & \dots & \dots \\
		2 & \vdots & 5 & 4 \\
		1 & \vdots & 1 & 2
	\end{pmatrix}
	=
	\begin{pmatrix}
		\mx{a} & \mx{b} \\
		\mx{d} & \mx{B}
	\end{pmatrix}
	\notag
\end{equation}

where 

\begin{equation}
	\mx{a} = 
	\begin{pmatrix}
		3
	\end{pmatrix}
	,\  
	\mx{b} = 
	\begin{pmatrix}
		1 & 2
	\end{pmatrix}
	,\ 
	\mx{d} = 
	\begin{pmatrix}
		2 \\
		1
	\end{pmatrix}
	,\ 
	\mx{B}
	\begin{pmatrix}
		5 & 4 \\
		1 & 2
	\end{pmatrix}
	\notag
\end{equation}

There are also other simple ways to partition matrices. 

### Addition and subtraction

To add matrices, they must have the same dimensions. Just add the corresponding elelments, so for adding matrices $\mx{A}$, and $\mx{B}$ to make $\mx{C}$, it is simply $C_{ij} = A_{ij} + B_{ij}$. Subtraction is defined similarly.

__Can add examples here if you really want to__

### Multiplication

To multiply matrix $\mx{M}$ by scalar $a$, then just multiply each element of $\mx{M}$ by $a$.

Dot product of two vectors, $\mx{a} \cdot \mx{b}$, is a scalar given by

\begin{equation}
	\mx{a} \cdot \mx{b} = \sum^n_{i=1} {a_ib_i} \notag
\end{equation}

For example, for the two vectors given by

\begin{equation}
	\mx{a} = 
	\begin{pmatrix}
		1 & 2 & 3 & 4
	\end{pmatrix} 
\ \ \ \mathrm{and} \ \ \ 
	\mx{b} = 
	\begin{pmatrix}
		4 \\
		5 \\
		7 \\
		9
	\end{pmatrix}
	\notag
\end{equation}

the dot product $\mx{a} \cdot \mx{b}$ = (1 x 4) + (2 x 5) + (3 x 7) + (4 x 9) = 71. Dot product is not defined if the vectors have different lengths.

Now consider the matrix $\mx{L} = \mx{M}\mx{N}$ produced by multiplying the $r$ x $c$ matrix $\mx{M}$ by the $c$ x $b$ matrix $\mx{N}$. Partitioning $\mx{M}$ as a column vector of $r$ row vectors,

\begin{equation}
	\mx{M} = 
	\begin{pmatrix}
		\mx{m_1} \\
		\mx{m_2} \\
		\vdots \\
		\mx{m_r}
	\end{pmatrix} 
	\ \ \ \mathrm{where}\ \ \ 
	\mx{m_i} = 
	\begin{pmatrix}
		M_{i1} & M_{i2} & \dots & M_{ic}
	\end{pmatrix}
	\notag
\end{equation}

and $\mx{N}$ as a row vector of $b$ column vectors,

\begin{equation}
	\mx{N} = 
	\begin{pmatrix}
		\mx{n_1} & \mx{n_2} & \dots & \mx{n_b}
	\end{pmatrix}
	\ \ \ \mathrm{where}\ \ \ 
	\mx{n_j} = 
	\begin{pmatrix}
		N_{1j} \\
		N_{2j} \\
		\vdots \\
		N_{cj} \\
	\end{pmatrix} 
	\notag
\end{equation}

the $ij$th element of $\mx{L}$ is given by the dot product

\begin{equation}
	L_{ij} = \mx{m_i} \cdot \mx{n_j} = \sum^c_{k=1} {M_{ik}N_{kj}}
	(\#eq:matrix-multiplication-L)
\end{equation}

Use this to write out matrix $\mx{L}$ -- cba right now...

To be definied, the number of columns in the first matrix must equal the number of rows in the second matrix. This means that unless the two matrices are square, it is only possible to multiply them together one way round and not the other (e.g. $\mx{M}\mx{N}$ may be defined, but $\mx{N}\mx{M}$ won't). Writing $\mx{M}_{r \times c}\mx{N}_{c \times b} = \mx{L}_{r \times b}$ shows the inner indices must match, while the outer indices give the number of rows and columns of the resulting matrix. Even if the matrices being multiplied are square, the order in which they are multiplied is important, i.e. multiplying $\mx{A}$ by $\mx{B}$ is not the same as _vice versa_. So there is terminology to help differentiate what is being multiplied by what. In the example just given, one would say matrix $\mx{B}$ is premultiplied by matrix $\mx{A}$, or that matrix $\mx{A}$ is postmultiplied by matrix $\mx{B}$.

### Transposition

The transpose of a matrix $\mx{A}$ is written as $\mx{A}^T$ or $\mx{A}'$. It is obtained by simply switching the rows and columns of the matrix.

__Add example here if you want__

A useful identity is

\begin{equation}
	(\mx{A}\mx{B}\mx{C})^T = \mx{C}^T\mx{B}^T\mx{A}^T
	(\#eq:transpose-of-ABC)
\end{equation}

which holds for any number of matrices.

Vectors tend to be written as column vectors, and therefore will be written as such henceforth, and as lowercase bold letters, e.g. $\mx{a}$, for a column vector and $\mx{a}^T$ for the corresponding row vector. Further, when multiplying vectors we can assess the inner product (dot product), which yields a scalar and the outer product, which yields a matrix. For the two $n$-dimensional column vectors $\mx{a}$ and $\mx{b}$,

\begin{equation}
	\mx{a} = 
	\begin{pmatrix}
	a_1 \\
	\vdots \\
	a_n
	\end{pmatrix}
	\ \ \ 
	\mx{b} = 
	\begin{pmatrix}
	b_1 \\
	\vdots \\
	b_n
	\end{pmatrix}
	\notag
\end{equation}

the inner product is given by

\begin{equation}
	\begin{pmatrix}
	a_1 & \dots & a_n
	\end{pmatrix}
	\begin{pmatrix}
	b_1 \\
	\vdots \\
	b_n
	\end{pmatrix}
	 
	= \mx{a}^T\mx{b} = \sum^n_{i=1} {a_ib_i}
	(\#eq:inner-product)
\end{equation}

while the outer product yields an $n \times n$ matrix

\begin{equation}
	\begin{pmatrix}
		a_1 \\
		\vdots \\
		a_n
	\end{pmatrix}
	\begin{pmatrix}
		b_1 & \dots & b_n
	\end{pmatrix}

	= \mx{a}\mx{b}^T = 

	\begin{pmatrix}
		a_1b_1 & a_1b_2 & \dots & a_1b_n \\
		a_2b_1 & a_2b_2 & \dots & a_2b_n \\
		\vdots & \vdots & \ddots & \vdots \\
		a_nb_1 & a_nb_2 & \dots & a_nb_n
	\end{pmatrix}
	(\#eq:outer-product)
\end{equation}

### Inverses and solutions to systems of equations

Inverting a matrix is an operation similar to division of scalars. Multiplying an matrix by its inverse gives the identity matrix, $\mx{I}$, which is a matrix where the diagonals are 1 and all other elements are 0. This plays the same role as the number 1 in scalar multiplication and division. The notation for the inverse of matrix $\mx{A}$ would be $\mx{A}^{-1}$ and $\mx{A}\mx{A}^{-1} = \mx{I}$. This is useful for solving equations containing matrices, e.g. if you want to divide both sides of the equation by a matrix. A matrix is called nonsingular if its inverse exists. A useful property of inverses is that if the matrix product $\mx{AB}$ is a square matrix (where $\mx{A}$ and $\mx{B}$ are both square), then

\begin{equation}
	(\mx{AB})^{-1} = \mx{B}^{-1}\mx{A}^{-1}
	(\#eq:inverse-of-square-matrix-product)
\end{equation}

When you have a non-square or singular matrix you can still obtain solutions to equations containing the matrix by using generalised inverses, but such solutions are not unique. See appendix 3 in the book for deets.

Recalling equation \@ref(eq:abbreviated-multiple-regression), we can see that the solution ($\mx{\beta}$) can be expressed as 

\begin{equation}
	\mx{\beta} = \mx{V}^{-1}\mx{c}
	(\#eq:beta-multiple-regression)
\end{equation}

Likewise, for the Pearson-Lande-Arnold regression giving the best linear prediction of fitness,

\begin{equation}
	\mx{\beta} = \mx{P}^{-1}\mx{s}
	(\#eq:beta-pearson-lande-arnold-regression)
\end{equation}

where $\mx{P}$ is the covariance matrix for phenotypic measures $z_1, ..., z_n$, and $\mx{s}$ is the vector of selection differentials for the $n$ characters.

Before going on to the formal expression for inverting a matrix, let's have a look at some sweet sweet examples of extreme, but hella useful cases that lead to simple expressions for the inverse, shall weeeeee? First up, if the matrix is diagonal (all off-diagonal elements are zero), then the matrix inverse is also diagonal, with $\mx{A}^{-1}_{ii} = 1/A_{ii}$, i.e. just raise the diagonal elements to the power of negative one. Simple! Of course, if any diagonal elements are el zilcho, then it wonee work as you canee divide by zero (inverse is undefined).

Second, for any 2 x 2 matrix $\mx{A}$,

\begin{equation}
	\mx{A} = 
	\begin{pmatrix}
		a & b \\
		c & d
	\end{pmatrix}
	\ \ \ \mathrm{then}\ \ \ 
	\mx{A}^{-1}
	= 
	\frac{1} {ad - bc}
	\begin{pmatrix}
		d & -b \\
		-c & a
	\end{pmatrix}
	(\#eq:inverse-of-twobytwo-matrix)
\end{equation}

If $ad = bc$, the inverse doesn't exist as division by zero is undefined! 

### Determinants and minors

F0r a $2 \times 2$ matrix, the quantity

\begin{equation}
	|\mx{A}| = A_{11}A_{22} - A_{12}A_{21}
	(\#eq:determinant-of-twobytwo-matrix)
\end{equation}

is called the determinant, which more generally is denoted by det($\mx{A}$) or $|\mx{A}|$. $\mx{A}^{-1}$ only exists for a square matrix $\mx{A}$ if $|\mx{A}| \neq 0$. For square matrices greater than 2, the determinant is obtained recursively from the general expression

\begin{equation}
	|\mx{A}| = \sum^n_{j=1} {A_ij}(-1)^{i+j}|\mx{A}_{ij}|
	(\#eq:general-determinant-equation)
\end{equation}

where $i$ is any fixed row of the matrix and $\mx{A}$ and $\mx{A}_{ij}$ is a submatrix obtained by deleting the $i$th row and $j$th column from $\mx{A}$. Such a submatrix is known as a minor. Essentially you should be left with some $2 \times 2$ matrices at the end of all this. If the matrix is a diagonal, then the determinant is simply the product of the diagonal elements of that matrix, i.e. if

\begin{equation}
	A_{ij} = 
	\begin{cases}
		a_i, & i = j\\
		0, & i \neq j
	\end{cases}
	\ \ \ \mathrm{then} \ \ \ 
	|\mx{A}| = \prod^n_{i=1}a_{i}
	\notag
\end{equation}

### Computing inverses

The general solution of a matrix inverse is 

\begin{equation}
	A^{-1}_{ij} = \left[\frac{(-1)^{i+j}|\mx{A}_{ij}|} {|\mx{A}|} \right]^T
	(\#eq:solution-to-matrix-inversion)
\end{equation}

where $A^{-1}_{ij}$ denotes the $ij$th element of $\mx{A}^{-1}$ and $\mx{A}_{ij}$ denotes the $ij$th minor of $\mx{A}$. From equation \@ref(eq:solution-to-matrix-inversion), division by the determinant is required for inversion, therefore the determinant of a matrix must be nonzero if that matrix can be inverted. Thus, a matrix is singular if its determinant is zero. This occurs whenever a matrix contains a row (or column) that can be written as a weighted sum of any other rows (or columns). In the context of our linear model, \@ref(eq:multiple-regression-matrix-form), this happens if one of the $n$ equations can be written as a combination of the others, a situation that is equivalent to there being $n$ unknowns but less than $n$ independent equations.

## Expectations of random vectors and matrices

Matrix algebra provides a powerful approach for analysing linear combinations of random variables. Let $\mx{x}$ be a column vector containing $n$ random variables, $\mx{x} = (x_1, x_2, \dots, x_n)^T$. We may want to construct a new univariate (scalar) random variable $y$ by taking some linear combination of the elements of $x$,

\begin{equation}
	y = \sum^n_{i=1} {a_ix_i} = \mx{a}^T \mx{x}
	\notag
\end{equation}

where $\mx{a} = (a_1, a_2, \dots, a_n)^T$ is a column vector of constants. Good example of this is when we want to make a weighted genetic score. Likewise, we can construct a new $k$-dimensional vector $\mx{y}$ by premultiplying $\mx{x}$ by a $k \times n$ matrix $\mx{A}$ of constants, $\mx{y} = \mx{A}\mx{x}$. More generally, an ($n \times k$) matrix $\mx{X}$ of random variables can be transformed into a new $m \times l$ dimensional matrix $\mx{Y}$ of elements consisting of linear combinations of the elements of $\mx{X}$ by

\begin{equation}
	\mx{Y}_{m \times l} = \mx{A}_{m \times n} \mx{X}_{n \times k} \mx{B}_{k \times l}
	(\#eq:transforming-matrix-of-random-variables)
\end{equation}

where the matrices $\mx{A}$ and $\mx{B}$ are constants with dimensions as subscripted.

If $\mx{X}$ is full of random variables, the expected value of $\mx{X}$ is $E(\mx{X})$ containing the expected value of each element of $\mx{X}$. If $\mx{X}$ and $\mx{Z}$ are matrices of the same dimension, then

\begin{equation}
	E(\mx{X} + \mx{Z}) = E(\mx{X}) + E(\mx{Z})
	(\#eq:matrices-addition-expectations)
\end{equation}

Similarly, just remember other rules of expectations to obtain an expression for the expectation of $\mx{Y}$ from \@ref(eq:transforming-matrix-of-random-variables). The important point is of course that the dimensions of matrices need to match up! 

## Covariance matrices of transformed vectors

If we have an $n \times n$ square matrix $\mx{A}$ and an $n \times 1$ column vector $\mx{x}$, then:

\begin{equation}
	\mx{x}^T\mx{Ax} = \sum^n_{i=1} \sum^n_{j=1} {a_{ij}}x_ix_j
	(\#eq:matrix-columnvec-multiplication)
\end{equation}

This expression is called a quadratic form (or quadratic product) and yield a scalar. A generalisation of a quadratic form is the bilinear form, $\mx{b^TAa}$, where $\mx{b}$ and $\mx{a}$ are, respectively $n \times 1$ and $m \times 1$ column vectors and $\mx{A}$ is an $n \times m$ matrix. Index the matrices and you can tell what the resulting product is! Hint: it's a scaler, figure out why! As scalars, bilinear forms equal their transposes, giving the useful identity:

\begin{equation}
	\mx{b^TAa} = (\mx{b^TAa})^T = \mx{a^TA^Tb}
	(\#eq:bilinear-form)
\end{equation}

If $\mx{x}$ is a vector of $n$ random variables, then you can express the $n$ variances and $n(n-1)/2$ covariances associated with the elements of $x$ as the matrix $\mx{V}$, where $V_{ij} = \sigma(x_i, x_j)$ is the covariance between the random variables $x_i$ and $x_j$. This is a covariance matrix (or variance-covariance matrix)! Remember: diagonals = variances and off-diagonals = covariances! The $\mx{V}$ matrix is symmetric such that:

\begin{equation}
	V_{ij} = \sigma(x_i, x_j) = \sigma(x_j, x_i) = V_{ji}
	\notag
\end{equation}

If we have a univariate random variable $y = \sum{c_{k}x_{k}}$ generated from a linear combination of the elements of $\mx{x}$, in matrix notation we have $y = \mx{c}^T\mx{x}$, where $\mx{c}$ is a column vector of constants. The variance of $y$ can be expressed as a quadratic form involving the covariance matrix $\mx{V}$ for the elements of $\mx{x}$, 

\begin{equation}
	\mx{c}^T\mx{V}\mx{c}
	(\#eq:variance-of-univariate-random-variable-y)
\end{equation}

(See book for derivation, \@ref(eq:variance-of-univariate-random-variable-y) is the final result)

Likewise, the covariance between two univariate random variables created from different linear combinations of $\mx{x}$ is given by the bilinear form

\begin{equation}
	\sigma(\mx{a^Tx}, \mx{b^Tx}) = \mx{a^TVb}
	(\#eq:covariance-of-univariate-random-variables-created-from-x)
\end{equation}

If we transform $\mx{x}$ to two new vectors $\mx{y}_{x \times 1} = \mx{A}_{l \times n}\mx{x}_{n \times 1}$ and $\mx{z}_{m \times 1} = \mx{B}_{m \times n}\mx{x}_{n \times 1}$, then instead of a single covariance we have an $l \times m$ dimensional covariance matrix, denoted $\mx{\sigma(y, z)}$. Letting $\mx{\mu_{y}} = \mx{A_\mu}$ and $\mx{\mu_{z}} = \mx{B_\mu}$, with $E(\mx{x}) = \mu$, then $\mx{\sigma(y, z)}$ can be expressed in terms of $\mx{V}$, the covariance matrix of $\mx{x}$, 

\begin{equation}
\begin{split}
	\mx{\sigma(y, z)} &= \mx{\sigma(Ax, Bx)} \notag \\
	&= E\left[(\mx{y} - \mx{\mu_{y}})(\mx{z} - \mx{\mu_{z}})^T \right] \notag \\
	&= E\left[\mx{A}(\mx{x} - \mx{\mu})(\mx{x} - \mx{\mu})^T \mx{B}^T \right] \notag \\
	&= \mx{AVB}^T 
\end{split}
	(\#eq:covariance-of-two-vectors)
\end{equation}

In particular, the covariance matrix for $\mx{y} = \mx{Ax}$ is 

\begin{equation}
	\mx{\sigma(y, y)} = \mx{AVA}^T
	(\#eq:covariance-matrix-of-y-equals-Ax)
\end{equation}

so that the covariance between $y_i$ and $y_$ is given by the $ij$th element of the matrix product $\mx{AVA}^T$. Finally, note that if $\mx{x}$ is a vector of random variables with expected value $\mu$, then the expected value of the scalar quadratic product $\mx{x}^T\mx{Ax}$ is 

\begin{equation}
	\mx{x}^T\mx{Ax} = tr(\mx{AV}) + \mx{\mu}^T\mx{A\mu}
	(\#eq:no-idea-what-to-call-this-eq)
\end{equation}

where $\mx{V}$ is the covariance matrix for the elements of $\mx{x}$, and the trace of a square matrix, $tr(\mx{M}) = \sum{M_{ii}}$ is the sum of its diagonal values.

## The multivariate normal distribution

Multivariate normal distribution = MVN. 

Consider the probability density function for $n$ independent normal random variables, where $x_i$ is normally distributed with mean $\mu_i$ and variance $\sigma^2_i$. In this case, because the variables are independent, the joint probability density function is simply the product of each univariate density, 

\begin{equation}
	TO-DO
	(\#eq:joint-prob-density-function-independent-vars)
\end{equation}

Let's express this in matrix form, because we're cool! (also it compacts things a lot). To do this, we need to defin these matrices

\begin{equation}
	\mx{V} = 
	\begin{pmatrix}
		\sigma^2_1 & 0 & \dots & 0 \\
		0 & \sigma^2_2 & \dots & 0 \\
		\vdots & \vdots & \ddots & \vdots \\
		0 & 0 & 0 & \sigma^2_n
	\end{pmatrix}
	\ \ \ \mathrm{and}\ \ \
	\mx{\mu} = 
	\begin{pmatrix}
		\mu_1 \\
		\mu_2 \\
		\vdots \\
		\mu_n
	\end{pmatrix}
	\notag
\end{equation}

Since $\mx{V}$ is diagonal, its determinant is simply the product of the diagonal elements

\begin{equation}
	|\mx{V}| = \prod^n_{i=1}\sigma^2_i
	\notag
\end{equation}

Likewise, using quadratic products, note that

\begin{equation}
	\sum^n_{i=1} \frac{(x_i - \mu_i)^2} {\sigma^2_i} = (\mx{x} - \mx{\mu}^T)\mx{V}^{-1} (\mx{x} - \mx{\mu})
	\notag
\end{equation}

putting these together, equation \@ref(eq:joint-prob-density-function-independent-vars) can be written as

\begin{equation}
	TO-DO
	(\#eq:joint-prob-density-function-independent-vars-as-matrices)
\end{equation}

This can also be written as $p(\mx{x}, \mx{\mu}, \mx{V}$ to stress that it is a function of the mean vector $\mx{\mu}$ and the covariance matrix $\mx{V}$.

More generally, when the elements of $\mx{x}$ are correlated, equation \@ref(eq:joint-prob-density-function-independent-vars-as-matrices) gives the probability density function for a vector of multivariate normally distributed random variables, with mean vector $\mx{\mu}$ and covariance matrix $\mx{V}$. We denote this by 

\begin{equation}
	\mx{x} \sim \mathrm{MVN_n}(\mx{\mu, V})
	\notag
\end{equation}

where the subscript indicating the dimensionality of $\mx{x}$ is usually omitted. MVN also called Gaussian distribution.

### Properties of the MVN

Like the univariate normal distribution, the MVN is expected to arise naturally when the trait of interest result from a large number of underlying variables, so fits many human traits! Prepare your butts for some useful properties of MVN, because here they come!!! 

1. If $\mx{x} \sim \mathrm{MVN}$, then the distribution of any subset of the variables in $\mx{x}$ is also MVN. For example, each $x_i$ is normally distributed and each pair $(x_i, x_j)$ is bivariate normally distributed.

2. If $\mx{x} \sim \mathrm{MVN}$, then any linear combination of the elements of $\mx{x}$ is also MVN. Specifically, if $\mx{x} \sim \mathrm{MVN_n}(\mx{\mu}, \mx{V})$, $\mx{a}$ is a vector of constants, and $\mx{A}$ is a matrix of constants, then

\begin{equation}
	\mathrm{for}\ \ \ \mx{y} = \mx{x} + \mx{a}, \ \ \ \ \ \mx{y} \sim \mathrm{MVN_n}(\mx{\mu} + \mx{a}, \mx{V}) \\
	\mathrm{for}\ \ \ \mx{y} = \mx{a}^T \mx{x} = \sum^n_{k=1} {a_ix_i}, \ \ \ \ \ y \sim \mathrm{N}(\mx{a}^{T}\mx{\mu}, \mx{a}^{T}\mx{V}\mx{a}) \\
	\mathrm{for}\ \ \ \mx{y} = \mx{A}\mx{x}, \ \ \ \ \ \mx{y} \sim \mathrm{MVN_m}(\mx{A\mu}, \mx{A^{T}VA})
\end{equation}

3. Conditional distributions associated with MVN are also multivariate normal. __ADD IN BOOK PART HERE!!__

4. If $\mx{x} \sim \mathrm{MVN}$, the regression of any subset of $\mx{x}$ on another subset is linear and homoscedastic. __ADD IN BOOK PART HERE!!__

## Overview of linear models

Consider a multivariate linear model where all the covariates are binary. For example, consider the half-sib design wherein each of $p$ unrelated sires is mated at random to a number of unrelated dams and a single offspring is measured from each cross. The simplest model for this design is 

\begin{equation}
	y_{ij} = \mu + s_i + e_{ij}
	\notag
\end{equation}

where $y_{ij}$ is the phenotype of the $j$th offspring from sire $i$, $\mu$ is the population mean, $s_i$ is the sire effect, and $e_{ij}$ is the residual error. __DON'T UNDERSTAND THIS SENTENCE__ Although this is clearly a linear model, it differs significantly from the regression model described above in that while there are parameters to estimate (the sire effects, $s_i$), the only measured values are the $y_{ij}$. Nevertheless, we can express this model in a form that is essentially identical to the standard regression model by using $p$ binary variables to classify the sires of the offspring. The resulting linear model becomes

\begin{equation}
	y_{ij} = \mu + \sum^p_{k=1}s_{k}x_{ik} + e_{ij} 
	\notag
\end{equation}
where
\begin{equation}
	x_{ik} = 
	\begin{cases}
		0, & \text{ if k = i } \\
		1, & \text{ otherwise }
	\end{cases}
\end{equation}

By using binary variables, an wide class of problems can be handled by linear models. Models containing only binary variables are usually called ANOVA (analysis of variance) models! Whether predictor variables are a continuous (as in regression) or not (as in ANOVA), the procedures are special cases of the general linear model (GLM), wherein each observation ($\mx{y}$) is assumed to be a linear function of $p$ observed and / or binary variables plus a residual error ($e$), 

\begin{equation}
	y_i = \sum^p_{k=1} \beta_{k}x_{ik} + e_{i}
	(\#eq:general-linear-model)
\end{equation}

where $x_{i1}, \dots, x_{ip}$ are the values of the $p$ predictor variables for the $i$th individual. For a vector of $n$ observations, the GLM can be written in matrix form as 

\begin{equation}
	\mx{y} = \mx{X\beta} + \mx{e}
	(\#eq:general-linear-model-matrix-form)	
\end{equation}

where the design or incidence matrix $\mx{X}$ is $n \times p$, and $\mx{e}$ is the vector of residual errors. $\mx{\beta}$ is to be estimated obvs.

### Ordinary least squares

Estimates of $\mx{\beta}$ are usually obtained using least-squares that make assumptions about the residuals. Ordinary least squares assumes that the residual errors are homoscedastic and uncorrelated, i.e., $\sigma^2(e_i) = \sigma^2_e$ for all $i$ and $\sigma(e_i, e_j) = 0$ for $i \neq j$.

SOME MATHS HERE

\begin{equation}
	\mx{b} = (\mx{X}^T\mx{X})^{-1}\mx{X}^T\mx{y}
	(\#eq:estimate-for-beta)
\end{equation}

where $\mx{b}$ is an estimate of $\mx{\beta}$. Under the assumption the residual errors are uncorrelated and homoscedastic, the covariance matrix of the elements of $\mx{b}$ is 

\begin{equation}
	\mx{V_b} = (\mx{X}^T\mx{X})^{-1}\sigma^{2}_{e}
	(\#eq:estimate-for-beta-covariance-matrix)
\end{equation}

If the residuals follow a multivariate distribution with $\mx{e} \sim \mathrm{MVN}(0, \sigma^{2}_{e} \dot \mx{I}$, the OLS estimate is also the maximum-likelihood estimate. 

## Generalised least squares

